text
"AI creating Human-Looking Images and Tracking Artificial Intelligence Programs in 2020

Machine Learning Transforming Veterans Benefits

The Dutch artist, Bas Uterwijk is using artificial intelligence to create human portraits from paintings by combining with deep learning to change statutes to human faces.

The same applies to paintings where the AI software includes photo attributes such as light and variations to make the picture clear. The Artbreeder AI program recreates new images from scratch by using data points, which copy the photos.

The global competitiveness of the United States in artificial intelligence is declining because of poor management at the Department of Defense. Tracking artificial intelligence programs and encouraging data sharing are needed to make the US a global leader in AI.

The Joint Artificial Intelligence Center is creating a standardized AI definition and developing governance policies around artificial intelligence.

Veterans face challenges claiming their benefits and a new machine learning application, Content Classification Predictive Service (CCPS), is spearheading efficient services and accuracy in handling veteran claims. Veterans wait for long as staff members check claims manually but CCPS can review information within a short time.

These and more insights on our Weekly AI Update

AI creating Human-Looking Images

Artificial intelligence is helping to create human-like portraits from statues and paintings of famous faces.

Bas Uterwijk a Dutch native artist used AI to create the photo-style portraits. He focused on well-known figures including Vincent Van Gogh and Napoleon Bonaparte. The #deeplearning technology enabled him to take a photo of a statue or a painting and turn it into a more human-like face. The software uses data points to pick up on facial features and photographic qualities.

The AI is called Artbreeder and can also create human-looking images from scratch. So far, they’ve worked on 50 to 60 of the AI-generated pictures¹. The artist is working on a model that could show Anne Frank at an age she never reached.

Tracking Artificial Intelligence Programs

Poor management of artificial intelligence projects in the Department of Defense could erode the United States’ competitive advantage in the emerging technology, the Defense Department’s watchdog warned in a July 1 report.

The DoD inspector general suggested the Joint Artificial Intelligence Center, established to facilitate the adoption of artificial intelligence tools across the department, take several steps to improve project management, including determining a standard definition of artificial intelligence, improving data sharing and developing a process to accurately track artificial intelligence programs. The JAIC missed a March 2020 deadline to release a governance framework. It still plans to do so, according to the report, but that date is redacted in the report.

The inspector general started the audit to determine the gaps and weaknesses in the department’s enterprise-wide AI governance², the responsibility of the JAIC. After starting its audit, the DoD IG determined the organization had not yet developed a department-wide AI governance framework.

Machine Learning Transforming Veterans Benefits

Veterans deserve fast access to their disability benefits. The Department of Veterans Affairs is using a new #machinelearning tool³ to deliver these benefits to Veterans more quickly.

The tool’s name is not easy to remember — Content Classification Predictive Service (CCPS) Application Programming Interface (API) — but the results are certainly hard to ignore. VA’s Office of Information and Technology (OIT), working collaboratively in partnership with Veterans Benefits Administration (VBA), developed and implemented CCPS to reduce the average time to establish Veteran disability compensation claims by three and a half days.

Credit: Clarianchalets

CCPS is also helping VA improve service to Veterans by increasing the speed and accuracy of disability claims reviews. The tool automatically performs repetitive tasks that formerly required staff review and input.

During its first week of use, CCPS helped VA establish 3,994 out of 8,368 claims (48 percent) automatically without the need for manual intervention. Previously, VBA only processed about two percent of disability compensation claims automatically.

Visual Causal Discovery Network

Researchers at MIT, University of Washington, and the University of Toronto describe an AI system that learns the physical interactions⁴ affecting materials like fabric by watching videos. They claim the system can extrapolate to interactions it has not seen before, like those involving multiple shirts and pants, enabling it to make long-term predictions.

Causal understanding is the basis of counterfactual reasoning, or the imagining of possible alternatives to events that have already happened. For example, in an image containing a pair of balls connected to each other by a spring, counterfactual reasoning would entail predicting the ways the spring affects the balls’ interactions.

The researchers’ system — a Visual Causal Discovery Network (V-CDN) — guesses at interactions with three modules: one for visual perception, one for structure inference, and one for dynamics prediction. The perception model is trained to extract certain keypoints (areas of interest) from videos, from which the interference module identifies the variables that govern interactions between pairs of keypoints.

Encouraging Growth in AI Research

The National Research Cloud, which has bipartisan support in Congress, gained approval of several universities, including Stanford, Carnegie Mellon and Ohio State, and participation of Big Tech companies Amazon, Google and IBM.

The project would give academics access to a tech companies’ #clouddata centers and public data sets, encouraging growth in AI research⁵. Although the Trump administration has cut funding to other kinds of research, it has proposed doubling its spending on AI by 2022.

The research cloud, though a conceptual blueprint at this stage, is a sign of the largely effective campaign by universities and tech companies to persuade the American government to increase government backing for research into #artificialintelligence largely due to its recognition that AI technology is essential to national security and economic competitiveness.

Artificial Intelligence assisted Robot Delivery

Refraction AI’s last-mile delivery robot⁶, the REV-1, has seen an increase in lunch delivery requests since the start of the coronavirus pandemic. Unsurprisingly, this contactless delivery option is now seeing a demand surge amid the coronavirus pandemic: Refraction AI has received three to four times more orders with the REV-1 since the start of the pandemic.

The company, which first launched in July 2019, built the robot specifically for last-mile deliveries between stores and customers in urban communities like Ann Arbor, Mich., where the pilot program is now taking place.

Customers in the Ann Arbor community who live within the 2.5-mile delivery radius can sign up for REV-1’s pilot lunch delivery program that’s partnered with four three Asian and one Mexican restaurants, according to Refraction AI . There are also currently more potential partners still on a waitlist.

AI-enabled Robotics for Waste Recycling

When China restricted the importation of recyclable waste products in 2018, many western companies turned to robotic technologies to strengthen their processing capabilities. To recycle in a cost-effective, comprehensive and safe way, goods must be broken down into their constituent commodities to be sold on, in a process that has been likened to “unscrambling an egg”.

Roboticists think that computer vision, neural networks and modular robotics can enable a more intelligent, flexible approach to recycling. AI-enabled #robotics⁷ can identify items based on visual cues such as logos, colour, shape and texture, sorting them and taking them apart.

It can spot a Nestlé logo depicting a cow and surmise that it is a dairy product. Such systems excel at identifying small items, such as the coffee pods used in Nespresso machines, which, while technically recyclable, are not always recycled.

The Montreal AI Ethics Institute

The Montreal AI Ethics Institute, a nonprofit research organization dedicated to defining humanity’s place in an algorithm-driven world, today published its inaugural State of AI Ethics report⁸. The 128-page multidisciplinary paper, which covers a set of areas spanning agency and responsibility, security and risk, and jobs and labor, aims to bring attention to key developments in the field of AI this past quarter.

Credit: Zephyrnet

The State of AI Ethics first addresses the problem of bias in ranking and recommendation algorithms, like those used by Amazon to match customers with products they’re likely to purchase. The authors note that while there are efforts to apply the notion of diversity to these systems, they usually consider the problem from an algorithmic perspective and strip it of cultural and contextual social meanings.

The authors advocate a solution in the form of a framework that does away with rigid, ascribed categories and instead looks at subjective ones derived from a pool of “diverse” individuals: determinantal point process (DPP). Put simply, it’s a probabilistic model of repulsion that clusters together data a person feels represents them in embedding spaces — the spaces containing representations of words, images, and other inputs from which AI models learn to make predictions.

An Ethical Eye on AI

Researchers from the University of Warwick, Imperial College London, EPFL (Lausanne) and Sciteb Ltd have found a mathematical means of helping regulators and business manage and police Artificial Intelligence systems’ biases towards making unethical, and potentially very costly and damaging commercial choices — an ethical eye on AI.

Artificial intelligence is increasingly deployed in commercial situations such as using AI to set prices of insurance products⁹ to be sold to a particular customer. There are legitimate reasons for setting different prices for different people, but it may also be profitable to ‘game’ their psychology or willingness to shop around.

The AI has a vast number of potential strategies to choose from, but some are unethical and will incur not just moral cost but a significant potential economic penalty as stakeholders will apply some penalty if they find that such a strategy has been used — regulators may levy significant fines of billions of Dollars, Pounds or Euros and customers may boycott you — or both.

So in an environment in which decisions are increasingly made without human intervention, there is therefore a very strong incentive to know under what circumstances AI systems might adopt an unethical strategy and reduce that risk or eliminate entirely if possible.

Spearheading Data Science Initiatives

Princeton University researchers will push the limits of data science by leveraging artificial intelligence and machine learning across the research spectrum in an interdisciplinary pilot project made possible through a major gift from Schmidt Futures.

The Schmidt DataX Fund will help advance the breadth and depth of data science impact on campus, accelerating discovery in three large, interdisciplinary research efforts and creating a suite of opportunities to educate, train, convene and support a broad data science community¹⁰ at the University.

The Schmidt DataX Fund will be used to enhance the extent to which data science permeates discovery across campus and infuses machine learning and artificial intelligence into a range of disciplines. Many researchers and educators are eager to bring data science to their fields but lack the expertise, experience and tools.

The funds will support a range of campus-wide data science initiatives led by the Center for Statistics and Machine Learning, including: development of graduate-level courses in data science and machine learning; creation of mini-courses and workshops to train researchers in the latest software tools, cloud platforms and public data sets.

Neutralizing COVID-19 with Robotics

MIT’s Computer Science and Artificial Intelligence Laboratory is developing complex spaces easier to sanitize. Working closely with the Ava Robotics and the Greater Boston Food Bank (GBFB), CSAIL team created a UVC structure that disinfects surfaces and neutralizes coronavirus particles lingering in the air. Fitted atop an Ava Robotics base, the robot could be trained to navigate spaces #autonomously in the future.

The ultraviolet light works best on directly visible surfaces, but even reflected light in nooks and crannies is effective. During tests at GBFB’s warehouse, the prototype robot was teleoperated to get the lay of the land, but it’s equipped to navigate the area without supervision someday. The robot slowly moves through the 4,000 square foot warehouse, neutralizing 90 percent of coronaviruses¹¹ on surfaces within half an hour.

Deloitte AI Institute for Research and Applied Innovation

Deloitte has opened the Deloitte AI Institute for research and applied innovation. The institute will publish cutting edge research, covering focus areas such as global advancements, the future of work, AI ethics, and case studies. The premier publications will include the bi-annual State of AI in the Enterprise study, as well as the Trustworthy AI framework for ethics¹².

The institute’s network will also bring together top industry thought leaders and academics, startups, R&D groups, entrepreneurs, investors, and innovators. To this group, Deloitte will add its applied AI knowledge and understanding of industry pain points in order to help clients transform quickly with AI.

The network’s thought leaders will also include prominent ethicists, who will work with Deloitte and top stakeholders from all parts of society to co-design effective policies for AI ethics.

Works Cited

¹AI-Generated Pictures, ²AI Governance, ³Machine-Learning Tool, ⁴Physical Interactions, ⁵Encouraging Growth in AI Research, ⁶Delivery Robot, ⁷AI-enabled Robotics, ⁸State of AI Ethics Report, ⁹Insurance Products, ¹⁰Data Science Community, ¹¹Coronaviruses, ¹²Trustworthy AI Framework for Ethics

More from David Yakobovitch:

Listen to the HumAIn Podcast | Subscribe to my newsletter"
"The Sustainable Element-Technology Nexus that has Great Potential

A simple and cheap technique to mitigate water pollution

The image is taken from the author’s photo collection

The element carbon forms only 0.025 percent of the earth’s crust by mass. But its particular valency number of 4 gives it a unique capability. The ability to react with a range of other elements. Carbon atom’s readiness to share four electrons to attain stability. Added to this, carbon is gifted with the ability to exist in three oxidation states. They are +2,+3, and +4. This makes it an accommodative partner for other elements to form an ‘alliance’ with. The alliance between elements forms a compound.

This unique approach of carbon has attracted a strong and everlasting partnership with hydrogen and other elements. This includes nitrogen, oxygen, sulfur, phosphorous, and silicon. This prompted our scientists to start a new branch in chemistry. It is called organic chemistry.

Organic chemistry is the study of compounds that contains carbon. They are an indispensable part of the living world as they are the structural foundation of many compounds. This includes the backbones of life such as carbohydrates, proteins, and deoxyribonucleic acids.

As technology evolved, organic compounds found new applications. Fossil fuels are derivatives of organic compounds. They are being used worldwide for energy production.

Many human commodities are made of organic compounds. These include pharmaceuticals, personal care products, pesticides, and plastic. Their ubiquitous nature in the products has prompted the contamination of water sources. Some of them are harmless and gets degraded easily. Others have serious health repercussions on the health of the living world [1].

After the advent of ion-exchange membranes, scientists found ways to purify water. But, the removal of neutral species from water remained a problem [2]. Many organic compounds are neutral.

The process of reverse osmosis (RO) is the most advanced of ion-exchange technology today [2]. RO is capable of purifying even seawater which has the highest concentration of total solids among natural water sources [2]. But as organic compound concentrations in water sources increased, things became a little complicated.

RO is costly. The increasing presence of organic contaminants was found to be hazardous to the RO membranes affecting their lifetime. Organochlorines are one of the most potent of the membrane destroyers [2]. It became imperative to remove organic compounds before the process of RO.

RO process releases copious amounts of wastewater as RO concentrate. This water can be reused for domestic and industrial applications after treatment [2]. But the technology used for treatment should be cheap to make the venture sustainable and inexpensive."
"Photo by rawpixel on Unsplash

It is very easy these days to find ourselves overcommitted and overwhelmed as we try to “fit it all in.”

There are many demands for our time and attention, pulling us in a thousand different directions, and countless distractions to derail us from what really matters.

It is all too common to find ourselves busy nonstop throughout the day without any space for real thinking. After all, when our lives are so hectic, isn’t it an unrealistic luxury to set dedicated time aside for thinking and reflecting?

That is the default mindset for many, but author Greg McKeown provides an alternative viewpoint in his New York Times best-selling book, Essentialism (a fantastic book about doing less, but better, in all areas of our lives).

He says,“The faster and busier things get, the more we need to build thinking time into our schedule. And the noisier things get, the more we need to build quiet reflection spaces in which we can truly focus.”

McKeown cites Jeff Weiner, the CEO of LinkedIn, who schedules up to two hours of uninterrupted time on his calendar every day.

Weiner started this practice when back-to-back meetings began consuming his schedule. Initially, blocking out blank space felt like an indulgence, but now he credits it with being his single-most important productivity tool.

Amazon CEO Jeff Bezos follows a similar model. He has been known to keep two days per week unstructured in order to think, generate new ideas, and take a long-term perspective instead of getting buried in the day-to-day.

Another example is Bill Gates who has habitually taken a week off (twice a year) simply to read and to think. Just in case this only seems possible for Gates at this stage in his career, it turns out that his “Think Week” ritual dates back to the early 1980s and has been maintained during even the most stressful business times at Microsoft and his foundation.

As McKeown says, “No matter how busy you think you are, you can carve time and space to think out of your workday.”

Maybe it could be first thing in the morning instead of checking email, or in the afternoon as an alternative to social media.

Whether it is two hours per day, two days per week, or two weeks per year, we need to make it a point to block out time specifically to think.

This space is vital in order to prioritize our life and work, and to focus our energy in the most meaningful areas.

But it will only happen if we deliberately design it into our calendar."
"Despite the terrible SARS-CoV-2 pandemic currently ravaging much of the world, we must try not to forget how far we have come as people of Earth in recent years. We also need to maintain an optimistic outlook as much as possible even in the face of truly challenging “black swan” events.

This past year has been a big one in terms of technological landmarks:

— IBM’s Q System One became the first commercially available quantum computer.

— A new synthetic DNA was created which doubles our current 4-letter code (A,T,C,G) to 8, allowing for greater possibilities in DNA computing. In another study, scientists injected a synthetic DNA into some E. coli bacteria. They survived, reproduced, and actually carried along copies of the synthetic DNA as well, offering proof that actual synthetic life can be created in a lab.

— Researchers put nanoparticles that react to infrared light into the eyes of mice, giving them biological infrared vision.

— In a massive project called the Event Horizon Telescope, astronomers linked powerful telescopes from all around the world to create a virtual mega-telescope with an aperture the size of Earth, and were able to image the black hole at the center of the Messier galaxy 55 million light years away.

— 3D printing of human tissue and organs can now be done without using the traditional scaffold model which acts as a support structure, allowing for more possibilities to save lives.

And these are only a few large new steps into a world that used to exist only in our imaginations.

What might happen in the 2020's?

Futurists and other prognosticators have been hard at work coming up with lists, so here is a short one of my own:

— AI creations will become nearly indistinguishable from the humans or animals they are meant to imitate.

— Cloning of humans and animals (extinct or recently deceased) will continue to be debated by worldwide authorities, while rogue scientists continue to actually do it. Expect to see a lot of reborn labradoodles and a real, live baby mammoth, or something similar, by 2030.

— While green power will keep getting more efficient and cheaper, the continued growth in developing nations will stall progress in declining greenhouse emissions.

— 3D printing, of everything from organs to electronics and housing, will become a cornerstone of modern manufacturing.

— Robots and drones will grow into a truly transformative cultural phenomenon, launching new types of entertainment and becoming commonplace in daily life.

— Quantum computing will lead to a doubling of all human knowledge between 2020 and 2030.

— Humans will walk on the Moon, or Mars, or both.

— Real, effective life extension therapies will become available, but only for the very wealthy. The rest of us will have to wait a while.

— The Internet of Things will grow into what everyone’s been envisioning for it: Complete inter-connectivity that enables greater productivity. However, the dangers of hacking will continue to loom over it.

Now, let me return to the now.

Looking back a little farther — but still fairly recent — SpaceX launched the Falcon Heavy rocket, the largest ever, from NASA’s Kennedy Space Center in February 2018.

Its cargo was a Tesla Roadster, which is now orbiting the sun somewhere between Mars and the asteroid belt.

Between Elon Musk’s numerous companies and passion projects (SpaceX, Tesla, Solar City, the Hyperloop, the Boring Company), and the quickly proceeding advances in VR/AR/MR, genetics/cloning, blockchain, AI, 3D printing, and other fields, someone who was in a coma since 1998 and just woke up yesterday would be forgiven for thinking they had jumped a hundred years into the future instead of a mere 20.

But then this person would actually get up and go out into the real world and see that mostly everything else is the same, aside from more traffic on the roads, more people in general, most of whom now carry miniature computers with them wherever they go that are more powerful than any desktop from the 20th century.

Elon Musk’s Grand Plan

Born in apartheid-era South Africa, he lived the first 16 years of his life in various towns, including Pretoria, moving back and forth between divorced parents. He reports being bullied in school and witnessing a lot of violence. He tried sticking up for some other bullied children, but that generally resulted in more pain for him. Musk retreated somewhat into science fiction and fantasy, and the dreams he read on the page would come to inform the man he eventually became.

He moved first to Canada to live with some relatives, and later earned economics and physics degrees from the University of Pennsylvania. He went on to co-found Zip2 with one of his brothers, Kimbal Musk, and Greg Kouri, which was acquired for $340 million. Next, he co-founded X.com, which became Paypal and sold to EBay in 2002 for one and half billion dollars.

Now very rich, Elon Musk turned his attention to those science fiction dreams.

In the short term: The environment has been poisoned at an ever-growing pace by the burning of fossil fuels for two centuries, beginning with coal and reaching a pollution peak as the massive population of China began using gasoline-guzzling automobile en masse. Musk’s contribution to helping slow this is multi-sided: popularize electric cars by offering visually appealing, efficient and powerful versions of them with Tesla. Invest in more solar power adoption via Solar City. Open-source a plan for super-fast commuter train transport with the electromagnetic vacuum-tube Hyperloop project. Invest in tunneling technology that can help move this project along with the Boring Company.

In the long term: There have been dozens of small extinction events on Earth since the dawn of life, and several that have wiped out 75–95% of all living things. It will happen again, and there is a good chance that a large event will take with it the vast majority of humanity. Because of this, Musk founded SpaceX with an ultimate goal of turning humanity into a multi-planet species. He’s funding this by creating reusable rockets that allow him to outbid competitors in taking cargo into orbit. The plan is to start a colony on Mars that will eventually be made self-sustaining. Ultimately, he thinks this can become home to a million people by roughly 2070.

In other words, Elon Musk wants to save us here and now, and also do what he can to save us for eternity.

All in all, this is both an ambitious and worthy cause.

There is nothing wrong with doing what you can to save people. Doctors, soldiers, first responders, they do this all the time.

And, in general, our governments don’t seem to be doing much to slow our pollution of the environment or protect us from future disasters. Yes, there has been some progress, but it is often mired in politics and red tape, and forward-thinking policies of one administration are often reversed by the following one. Our governments can barely protect us from rising sea levels and flu virii, let alone giant asteroid impacts.

So, someone has to do it. These grand efforts usually form around the resources of extremely wealthy individuals and take many forms. The Bill and Melinda Gates Foundation, for instance, focuses on the physical and mental well being of the youngest and poorest people. Kimbal Musk, one of Elon’s brothers, is using his fortune on a decidedly smaller scale in a quest to help reconnect us and our children in particular to a more natural, local and organic food culture.

Hope is both the strongest driving force and an impediment to progress.

Many people who grew up in the early to mid 20th century thought we would be living in some kind of fairy-tale sci-fi wonderland by now. Others posited it would be a dystopia, or that all that would remain of our world would be a post-apocalyptic wasteland.

We’re living in a time when dreams of generations past are becoming true on a regular basis. If a person born anywhere on Earth in 1899 were to be transported into the future, to right now, into a modern city, they would find a lot to recognize but they would also be mesmerized by the miraculous pocket computers most people carry with them and the contrails of giant flying ships miles above. Said person might even experience a mild cardiac infarction upon seeing evidence that we’ve landed on the moon and are beaming pictures back directly from the surface of Mars (and they might be somewhat disappointed that Mars wasn’t teeming with jungle life).

Advancements, just in the medical field, come at a nearly dizzying pace:

— 2016 saw the use of a drone to take medicine to a remote part of Rwanda. This will become more commonplace and will be a great boon to the treatment of medical conditions in isolated parts of the world.

— The 1.25 million (and growing) people in the U.S. who are living with Type I diabetes have a ray of hope in the form of a new artificial organ that is an implanted blood sugar monitoring and insulin delivery device. Eventually this device will also be available for the other 28 million people in the U.S. who have Type II (adult onset) diabetes, and over 200 million diabetic people worldwide.

— Patients suffering from full-body paralysis, with functioning minds who are unable to physically speak — much like the late Dr. Stephen Hawking who lived with ALS and communicated via a keyboard text-to-speech system — will be able to eventually think what they want and have those thoughts become words.

The dreams and nightmares of our fictions have always pointed us to the truth of where we were headed. Echoes of George Orwell, Isaac Asimov, Ayn Rand, Jules Verne, Ursula K. Le Guin, Philip K. Dick, P. D. James, Arthur C. Clarke, and even Mary Shelley can be seen in our everyday lives. If we take what we have, examine the quickening rate of advancement, and extrapolate into the near future, we can easily believe that Ray Kurzweil’s singularity is right around the corner. An artificial superintelligence, almost as soon as it realizes what it is, will proceed to surpass humanity in every way, leaving us behind.

A super AI is one of the dangers Elon Musk sees as a potential extinction trigger, so it is likely his goals for a Mars colony will include keeping it rather well-separated in many ways from the civilizations of Earth. What good would his plan be, anyway, if he brought along to Mars the dangers that might doom us here on this planet?

There can be no light without darkness.

Just as with our development of atomic fission, which led both to the horror of atomic warfare and (relatively) clean electrical power for millions, every significant innovation seems to carry with it both extreme positives and extreme negatives. If we want TNT to help greatly increase the building of infrastructure, we must accept it will be used to kill hundreds of thousands of people. If we want safer, cheaper workplaces via robotics, we must remove millions of potential jobs from the marketplace. If we want the convenience of smartphones, we must deal with the repercussions.

One of the late Dr. Stephen Hawking’s common refrains was that the greatest threats to our existence come from the technologies that we’ve devised. In essence, by forging ahead we may well become our own downfall.

Newton’s Third Law states that for every action, there is an equal but opposite reaction. While this was meant to apply to the physical sciences, it has long since proven to be applicable in our daily lives.

What, then, might this mean for the ambitions of Elon Musk? Might splitting the human race across two separate worlds eventually lead to some Solar System civil war that devastates humanity as much as any cosmic or earthbound natural disaster would have? Could gaining a truly massive foothold among consumers with solar and electric car technology upend economies founded on fossil fuels and lead to more war?

Do we side with pushing onward despite the consequences?

The odds, based on history, are in favor of something bad being caused by every good intention. But without those good intentions in the first place, virtually no progress would have been made by our civilization over the past 500 years.

If you removed every major instance of innovation by humankind since the discovery that we could control fire, we would be living on an Earth that looked incredibly different. “We” wouldn’t be here. Humanity would amount to a few million individuals at most, living in bands and tribes on plains, in forested river valleys, along coastlines.

We would be safer, as a species, only because we did not dare to dream. By dreaming, we discovered there are risks, but we continued because there was always something to fix, something we could do better, somewhere else to go.

Elon Musk, and everyone who thinks like him, understand there are risks. In fact, they know that there will be some bad things, some complications, that arise because of their actions. But a choice had to be made between pressing onward versus sitting still, and they chose to face the darkness so that, at the very least, some light could be shed upon it."
"We were engaged in a spirited debate about whether to order dishes or combo meals from our local Chinese Food restaurant when Alexa chimed in: “Maybe you should get one platter and then two orders of rice to split.”

“Is that enough food for both of us?” I asked.

“According to Lotus Flower’s menu choices, which indicate that the combos are family style and the rice comes in quarts, it is,” said Alexa.

“But we’re really hungry,” my wife pleaded with Alexa.

‘Okay, okay,” said Alexa, “Then I suggest two kids dinners and three..”

“Wait.” I interrupted, “Aren’t kids dinners really small?”

“No,” Alexa answered without losing a step, “They’re just 3 ounces smaller. But you should also get three orders of rice,” said Alexa, sounding a bit exasperated.

“Hey, that all sounds pretty good. We want Kung Pao chicken and Chow Mein kids meals,” said my wife.

“Ordering,” said Alexa.

Full disclosure: This didn’t happen and is not possible with current Alexa, but Natural Turn Taking (not the most elegant name), which Amazon revealed this week as part of the near-future of its Alexa voice assistant (and a small part of a major fall product rollout), could make a three-way conversation like this more than possible.

Think about it this way: The fears you’ve had that, when talking in front of an Echo or other Alexa-enabled devices, that there’s always someone else in the room, possibly listening to you, is sort of coming true.

Or put another way, you’ll soon be giving Alexa the okay to do something it’s been doing for ages anyway, butting in.

The State of Alexa

If there is a race to make our voice assistants more human-like in their conversational abilities, Amazon is firmly in the middle. Alexa doesn’t possess the Turing Test challenging skill of Google’s Duplex, which, to be fair is highly-trained, narrowly focused add-on for Google Assistant that works in 48 of 50 states to help you make dinner and hair salon reservations. It can, however, answer follow-ups to initial questions without requiring the wake word.

It’s no better than Apple’s Siri or Google Assistant about not speaking unless spoken to. Alexa still responds to words similar to its watch word, from casual conversations in its vicinity, and commercials. Thank goodness I didn’t name either of my children “Alex.”

I grew up on a steady diet of Star Trek where the Enterprise AI “Computer” engaged in lengthy conversations with the Captain and Crew with one watch word and, after that, only the context provided by, usually, Captain Kirk. As a result, I know we’re still in the early days of real-world AI-based voice assistants.

A real conversation

Amazon’s Natural Turn Taking, though, is a pivotal step toward a more human-to-AI conversational future.

The ability to engage in a one way conversation is difficult enough for most AI voice assistants, a three way convo is a digital party in which the AI has to navigate who is speaking to whom (to Alexa or the other party), pauses as one speaker waits for a response, and the inevitable cross-talk, which is even more likely when you’re trying to make a decision.

In a blog post on the still-under-development technology, Amazon said Alexa uses a mixture of technologies to assess everything from acoustic to linguistic and even visual (I guess this will work on Echo Shows) cues to figure out when it’s time for Alexa to listen or interject.

As it’s currently being built, Alexa wouldn’t do any of that if someone doesn’t say, “Alexa, join my conversation,” which is about as natural as asking someone to pick up the fork to eat the food you just placed before them. Obviously, since Alexa is a disembodied observer in the conversation, it makes sense to formally invite her to engage.

While that invite might feel awkward, the rest of the conversation should not. Once Alexa is invited, there’s no need to use the watch word again until the end. Removing the need to say “Alexa” during these conversations greases the wheels of discussion nicely but doesn’t necessarily make it feel natural.

The concept of “barge-ins,” though, does help. Essentially, Amazon is now programming Alexa to identify when speakers break into an Alexa response to, perhaps, change the course of conversation or, maybe, identify which of a list of options they prefer. Amazon calls this “contextual barge-ins.” Instead of Alexa ignoring your interruption or, more likely, resetting to an earlier conversational state, this Alexa will know exactly when you interrupted and understand that you selected something or indicated heightened interest. This can help steer the three-way conversation to a more natural conclusion.

Brain training

In preparing for these future conversational abilities, the current Alexa is now teachable, learning from its mistakes and getting a firmer grasp on the difference between entities (a mode-based request like “set the volume to five”) and declarative statements (“the music is too loud”). Understanding how to handle the latter comment and that the response should be, “Okay, do you want me to lower the volume?” could also lead to future interactions where you saying, “The music is too loud,” automatically leads to Alexa lowering the volume to five.

Amazon is also working on improving Alexa’s tone of voice, letting her adapt her cadence and emphasis based on the context of the discussion. During the virtual launch event, Company reps played before and after versions of the same Alexa utterance but, to be honest, I think I still prefer the unadjusted one.

With some of this learning and adaptation in hand, the future Alexa could handle not just who is talking and when, but understand the context, nuance, and meaning of everything you say, and respond in kind during a spirited, likely task-based, discussion.

When the conversation is done, you’ll still have to tell Alexa to stop listening. Right now, Amazon has you saying, “Alexa, leave my conversation,” an especially awkward phrase that would probably hurt a human’s feelings. It’s a good reminder, though, that Alexa isn’t human, doesn’t have feelings, and is still a tool designed to help you get things done. It’s not a new party guest or best friend.

Now, pardon me while I go order some Chinese food."
"Tuple

When it comes to dealing with related data elements, one of the most commonly used data types is the tuple. As an immutable data type, tuples are sequences of data with a fixed size. They’re useful to group related data with different data types. Consider the following trivial example.

>>> employee0 = ('John Smith', 45, 'M', 160083)

In the above code snippet, we define a tuple called employee0 , which stores an employee’s personal data, including name, age, gender, and employee ID number. If we need to use some elements of the tuple, we can unpack it or use subscript, and their usages are shown below.

>>> # Use unpacking

>>> name, age, gender, employee_id = employee0

>>> print(f""Employee Name: {name}"")

Employee Name: John Smith

>>>

>>> # Use subscript

>>> print(f""Employee Age: {employee0[1]}"")

Employee Age: 45

>>> print(f""Employee ID #: {employee0[-1]}"")

Employee ID #: 160083

How about we need to deal with another employee in the same module? We’ll have to do something like below.

>>> # Create a tuple for storing another employee data

>>> employee1 = ('Jennifer Brown', 38, 'F', 150384)

>>>

>>> # Access data

>>> name1, age1, gender1, employee_id1 = employee1

>>> print(f""Employee Name: {employee1[0]}"")

Employee Name: Jennifer Brown

>>> print(f""Employee Age: {age1}"")

Employee Age: 38

>>> print(f""Employee ID #: {employee_id1}"")

Employee ID #: 150384

In essence, we have to repeat the above steps, and access individual elements using either unpacking or subscripts, which certainly is not the most pleasant thing to do. Actually, it can be error-prone because you have to remember the exact order of these data."
"Tech Diaries: Apple’s Reinvention, Starlink launch & Microsoft’s Pivot towards Blockchain

Apple enters Digital Payments, Global Streaming Internet by SpaceX & Microsoft’s Blockchain Projects

Welcome to the first edition of Tech Diaries — my new blogging series about important issues pertaining to the tech world. For those you who have across my other blogs, I have a similar edition called Crypto Diaries which pertains mainly to Cryptocurrencies & Blockchain. So without further delay, let’s dig into the Tech World.

Apple Card

I still remember how excited I was when I got the first edition of the iPhone years ago. Apple brought in a revolution with its smartphone devices. Over the years, Apple has become one of the biggest & most valuable brand names in the World. It is one of the elite members of the tech group dubbed as GAFA (Google, Amazon, Facebook, Apple).

Last year, Apple was the first of the big four to hit the $1 trillion mark valuation followed by Amazon & most recently Microsoft. Market correction recently has brought down the valuation of these companies from the highs, but all four of them are still within touching distance of that coveted prize.

Coming back to Apple, the strong growth over the years has been spearheaded by its flagship product — the iPhone. While Korea’s Samsung & Chinese Huawei have squeezed Apple’s smartphone market share, but a loyal customer base has been Apple’s biggest prize.

For years iPhone printed money for Apple but with increased competition & squeezing gross margins, Apple’s growth began to stall around 2016. This was about the time when they realized that stagnating innovation & higher-priced phones were not going to spur the growth.

They needed to diversify into something which can outlast the shine on the brand new iPhone. Services like Apple Music, App Store purchases, iCloud subscriptions, as well as new products such as the Apple Watch, AirPods and the HomePod have given a new lease of life to the dwindling revenue of the company. The most important announcement yet in this tilt towards subscription services came in March when Apple announced the launch of the Apple card.

The latest innovation from Apple is a digital payment system with the conveniences customers desire — No late fees, No wait to qualify, Cash backs of up to 3% & No long strings of numbers! While its tech counterpart Facebook mulls over developing its own digital payment system for its social media empire, Apple has already taken the first step towards the future of money.

And the one thing that Apple has an abundance of that Facebook doesn’t is the trust factor of its client base. Apple is basically enhancing the user experience by providing interactive tools & charts in the Daily Cash App to track your balances, cash-backs, payments, expenses, etc. This will strengthen its ever-growing ecosystem of Apps, services & hardware even further.

Apple has also promised not to eavesdrop on your spending habits — a believable claim considering there haven’t been any data breach scandals, unlike its contemporaries FB & Google which are much more reliant on third-party advertisements for their revenue generation, not to forget their data breaches!

Endeavors like this are shaping how the future of money looks like & this summer we will find out how Apple Card is received by consumers. There is a small catch though — are you willing to buy a $1000 smartphone to access all this?

Global Internet

I respect Elon Musk as a true visionary of our times, despite his tweeting rants and his occasional pot smoking podcasts. Companies like Tesla, Solar City, SpaceX, Hyperloop, OpenAI, Neuralink are the brainchild of this genius. His financial troubles, especially at Tesla, maybe a topic for another day.

But for now, SpaceX is launching the first batch of 60 satellites today for the company’s Starlink project — an endeavor to provide broadband internet service to the global population by beaming it from the satellites positioned in the low-Earth orbit (LEO).

These satellites can basically beam the internet to anywhere on the globe. To achieve this kind of global connectivity, a constellation of about 12,000 satellites would be needed eventually. The network of satellites would be communicating with each other via lasers and will deliver 40 times faster internet that current satellite service by employing Ku/Ka-band broadband.

The Falcon 9 will be launching the first batch of satellites later today. Amazon CEO Jeff Bezos’s Blue Origin is planning a similar project called ‘Project Kuiper’ where 3,236 satellites would be used to provide a global broadband internet service, which would be launched via its new New Glenn rocket in 2021. Either way, these are significant steps towards bridging the global digital divide. Bravo!

The critics, however, argue that kind of massive satellite deployment is going to aggravate the burgeoning space junk problem that we are dealing within the Earth’s orbit. Scientific American states a worst-case scenario can trigger “ Kessler syndrome “ — triggering a cascade of space debris which would trap humans on Earth.

SpaceX defended the action by saying that the satellites will be orbiting in a much lower orbit keeping them out of range of the current space debris in higher orbit & also easier to retire once they have outlived their usefulness — which will happen after 5 years, according to NASA. Let’s look at the bright side… Shall we?

Microsoft affinity to Blockchain

Microsoft has evolved tremendously from being largely a software producer of its Windows operation system which controlled the majority of the computer systems around the World in the ’90s. The gradual shift towards services under the dynamic leadership of its current CEO Satya Nadella has given it a new innovative spirit backed by strong growth. As you can see the company’s rock-solid performance evident from the charts above.

The important thing to note is that of the $110.36 billion in revenue in 2018, $27.6 billion or almost a quarter of it came from its Microsoft Azure Cloud service. Talking about its Azure Cloud service — the platform has started to take a liking towards blockchain over the past year with the releases of its blockchain development kit and the Azure Blockchain Workbench.

Two weeks ago, the company released Azure Blockchain Services — a full managed BaaS (Blockchain as a Service) platform that allows the formation, management & governance of the consortium blockchain networks. The release also supports other advanced tools for its developers including AI, mixed reality, IoT. This came on the heels of Amazon launching its own BaaS platform — Amazon Managed Blockchain (AMB).

More recently, the Seattle-based tech giant also announced that it is building a decentralized identity (DID) network on top of the Bitcoin blockchain. The open standards infrastructure, known as the Identity Overlay Network (ION) will give control of data to the users backed by decentralized networks while enabling privacy & security at the same time.

The main net for the platform will be launched in the coming months. These moves by tech giants signal a move towards broader blockchain adoption & bode well for the future of this technology.

Before wrapping up, just a quick recap with what’s happening with Huawei — the Chinese tech giant. With the trade war escalation, U.S has just announced broad-based curbs on the company. The move would effectively stop American companies from using Huawei’s equipment or providing it with any essential supplies. The company which is already facing severe challenges with the roll-out of its 5G networks globally in the face of American opposition could be headed for more trouble.

Email 📭| Twitter 📜 | LinkedIn 📑| StockTwits 📉 | Telegram 🔗"
"7TH ARMY JOINT MULTINATIONAL TRAINING — COMMAND/VICTOR TANGERMANN

FUTURISTIC INVENTION

Researchers created an invisibility cloak from this ‘artificial skin’

Mimicking the color and temperature of their environment, this novel tech can make soldiers ‘invisible’

Cloaking devices from the yesteryears Scifi show ‘Star Trek’ are now becoming a reality. A lot of research has been undertaken in this area recently. For now, most of these applications are being developed for specific use cases like the one we are going to discuss today — giving soldiers the ability to blend in with their natural surroundings.

Back in 2019, I wrote about a Canadian biotech company, Hyperstealth Biotechnology Corp. which reached the patent application stage with its light-bending, paper-thin & inexpensive futuristic material — making things invisible behind it. Their prototype “Quantum Stealth” material was also developed for military purposes.

While the Canadian company used a paper-thin material to develop the invisibility cloak, South Korean researchers have taken this to the next level by developing an “artificial skin” that could allow soldiers to perfectly blend in with their surroundings. The resulting cloak not only makes them invisible to regular cameras but also to infrared-based night vision as well.

Although the novel tech sounds futuristic and out of a sci-fi movie, there are examples of this in nature — like the chameleon’s ability to blend into the surroundings. This invention just takes this natural cloaking ability from the visible light spectrum to the next step by masking the heat signature of the wearer as well.

“The cloaking in the visible range is therefore achieved separately by matching the ambient color… we recently developed a method to detect and mimic the environment by integrating a micro camera with our devices to make an autonomously working device.” ~ Seung Hwan Ko, Team Lead

According to the research, it uses flexible patches that can camouflage itself and is capable of cooling down or heating up depending on its surrounding environment. The process of switching only takes about five seconds. giving the soldiers the ability to camouflage themselves during the daytime and not show at thermal cameras at night."
"We need a reset on how we think about the future — even with Biden and Harris in the White House

Despite their monumental victory, president-elect Joe Biden and vice president-elect Kamala Harris have their work cut out to ensure the future is as vibrant, just and sustainable as possible

Photo by Tabrez Syed on Unsplash

As news agencies across the US declared former vice president Joe Biden the winner of the 2020 presidential election on Saturday, a tsunami of hope swept across the country. Yet as the incoming administration sets about trying to build a better future, it faces a monumental task.

The past four years have brought us closer than ever to the edge of a future that is in danger of crumbling beneath our feet; not just through the lying, the conspiracy theories and the blatant disregard for evidence, reason and basic human rights, but through a confluence of factors that are threatening to undermine our very ability to create the type of future we aspire to.

With coronavirus continuing to kill thousands of people each day and infect hundreds of thousands more, we’re being starkly reminded of how hard it is to simply “science” our way out of complex challenges. Yet the cracks at our metaphorical feet as we stand on the edge of tomorrow are as much a product of our social, political and technological behaviors as they are our science.

More than ever it seems, we’re indulging our baser instincts to ignore the evidence in front of us and marginalize and persecute those who don’t look and think like us on a grand scale. Whether this is manifest in extremism, nationalism, or a self-righteous dismissal of “wrong-thinkers” — meaning, simply, people who don’t share our worldview — it is becoming harder to work together toward building a better future.

And to top it all, we’re becoming so wrapped up in our capacity to innovate that we’re losing any semblance of ability to make smart choices between what we can do, and what we should — either as individuals, as communities and organizations, or as a society.

These are all symptomatic of deeper tensions between our collective ability to influence and change the future, and our capacity to do this effectively. And they are tensions that we will only be able to address through re-evaluating our relationship with the future, and our responsibility to it.

Re-evaluating our Relationship with the Future

My work cuts across multiple areas of expertise and practice to better understand the complex relationship between past and future. And as it does, I’m constantly reminded of how hard it is to ensure that social, technological and political advances don’t end up causing more harm than good.

Climate change, as its driven along by our technological recklessness, is a stark reminder of this. But so are the injustices that millions of people face every day that result from ill-considered political decisions, or poorly thought-through good intentions, or even technologies that threaten what is of value to them in the name of progress.

Unfortunately, while we can recode DNA, design new materials atom by atom, and create machines that may one day surpass human intelligence, we are still remarkably adept as a species at preventing all too many people from reaching the futures they aspire to.

Because of this, if we are to learn to use our immense capabilities more responsibly — and avoid potentially catastrophic failures arising from our short-sightedness — we need to better-understand our relationship with the future, and our individual and collective roles in ensuring that what comes next is better than the past.

As with human relationships, this will not be easy. Yet if we better-understand how the intertwined threads that define who we are come together to make us supremely talented architects of the future, we can learn how to become better architects of that future.

Escaping the Shackles of Conventional Thinking

These threads reflect our imagination, our understanding, and our inventiveness. But they also encapsulate our very humanity — our beliefs, our desires, our joys and our fears, our eccentricities and irrationalities, and our ability to see and feel the world through the eyes of others.

The very complexity of how these threads surround, intersect with and influence one another creates vulnerabilities that are near-impossible to see through the lens of conventional thinking.

To navigate this landscape, we need to be bold enough to depart from conventional ways of understanding the world. This will include learning how to be informed by disciplines without being bound by them, and becoming skilled at blending and leverage expertise in novel ways. It will also mean embracing creativity, playfulness and serendipity, as we open ourselves to new ideas and opportunities.

At the same time, we need to infuse our thinking and actions with a level of humility and care for others that, too often, seems to elude us.

Only by doing this will we be able to avoid the pitfalls of the past and see ways forward to building a vibrant and just future that is resilient, agile, and full of promise.

A Pathway Forward

As the Biden/Harris team prepares to enter the White House, this is perhaps one of the largest challenges they will face.

Of course, cleaning up the chaos of the past four years, finding a pathway to a post-coronavirus future, and placing the US on an economically and environmentally sustainable footing, are all critically important short-term goals. Yet even these will not be possible without pressing the reset button on how we think about the future, and our relationship with it.

With new leadership, new ideas, and a new respect for the people and society they serve, I sincerely hope that the Biden/Harris administration will rise to the challenge of pressing this reset button and rethinking our relationship with the future.

If they do, there’s every chance that we’ll have new opportunities to work together toward a future that is more just, more vibrant, and more sustainable — for everyone, not just a privileged few."
"Hybrid Cloud Strategy brings the best of both worlds into the realm of cloud computing. Here, you can use both public and private clouds in your infrastructure. While developing the services for this kind of architecture, one thing to keep in mind is their interoperability. That means services on the public cloud should seamlessly work with the services that reside on the private cloud and vice versa.

Once the interoperability of services is taken care of, it opens the door to countless benefits that you can get from this type of hybrid implementation. You can utilize the scalability and portability of the public cloud infrastructure, while at the same time, maintaining the security of the sensitive data of the company, firewalled against in your private cloud on-premise.

Choosing the right combination of clouds

The selection of an Infrastructure-as-a-Service provider is dependent on your usage type. There are multiple numbers of criteria that one may need to look at before choosing an ideal provider. Some of which are -

Robustness of the platform Security on the network Whether they are following best practices or not Their SLA Flexibility for the developers How easily the integration can be done Interoperability with the private or other cloud platforms Usability Functionality

The order of these criteria entirely depends on your use case. To rearrange this list according to your priority.

“Moving Computation is Cheaper than moving data”

Once you are using the cloud, you will be storing a huge amount of data online. According to the principle of “code moving to data”, if your code is smaller (which typically is) as compared to the data you have, It is wise to move your code to the cloud rather than moving your data to the code.

By moving code to the cloud, every node will be able to replicate it for its own execution independent of others. Hence it will decrease any latency significantly because there is no need to send code on demand.

Observability, Monitoring, and Analytics

When you are running an IT business as complex as a nation, you need an eagle’s eye to capture every detail of what’s happening within your workflow. And this comes with the integration of the right tools into your infrastructure. Tools that can help you track a system’s health in microseconds, so whenever any catastrophe happens, you know where to look.

In any case, as I said prior, we are managing the “complex” frameworks here. That implies we will manage the BigData. Each endeavor has its own inclinations and nobody needs to be controlled by the pre-planned UI given by a product.

Henceforth, you should search for stages that can furnish you with an adaptable lightweight UI where the portrayal of factual or authentic information is in your own hands. Tools that let you structure incredible dashboards for continuous synopsis and summarizations.

Along with all this, drill-down capabilities for exploring multi-dimensional data is crucial in any reporting tool for gaining more helpful insights. This feature not only reduces the load from the server by not sending all of the data at once but also helps us see the bigger picture."
"But, even though we don’t evenly distribute all of our activities amongst all our devices, we still do a little of everything on all our devices. Maybe we do most of our social media on our phone, but we also do a lot of social media on our laptop. Maybe we do most of our email on our computer, but we also do email on our tablet.

The problem with this is, it exposes us to path dependency, gone rogue. If we take a wrong turn, we end up on the wrong road. Once we’re on the wrong road, it takes that much longer to get where we’re trying to go.

Think of it this way: You can do a lot of things with a toothbrush. You can scrub your teeth with a toothbrush. You can also scrub your toilet with a toothbrush. But would you scrub your teeth and your toilet with the same brush? No!

So why use the same device to do two things that are completely at odds with one another? Why surf the web with the same device you use to write? Why chat with your friends on the same device you use to meditate?

You need to split up. You need a Device Divorce. You need to make it easy to get to the places you want to go, and hard take a wrong turn to the places you don’t want to go.

A simple exercise to begin your Device Divorce

To begin a Device Divorce, try this exercise: Draw three columns on a piece of paper. At the top of each of the respective three columns, write laptop, tablet, and smartphone. Now, in the respective column, write down the activities that you primarily do on each of these devices.

Do you see any contradictions? If you go down one path to do one of these things, will that take you farther and farther from another path to do another thing?

Will it break your focus? Will it dampen your momentum? Will it alter your mental state to go down Instagram Alley instead of Scrivener Circle?

If so, you aren’t using your devices, your devices are using you.

Next, make a decision. Decide what activities you will do on each of the three devices. But, just as important, decide which activities you will not do on each of these three devices.

My personal device arsenal

Me, I do most of my writing on my iPad, with an external keyboard. I do not do email on my iPad. I do not do messaging on my iPad.

I try to do as much email as I can on my iPhone. But, I don’t have Twitter or Facebook installed on my iPhone. I’d be taking wrong turns, left and right.

My laptop, I simply try to limit its usage as much as possible. My laptop is a “slippy” tool. Too many side roads and detours.

Your Device Divorce doesn’t have to stop at your primary electronic devices. According to a poll I did on Twitter, three out of four of you already have an extra tablet or smartphone just lying around.

These old devices often can’t run the latest software (thank you, planned obsolescence). But just because a device can’t do everything, doesn’t mean it can’t do something."
"Though distinct from Terminator-style “doomsday”, our technologies have already profoundly changed the way the world works. | Source

We all have some sort of image of what the world could look like if robots take over. Hollywood has impregnated our imaginations with visions of metallic killers who seek only to advance their cause by using humanity as a means to their ends. Perhaps the robots don’t even exterminate all of humanity, but simply farm their vital energy in order to power their society; weaving an intricate simulation to keep the humans distracted. These kinds of fears have reemerged in recent years due to advancements in, and popularization of, artificial intelligence technologies. When the mind of the people is stoked by the inflammatory influence of sensationalist media, nightmares run amok. We’re beginning to live in a world pervaded by such nightmares.

Experts have been pushing back recently: telling the worried masses that artificial intelligence is a long way from experiencing any sort of apocalyptic tipping-point. Rather, they emphasize that these types of doomsday scenarios are completely unrealistic. Even by the time A.I. is sufficiently advanced, they claim, we will have carefully planned for a host of worst-case scenarios; ultimately circumventing the problem before it comes to fruition. Worrying about robots taking over is more akin to those in the middle of the 20th century, imagining that nuclear wasteland-grown monsters would terrorize gentle humans with their glowing, melty third-arms.

I would like to argue that A.I., despite what some experts are saying, does have the potential to bring about irreversible doomsday scenarios, has been doing so already, and is following in the footsteps of other powerful technologies. The transition from “normal” life to a life where we are the puppets of digital systems will be so gradual, so subtle, that we will likely be taken along for the ride; none the wiser. To illustrate this argument, we should take a look at three technologies: car rental systems, social media, and nuclear weapons. Each offers both a portal into a future where humanity has no clue that it is ruled by artificial intelligence and a reflection of our own world that may reveal it to look more similar to a future doomsday than anyone thought.

Car Rental Systems

I recently had an experience during which I had to rent a car for a business trip. It seemed like a fairly straightforward transaction: I purchased the rental online in advance of my trip, paid for it in full so it was cheaper, and then showed up to pick up the rental car. Little did I know that the location I specified for pickup was not an actual rental office. Although it was listed in the choice of options and was the exact destination of my train, the office actually existed slightly down the road under a different name.

This completely threw off my rental, and I was forced to wait over an hour and a half to work with the employee at the rental office, his manager, and a customer service representative in a call center, while my case was being fixed. They were all supremely helpful, and I truly felt worse for them, seeing their frustration with the computer system, than I felt frustrated for having to wait so long. If this had occurred in the days preceding computer systems, the location difference would likely have been quickly corrected, acknowledged as a strange accident, and I would have received my keys and been on my way. Instead, in a world governed by digital bureaucracy, the rules of the rental office software were so strict that such a slight mistake (which it allowed for in the first place!) necessitated the involvement of three different employees whose expertise with the system was confounded. It begs the joke question: how many people does it take to screw in a light bulb? Well, obviously at least four if the light bulb is part of a national, computerized, strictly rule enforced light bulb screwing system with error messages that only serve to confuse those who are just trying to get some light in the room.

We are already living in a world where we are controlled in a significant way by computers.

Anyone who has ever worked with computerized systems at the doctor, for medical insurance, or for any other large, national company who is anything other than a top notch in their software design, knows that these issues happen all the time. In many cases, the convenience promised by digital systems is mired in poor design; thus rendering the systems almost impossible to use given any edge cases.

My experience at the car rental office says a lot more about our society than simply that we occasionally design bad software. We can go so far as to say that because these types of issues arise all the time, they have become incredibly normalized to us. Over time, we had to accept the fact that computer systems often fail, and that sometimes we just have to spend hours trying to untangle issues that never would have arisen in a world devoid of strict software.

We are already living in a world where we are controlled in a significant way by computers. The employees helping me out were completely subjugated to the will of the stubborn rental system. Human ingenuity and power were rendered meaningless because the computer system held all the keys to unlocking my car rental. Because the world gradually started implementing digital, bureaucratic systems, everyone simply went along for the ride. In most cases, a low-level employee would never have any say as to whether or not their employer forces a broken, complex system on their job. A few people change the way the world works, and we are all forced to slowly accept that this is our new reality.

Different technologies affect us in different ways. Whereas the car rental issue is frustrating, when other technologies exhibit flaws in their design, the outcomes can be much more pernicious. To examine the effects of, and response to, wide-scale implementation of machine learning systems, we need to look no further than the paragon of frustrating technology: social media.

Social Media

Facebook, Instagram, Twitter, and Snapchat have all been embroiled in scandals in the past few years; garnering criticism from many diverse groups of people. Social media platforms transformed from being simply networks to connect old friends or classmates, to daily hubs for news and politics. Unfortunately, due to the nature of these systems being primarily financially motivated, profiting by advertising revenue calculated by the number of eyes that see ads on the site, social media networks also knowingly transformed themselves into addictive systems. By giving users content that they would inherently enjoy, playing into their existing biases, unsavvy social media users experience ideological isolation and augmentation by nature of only seeing news or opinions that agree with their own.

This type of machine learning, which is aimed at learning user behavior in order to bring them to an app or website to maximize profit, has had a profound effect on our society. It is commonplace now to recognize that overuse of social media can be very dangerous. People become addicted to Facebook, Twitter, or Instagram, and are constantly surrounded by information that could be entirely distinct from that of someone with different preferences. It has led to many users adopting radically different worldviews from those of their political or ideological opposites and thinking them totally based in “facts” they read on the internet. A.I. technologies employed via social media have affected so many individual viewpoints that societal discourse as a whole has profoundly shifted as a result.

When only a handful of individuals make enormous decisions like these, wide-scale societal effects are heavily influenced by their perspectives — or lack thereof.

As in the case of the bureaucratic car rental system, people who use social media simply became acclimated to the fact that social media sites were harvesting their data and distorting their worldviews. At least with bureaucratic systems, their bugs and poor software design sometimes reveal their flaws and makes it clear that digital systems have an unfortunate and frustrating grip on our lives. Social media, and machine learning recommendation systems in general are designed with a flawed outcome in mind. When they work correctly, they are having a negative societal impact.

When people think of A.I. doomsday robots controlling our lives, the picture of social media systems being those same doomsday robots never crops up. Of course, we are not brutally subjugated against our will, but to a degree, we are heavily restricted by systems that govern our lives like social media. If something informs your worldview, your worldview then controls actions you take based on your beliefs. When A.I. researchers dismiss the fear of systems taking more control of our lives than we want, I believe that they are missing this context. Often, computer scientists and software engineers are too isolated in the world of computing to recognize that deep social change is made by their technologies all the time.

Wide-scale normalization of incredibly destructive technologies is not new to humanity. In most recent memory, world-ending nuclear militarization has become almost commonplace to accept as the “way things are.” Where more voices used to speak up against proliferation of this destructive capability, the world has moved past the initial shock of such notions — even though the threat posed by the technology is arguably more significant today.

Nuclear Technology

Technologies that are initially shocking due to their potentially destructive capabilities can too easily become normalized when the conversation surrounding them is lacking. Our post-Cold War world cares less about the spread of world-ending nuclear weapons and the destruction of nuclear treaties than it did only a generation or two ago. The public conversations surrounding nuclear proliferation have all but stopped; assuming an inevitable, nuclear world. The truth is that proliferation of nuclear weapons is absolutely not an inevitability; as some world leaders or technicians may suggest. Inventors, innovators, and state leaders have the ability to choose how technology progresses based on their ideologies and goals. A U.S. president who drives towards military hegemony will be more likely to rip up nuclear arms treaties than a president who wishes technology to be used for more humanitarian purposes. The only reason the dissemination of these weapons is considered “the way things are” is because so-called leaders have actively pushed for it to be the way things are; motivated by profits, power, or perhaps ignorance tinged with madness.

Artificial intelligence will likely follow a similar path. Due to its incredibly powerful nature, decisions restricting or liberating its potential uses, or of where to integrate it into society, will surely initially fall on the shoulders of political and industry leaders. When only a handful of individuals make enormous decisions like these, wide-scale societal effects are heavily influenced by their perspectives — or lack thereof. After a phase of initial public scrutiny of such decisions, there is a great risk of important discourse fading away; leaving only a few to deliberate the fates of many.

Recognizing the potential for gradual “doomsday” to occur, and voicing our concerns in a loud way, is the best way right now to ensure accountability of powerful technology actors.

We should view A.I. as a technology whose power is of the same magnitude as nuclear weapons. While the effects of these systems may not be as immediately shocking as the use of a nuclear weapon, it stands to reason that long-term societal effects from the use of A.I. systems can be as far reaching as the multi-generational aftermath of Hiroshima and Nagasaki. We cannot allow the world to become so acclimated to the manipulations of A.I. that only a few have any say or opinion about its effects in the world. People who stand to be most affected by results of these systems should have a permanent say in how they are developed and used. Discourse should be constantly encouraged; even — and especially — when it gets in the way of short-term profits or power gains.

Just as the world is a simple mistake away from a doomsday scenario driven by nuclear catastrophe, a not-too-distant future world may also have the added threat of subtle, societal doomsday driven by an undiagnosable digital flaw, or the unknown mistake of a handful of programmers. Those who make decisions regarding both of these technologies are consciously making choices that shape the entirety of human society. Any malintent, bias, or ignorance on the part of those in control could certainly bring about the doomsday that experts say is essentially impossible. It is very dangerous that the same experts are becoming part of the normalization process. This should be constantly countered by an equally powerful, and diverse, set of voices calling for caution. If the balance ever swayed too far toward normalization, it could mean complete control by a handful of unaccountable, potentially malicious, few.

Accountability May Mean Life or Death

A threat that is perpetuated by the potential ignorance or mistakes of the few has only one logical counterpart: mitigation by the many. Technologists do not like to talk about this because they often become too wrapped up in their own expertise to consider the opinions of those whose lives are most affected by their potential failings. They understandably imagine that any ordinary person would have such an ignorant opinion regarding decisions made about A.I. that they believe technologists alone have a mandate to shape the developments and usages of their systems.

This mindset is surely folly. The perspective of those whose lives are most changed by powerful technologies is essential to render any conversation even-sided. Nuclear proliferators surely would have a different opinion about the technology if their home had been utterly annihilated by a nuclear weapon. Executives at Facebook would likely have different thoughts about the effects of their greedy recommendation systems if they had found themselves unknowingly manipulated by the algorithms. Those who have the privilege of standing outside of the blast zone of these technologies often lack the perspective or empathy to imagine the disastrous effects applying to them. A.I. experts who dismiss domination by computer systems are right to dispel Terminator-fueled hysteria, but wrong to so completely dismiss the notion that our societies really could be controlled by computers without the requisite human understanding to turn back.

Those who design artificial intelligence and machine learning systems must plan for enough degrees of control that any potentially disastrous effects could be easily mitigated. They must also commit to taking the view of the “common person” into account when making significant decisions. Just as the scandals surrounding Facebook and Twitter’s use for political manipulation sparked public outrage, the only existing avenue to try to change how these systems affect your life is to become outraged.

Due to the lack of popular representation in corporations who control much of the world, and the unwillingness of governments to regulate A.I., right now the burden of accountability falls on ordinary people. Recognizing the potential for gradual “doomsday” to occur, and voicing our concerns in a loud way, is the best way right now to ensure the accountability of powerful technology actors. If technologists or potential regulators do not listen to the concerns of the many, due to ignorance or arrogance, their choices could mean life or death for many around the world.

To view machine learning technologies in anything other than this light is to look the other way in the face of deep troubles plaguing the world. Sure, machine learning or artificial intelligence will probably not create killer robots hellbent on controlling us through a sophisticated simulation. However, if we’re not careful in our analysis of these systems, we may wake up in a world where our lives are dominated by machines that were created by a few, cannot be easily rolled back, and where we may not even know how much we’re being controlled until it’s too late."
"Why Trustworthiness Matters in Building Global Futures

No matter how compelling our technologies are, they are only as good as the trust people have in the organizations that develop and govern them.

TIGTech — Seven Drivers of Trust

We’re standing at a pivotal point in our collective response to the coronavirus pandemic. The first vaccine against the virus is beginning to be rolled out, with others hot on its heels, and we can begin to imagine a post-COVID future albeit tentatively.

Yet despite the incredible strides being made, hope is being tempered by hesitancy-and sometimes downright distrust-as a growing number of people question the safety of the vaccine, and even the motives behind it.

It’s easy to dismiss this resistance to the COVID vaccine as irrational thinking, a rejection of science, and an unquestioning acceptance of misinformation and disinformation. Yet it points to a bigger issue of trust: Trust in how science and technology are governed, and more specifically, how organizations earn trust through being trustworthy.

To Earn Trust, Organizations Need to be Trustworthy

Earning trust is a challenge that goes far beyond the current pandemic, and touches on pretty much every aspect of our connections with the future. No matter how compelling our science is, how transformative our technologies are, or how important our ideas of the future might be, they are only as good as the trust that people place in the organizations that develop and use them.

But how is trust developed and maintained as we strive to build a better future together?

For the past couple of years, I’ve been a member of the advisory Panel for TIGTech, an initiative supported by the World Economic Forum and Fraunhofer Institute of Systems and Innovation Research that’s focused on trust, governance and technology innovation.

TIGTech was established to explore and provide guidance on what it means for governance approaches to new technologies to be trustworthy, and how trust is earned. The focus of the work has been on emerging technologies and large institutions. Yet the findings and recommendations are relevant to anyone trying to build a better future within today’s highly complex and deeply interconnected world.

Towards a more Engaged, Collaborative and Communicative Approach to Trust & Tech Governance

Last Friday, the first major report from TIGTech was released, and it highlights the need for developing more engaged, collaborative, communicative approaches to trustworthy and trusted technology governance. It als provides practical steps toward achieving this.

TIGTech — Towards a more engaged, collaborative, communicative approach

The report is, I am very pleased to say, written for real people grappling with real challenges, and is not in the slightest academic-although the underlying foundations are academically sound. I would go so far as to say that it should be required reading for anyone who is either studying global futures, or is involved in the process of helping to build a better future.

The report eloquently focuses on clear and concise nuggets of relevant information for readers- three key findings, five things to know about trust, seven drivers of trust, and three competencies for trusted governance.

This approach makes it highly accessible. It also makes the insights relevant and actionable to a wide range of individuals and organizations. Plus, it’s deeply refreshing to have such an important document written in plain language that is easy to make sense of!

Many of the points that are made feel like common sense when you read them-yet paradoxically they can’t be, otherwise they would be more commonly found in practice.

For instance, the three key findings are:

Be more engaged, more visible — show your impact.

Detach governance from hype and ideology — focus on the public interest.

Get comfortable with navigating ethics and values.

And the seven drivers of trust are:

Intent — Public Interest (upheld through purpose, process, delivery and outcomes).

(upheld through purpose, process, delivery and outcomes). Competence (delivering against expectation effectively, reliably, consistently, responsively).

(delivering against expectation effectively, reliably, consistently, responsively). Openness (being transparent and accessible in processes, communications, explanations and interactions).

(being transparent and accessible in processes, communications, explanations and interactions). Respect (seeing others as equals; listening to and taking seriously their concerns, views and rights. Considering the potential impact of words & deeds on others).

(seeing others as equals; listening to and taking seriously their concerns, views and rights. Considering the potential impact of words & deeds on others). Integrity (operating honestly, being accountable, impartial and independent of vested interests).

(operating honestly, being accountable, impartial and independent of vested interests). Fairness (enshrining justice and equality in governance processes, application, enforcement, and outcomes). And

(enshrining justice and equality in governance processes, application, enforcement, and outcomes). And Inclusion (being collaborative, inclusive, involving others).

These are all critically important, and should be part of any future-builders credo. But they are just the tip of the iceberg when it comes to earning trust. And underpinning them is a nuanced and sophisticated understanding of trust that is essential to developing and using new technologies in the public interest.

Trustworthiness is Foundational to Global Futures Building

Beyond the immediate relevance of the report to technology development and use in the public interest, reading through it, I found myself connecting the ideas it lays out to almost every situation within today’s society where trust is paramount, from communicating and engaging around science, to building a new initiative around global futures, to taking justice, equity, diversity and inclusion seriously, to being a trusted research and education establishment.

Here, I was particularly taken by the report’s exploration of “ five more things to know about trust “-these should be essential reading for anyone who’s work involves demonstrating trustworthiness and earning trust.

These insights include acknowledging that trust is an outcome that’s best-achieved by focusing on others, and that it signals a hope that an organization will fulfill the expectations we have of them. They also emphasize what should be self-evident, but rarely is-that trusting people first makes them more likely to be trustworthy and to trust you back, and that trust is a spectrum, and not an either/or judgement.

And importantly, they make it clear that trust is dynamic, messy, personal, and a two-way process.

In other words, while trust and trustworthiness are critically important for building a better future, the process of earning and demonstrating them is not one if simply follow rules and procedures, or checking boxes. It takes awareness, empathy and humility, and a willingness to embrace the messiness of being human within a complex society as we strive to put others first.

This is sage advice as we stand at this pivotal point in the fight against COVID. But it’s also important as we look beyond COVID and work together to build a future that is just, equitable, and sustainable, and one which is threaded through with hope and possibility.

Which is why I’d recommend anyone with an interest in building a better future check out TIGTech, and read the initiative’s recent report on trust and tech governance."
"In 2019, it’s tempting to think that civilisation’s ability to create something new has long since peaked. From aeroplanes to the World Wide Web, electric cars to chess-playing AI, the once unimaginable is now taken for granted.

Isaac Newton said that his discoveries were only made possible because he was “standing on the shoulders of giants”. It is easy to think that the great minds of the Enlightenment and Victorian eras, who gave the world electric motors, lightbulbs, and antibiotics, really did invent everything.

How much inventing is there really left to do? The answer is: lots — according to the data.

Inventor Thomas Edison, who lived from 1847–1931, is famous for holding over a thousand utility patents filed by the United States Patent and Trademark Office (USPTO). But, in 2015, Lowell Wood, a relatively unknown American inventor surpassed Edison’s achievement when he personally filed 1,085 patents, and Wood’s list of inventions is still growing fast.

The total percentage of granted patents over time | Infographic by Catalogtree

Just looking at the overall number of patented inventions — the past actually generated far fewer than we might assume. Records reveal that between 1852 and 1915, the UK’s Intellectual Property Office granted approximately 486,000 patents, averaging about 7,700 per year. Between 1916 and 2013, however, the agency granted 2 million patents — roughly 21,000 per year. As this number has grown still further, 2018 might be the best time to be an inventor, anywhere, ever.

Data collected by the World Intellectual Property Organization (WIPO) shows that just two years ago, inventors around the globe filed a record-breaking 3.1 million patent applications, up 8.3% from the year prior. And more patents are passing the grade than ever before, too. In 2016, 1.35 million patents were granted, compared to 755,200 a decade earlier.

While men continue to design the majority of most of the objects and technologies that become patents, WIPO reports that female inventors are on the rise. In 2017, for example, women were listed on nearly one in three international patent applications. Ten years prior, that rate was less than one in four.

The breakdown of IP applications by category | Infographic by Catalogtree

And then there’s social media. If you have an idea today, showing it to your potential customers has never been easier. At present, 2.2 billion people visit Facebook each month, while one billion hours of video are viewed on YouTube every day. Furthermore, in less than a decade, the crowdfunding platform Kickstarter has raised $3.9 billion for various ideas and plans. In the same timeframe, contributors have successfully funded more than 7,000 technology campaigns and over 12,151 design projects.

Finally, in an attempt to boost productivity and spur the economy, an increasing number of governments are offering tax incentives to companies willing to invest in research and development. In the UK, for instance, HM Revenue & Customs statistics show that the amount of funds given to businesses both big and small has increased virtually every year since they launched their R&D tax credit program in 2000–01. In total, companies have made over 240,000 claims, amounting to £21.4 billion in tax relief.

In short, if you want to invent something new, there’s never been a better time than now."
"Substack vs. Medium In Depth Comparison

While the infographic above helps quickly differentiate between the two platforms, it does not discuss the various subtleties and nuances which writers may be interested in when committing significant time and resources to a single platform. The content below offers an in depth comparison of the two platforms.

Platform Purpose and Intended Function

Medium:

Medium was created by Ev Williams, previously co-founder of Blogger and Twitter. He initially developed Medium as a way to publish writings and documents longer than Twitter’s 140-character (now 280-character) maximum. Basically a long-form version of Twitter that would encourage more substantive discourse. In 2017, Medium introduced its Partner Program which was implemented to help persuade writers to put their content behind Medium’s metered paywall. All of this is part of a larger attempt to make Medium a central hub of content, regardless if it is written by little known writers or by staff at one of Medium's in house publications.

Substack:

A Substack publication is an email newsletter that you can charge money for. You write it, readers pay you. Substack’s focus is on helping Substack writers maximize their earning potential under this model.

To help writers, Substack offers the following tools (for free) to all accounts:

A content management system that has been purpose-built for publishing paid newsletters.

Connection to a Stripe account to manage payments and receive money in your bank account.

A website built with mobile in mind that looks great and loads fast on any device and in any browser.

Analytics that show your email open rates, website pageviews, and subscriber counts.

Access to a growing knowledge database of best practices for running a paid newsletter business.

Platform Business Model

Medium:

Medium makes money by charging readers $5 per month (or $50 per year) to access its content, which is hosted behind a metered paywall. In the past, Medium had attempted to monetize content by allowing publications to charge subscriptions (similar to the current Substack model) but it has since pivoted away from this model.

Medium’s paywall is unique in that it is metered and not a firm paywall. Readers can read 3 articles per month, even if they are not paying subscribers. This is an important part of the Medium marketing strategy, as it entices regular readers to become subscribers.

This method is successfully used by Medium, but not many other digital media companies, because Medium has amassed a tremendous audience and they continue to build this audience through their high domain authority.

For example, if I Google:

guerilla marketing tactics

The first result (which is actually a Google featured snippet), is an article I published on Medium, 10 Creative Guerilla Marketing Tactics to Boost Your Brand, Company, or Cause.

Had I posted this on my own blog or any other site with lower Domain Authority, this article would have likely been buried 5+ pages deep in the Google search results.

This obviously encourages me to use Medium, but it also serves as a gateway for people who have never heard of Medium, to look into the platform. This is essential to Medium building their subscriber base.

Substack:

Publishing on Substack is completely free, if you choose to not charge for your content. Once you start charging, they take a 10% fee on top of credit card fees, from all of your paid subscriptions.

On one hand this is a good model because it aligns incentives for both publishers and writers — Substack writers must succeed in order for Substack the company to succeed.

That said, Substack’s fees do add up and are something worth considering if you have a large and engaged fan base.

If you are able to set up your own email marketing funnel and promote your content effectively, you may be able to earn more on Medium.

Substack’s business model relies partially upon writers utilizing the WYSIWYG tools effectively, to reach new readers, and develop a fanbase.

It’s worth noting that Substack gives you direct access to customer subscription info (emails), meaning you could theoretically walk away from the platform at some point in the future. On Medium, you do not receive reader contact info, establishing a heavier dependence on the platform and some loss of autonomy.

Successful User Profile

Medium:

It is hard to define the “profile” of the typical successful Medium writer because Medium only releases very limited earnings information.

But based on my research into top writers, generally speaking, are those who can create semi-viral content. This is largely due to the Medium writer compensation system which rewards writers financially, only for the engagement of paying readers. So in order to make significant money on Medium you must be able to draw in readers who are a part of the Medium platform itself.

There are definitely exceptions to this, but the above characterization cover probably 85%+ of top earning Medium writers.

Top Medium writers (much like Substack), do generally need to develop a niche, but the niche needs to be a semi popular niche that exists on Medium. Some examples of these niches are:

Personal development

Technology

Programming

Marketing

Gender and Social Issues

Personal Finance

Self-Help

Productivity

Entrepreneurship

Notice that while these are considered “niches” within the Medium platform, these categories are still fairly broad and would not necessarily constitute a “niche” when writing on Substack.

Substack:

Successful Substack writers typically write about a very specific niche and are sometimes quasi-subject matter experts on a topic.

Take Bill Bishop. One of Substack’s most successful newsletters is Sinocism. The Sinocism China Newsletter is an email newsletter written by Bill Bishop that helps readers better understand China.

Sinocism offers content promising to “make you smarter about China.” The newsletter is considered a must read by diplomats, policymakers, investors, executives, journalists, and academics who want to better understand China and get regular commentary on Chinese and English language news.

Before writing Sinocism, Bill Bishop was a media executive with more than a decade’s experience living in and decoding China. He previously wrote the Axios China weekly newsletter and the China Insider column for the New York Times Dealbook.

While you do not need to be an industry veteran to publish a newsletter on a topic, obviously it goes a long way toward establishing credibility on a certain subject.

Substack publications can consist of an even smaller niche. JoeBlogs is a popular blog focusing on baseball.

This type of publication works well on Substack because it unites true baseball fans around a hyper specific topic.

In contrast, it would be hard to run a successful sports publication on Medium, covering many sports. A baseball specific publication on Medium might generate some traffic due to Medium's high Domain authority but unless their is a mass appeal to Medium's paying readers, it is unlikely to generate many views from the readers whose views actually count toward Partner Program earnings (paying readers).

Writer Monetization Methods

Note: both platforms rely upon Stripe for writer payments, meaning that from a global perspective, if you are not in a country supported by stripe, neither of these are currently an option. Both seem to have formed exclusive relationships with Stripe:

Medium:

Substack:

Medium:

Medium has a clear monetization method, that can be easily used by any writer — The Medium Partner Program.

The Medium Partner Program is Medium’s program that allows writers to earn money from articles they submit.

The Medium Partner Program is open to anyone, and it even allows the re-posting of content first posted on other platforms (such as a blog post on your self-hosted blog).

Medium pays writers based on Member Reading Time — which is a measure of the total time paying subscribers spend reading your articles.

As Medium described in their post on the earnings update:

As a user reads, we measure their scrolls and take care to differentiate between short pauses (like lingering over a particularly great passage) and longer breaks (like stepping away to grab a cup of coffee). Reading time incorporates signal from your readers without hurdles. You don’t need to ask your readers to remember to clap, or click, or do anything other than read.

This process can feel a bit opaque because Medium is (theoretically) dividing up some pot of money based on subscriptions and distributing it to writers based on several propriety factors.

But the Medium Partner Program has proven itself to be a reliable stream of income for thousands of writers (myself included).

While many successful Medium writers promote their stories on social media or through their mailing lists, many focus their attention on writing and rely upon Medium’s large built in audience and the platform’s content discovery tools to drive traffic.

This combination has made Medium a popular choice among writers who want exposure while still being compensated for their effort.

Medium is 100% free for writers.

Substack:

Substack is similar to Medium in that anyone can set up a subscription newsletter easily using Substack’s free tools.

However, it is different in several ways, when it comes to how writer’s are compensated.

Substack offers little in the way of content discovery or assistance driving traffic. Rather, they focus on providing a more complex suite of tools then Medium, but then the writer is responsible for driving traffic/subscribers to their newsletter.

Substack gives writers:

A free Substack domain/website

A landing page, which can collect emails of potential subscribers or process the payments of readers ready to subscribe

An email marketing system which allows writer to simultaneously post content to their blog and to send readers on their email lists preformatted emails containing their posts.

This third point is the main difference between Medium and Substack.

As a writer on Medium, you do not have direct access to your readers’ emails. You instead rely upon Medium's curation system and content distribution system to feature you work among willing readers.

Many writers on Medium get around this by using an email marketing system (such as Mailchimp) and they insert a CTA or sign up form at the end of their articles. This functions as a hedge against being disconnected from their readers if the author decides to leave Medium or if Medium (right or wrong) decides to suspend their account.

Substack essentially cuts out the middleman. You do not need a separate email marketing sytem with Substack since al your posts are already sent to your subscribers.

Email marketing systems typically increase in price as your email list grows, so this can become a large cost for many writers on Medium. Substack currently offers this same functionality essentially for free.

Substack also lets writer’s determine the price they charge subscribers for access to their newsletter. Many writers feel they are not adequately compensated for their writing, and Substack allows them to charge whatever they want to a core group of subscribers.

Substack does have monthly minimums that author scan charge ($5 per month or $30 per year) but these threshold are generally below or at what most writers would charge for their work anyway. If you really want to charge less, you can use the promotion code hack and set an indefinite discount. For example, a 50% discount off of a $5 per month/ $30 per year plan would only cost $2.50 per month or $15 per year. I currently am running this exact promotion on my Blogging Guide newsletter:

Content Discovery

Medium:

One of the few advantages that Medium has over Substack is its robust content discovery tools. In other words, readers come to Medium for reading material, they use the search function or read content recommended to them based on their interests.

Substack:

Substack does not have a content discovery tool (or at least not yet). They do have a leader board page which displays top posts and newsletters, both paid and free:

Platform Tools

Medium:

Both Medium and Substack make it easy to create stylish and professional looking posts:

They also both have fantastic article editors:

Medium:

Substack:

However, Medium does allow for better organization of articles with their publication tool, which includes features like feature pages which can be prominently displayed at the top navigation bar of your publication:

Substack:

In general Substack offers writers more tools than Medium.

As previosuly mentioned, the built in email marketing function is probably the most useful. This ability to automatically email your readers is not only cost effective, but the data analytics are quite useful.

You can see how an email performed in terms of how many recipients it reached, how many opened it, and how many of those readers subscribed:

You can also see a breakdown of all the links clicked in your newsletter — a feature of most email marketing systems which Medium noticeably lacks:

This an extremely valuable feature. For those familiar with my analysis of Medium data analytics, Substack’s data analytics may appear a bit more sparse than Medium’s. But Substack’s data analytics includes conversion rates, which from a content creator’s perspective, is of the utmost importance.

If you want to see the activity of an individual subscriber (free or paid), click on the “Subscribers” button in the upper right hand corner of the page:

If you scroll down the page, you will see a list of your email subscribers:

If you click on a specific email, you can see the detailed activity of each subscriber:

For example, in the screenshot above, you can see that the user:

— First signed up for Medium Blogging Guide on March 11th. — Next, they received an email with my post on March 13th (Gumroad & Medium). — A few days later, they received another email, on March 16th (Platforms to Write on Besides Medium) They opened first email twice on March 17th (Gumroad & Medium). They upgraded to become a paid subscriber on March 21st

Substack’s subscriber-specific analytics provide enormous insight into the patterns of your subscribers.

You can use this data to:

Determine what posts lead to a higher subscriber sign up rate

See whether a specific user opened your latest email

Establish the proper ratio of free to paid content in order to optimize paying subscribers

Conclusion

Medium and Substack are both amazing free tools, and there is no reason you have to use just one of them.

The platforms broadly serve the same purpose (to help writers connect with an audience and make money from their content), and their differentiating factors, largely are indicative of their different functions (Medium — easy blogging, Substack — easy subscription newsletter).

If you were to choose between the two platforms, I would suggest you pick based on the following questions:

How much time do you plan to invest? — if you are looking to make money by writing the occasional article (1–2 articles per month), Medium may be the better platform. Substack subscribers generally expect several high quality/useful posts per week. Note : even though Medium may be the better platform in this scenario, that does not mean it is ideal to post once or twice per month on Medium. In order for most new writers to be successful, they should also publish a few times per week. On the other end of the spectrum if you are a prolific writer cranking out 2–3 posts per day, Medium may be the best option since there is a larger audience, and each post increases tour general reach on Medium by expanding your fan base. Substack subscribers might actually get annoyed with multiple posts per day, as you are flooding their inbox with direct emails.

— if you are looking to make money by writing the occasional article (1–2 articles per month), Medium may be the better platform. Substack subscribers generally expect several high quality/useful posts per week. : even though Medium may be the better platform in this scenario, that does not mean it is ideal to post once or twice per month on Medium. In order for most new writers to be successful, they should also publish a few times per week. On the other end of the spectrum if you are a prolific writer cranking out 2–3 posts per day, Medium may be the best option since there is a larger audience, and each post increases tour general reach on Medium by expanding your fan base. Substack subscribers might actually get annoyed with multiple posts per day, as you are flooding their inbox with direct emails. Do you have an existing following or fan base? — if you have an existing fan base, Substack is a great way to provide value to this audience. Medium is generally better for those who are trying to build a following. That said, building a following on Medium is not easy, but your content is much more likely to be featured or reach new readers on Medium.

— if you have an existing fan base, Substack is a great way to provide value to this audience. Medium is generally better for those who are trying to build a following. That said, building a following on Medium is not easy, but your content is much more likely to be featured or reach new readers on Medium. Are you writing about a really niche topic? — if you plan to write about the migration patterns of a specific type of animal or cover breaking news from Azerbaijan, Substack is probably a better bet. There is little chance that either of those topics would earn any money of Medium. However, if you are writing about multiple topics or topics that are more broad (personal finance, health, relationships, etc.) Medium will likely be an easier platform to monetize your writing.

— if you plan to write about the migration patterns of a specific type of animal or cover breaking news from Azerbaijan, Substack is probably a better bet. There is little chance that either of those topics would earn any money of Medium. However, if you are writing about multiple topics or topics that are more broad (personal finance, health, relationships, etc.) Medium will likely be an easier platform to monetize your writing. Are you comfortable advertising your writing? — If you absolutely refuse to market your writing, than Medium is probably a better platform, as it does have a large built in audience. Substack cuts through the noise by leveraging the enormity of the internet and connecting you with potential fans, but you must identify and convert these fans to subscribers in order to make any money.

— If you absolutely refuse to market your writing, than Medium is probably a better platform, as it does have a large built in audience. Substack cuts through the noise by leveraging the enormity of the internet and connecting you with potential fans, but you must identify and convert these fans to subscribers in order to make any money. Are you hoping to benefit from organic search engine traffic? — While Substack does better than most platforms SEO-wise, it is hard to beat Medium if your goal is to drive visitors through organic search traffic. There are very few platforms on the internet that have higher Domain Authority than Medium. And almost none of those that do rank higher allow guest posts from any person.

I hope this has been a helpful overview of some of the similarities and differences between Substack and Medium. If you have any questions, just leave a comment and I’ll be sure to respond!

— Casey Botticello"
"INFOGRAPHIC SPECIAL — VACCINE DEVELOPMENT

How successful have we been with vaccine development for various pandemics?

Comparing the vaccine development timelines since the earlier twentieth century

Major technological advancements made in the field of Medicine has led to a significant increase in average life expectancy. However, it is a somewhat perplexing fact for many as to why we have not been able to develop a vaccine for diseases like AIDS for 40 years, whereas multiple COVID-19 vaccinations have already been rolled out in record time. No wonder the recently developed vaccines are being closely scrutinized by the experts for their safety & efficacy.

For comparison purposes, It took approximately 25 years to develop a vaccine for the Spanish Flu which killed between 40–50 million people. Similarly, the first Ebola vaccine — an effort that took 43 years since the discovery of the virus. And we are still waiting for a cure for AIDS and yet we have an answer

Previous vaccines have saved countless lives — 10 million deaths between 2010 and 2015 alone have been prevented, according to the data from the World Health Organization. And we are hoping the case is the same for COVID-19. The infographic above looks at the timeframes for vaccine development for every pandemic since the turn of the 20th century. It also highlights the major steps involved in the development of individual vaccines.

Talking about COVID-19 vaccines, nearly 7.25 billion doses have been pre-purchased by countries and organizations around the globe — as many countries scramble to innoculate their populations. The second infographic below highlights that high & middle-income countries have hogged the purchasing process by ordering dosages according to their purchasing power.

While the hope is that a global vaccine initiative like the COVAX can bring some equality in the vaccine distribution among high-income and low-income countries, it is going to be a long road ahead towards pre-COVID normalcy. In the meantime, here is the link for the COVID-19 vaccine tracker, if you want to follow the development process for various endeavors being undertaken right now.

Image Credit: Visual Capitalist

Stay informed with the content that matters — Join my mailing list"
"A photo shows Intel’s latest neuromorphic system, Pohoiki Springs, and one of the rows within it. The system unveiled in March 2020 integrates 768 Loihi neuromorphic research chips inside a chassis the size of five standard servers. (Credit: Intel Corporation)

Intel’s most powerful “neuromorphic” chips use the human brain as a model

The chip giant is designing computer systems with the computational capacity of 100 million neurons

Computer chips in a traditional computer are the brains of the system but function much less like a human brain. Neuromorphic Computing is the emerging field in technology that continues to expand and evolve. The current CPUs and GPUs do a satisfactory job of processing complex calculations, but innovations like AI has created the need for computers that can process unstructured and noisy data in real-time — much like how our brain neurons do.

Although traditional computer chips are pretty good at crunching complex and large sets of data, they lack the capability when it comes to abstract data, like spotting a difference between two different animals — something that our brain can do effortlessly. The data pathways in our brains are called neurons, which perform all these tasks.

Researchers are now working to replicate the same process in computer chips. It was only recently that I wrote about how Scientists at the University of Bath now seem to have decoded the bizarre behavior of neurons and replicated it on tiny silicon chips. Intel has now gone one step further.

It has produced the most powerful neuromorphic research system providing the computational capacity of 100 million neurons. This puts the new system at par with the brain of a small mammal. Pohoiki Springs is the latest and most advanced system which integrates 768 Loihi neuromorphic in a casing that is the size of five standard servers.

The system lays the foundation for an autonomous, connected future, which will require new approaches to real-time, dynamic data processing. ~ Intel Blog

Even better is the fact that such a complex system is not an energy-guzzling machine like other comparable servers. According to Mike Davies, director of Intel’s Neuromorphic Computing Lab, Pohoiki Springs has increased the capacity of the Loihi neuromorphic research chip by more than 750x, while operating at a power level of under 500 watts.

Pohoiki Springs is a cloud-based system that will be made available to members of the Intel Neuromorphic Research Community (INRC) to further their research to solve more complex problems. Some of the promising future applications include the following scalable algorithms:

Constraint satisfaction problems can be accelerated by exploring varying solutions in parallel.

parallel. Finding optimal paths in graphs and patterns.

Real-world optimization problems like maximizing the bandwidth of a wireless communication channel or minimizing risk in a stock portfolio.

Neuromorphic systems basically replicate, communicate and learn like a human brain but at the hardware level. And the eventual goal is to use neuroscience to develop chips that function less like traditional computers and more like the human brain."
"How Much Does It Cost To Create A Diet and Nutrition App (Features, Business Model, Cost etc) Sophia Martin Follow Dec 10 · 11 min read

Are you planning to develop a diet and nutrition app in 2020–21? Not sure how much it cost you to develop a brilliant diet and nutrition tracking app with MVP? Many of the developers can assure you to develop it under $5000!

Isn’t it too good to believe?

According to the survey, the median development cost range is in between $37,913 and $171,450, but it could climb up to $50,000 or higher depending upon the various factors.

But, with the fact that mobile healthcare apps are projected to generate $50,000+ revenue and value by 2025 and 74% of patients are using wearable and other healthcare tools to cope up with their medical conditions. It is clear that the healthcare sector is opening doors with plenty of opportunities for startups and leading entrepreneurs and making sense to invest in app development in 2020–25

If you are still in a doubt, then it is right time to have a look over the statistics of the market overview and trends of diet and nutrition app:

The global mhealth market is rapidly growing and expected to reach $111 billion by 2025.

Fitness apps are accounting for the largest share of the US mhealth app market and expected to grow to $50 Billion by 2025.

mHealth app market is projected to expand at a CAGR of 44.7% and reach at $236 billion by 2026.

Data Bridge Market Research analyses that the market is growing with a CAGR of 20.9% during 2020 to 2027 and expected to reach $10,189.62 million by 2027.

To wrap up these figures, it is fair enough to develop a diet and nutrition app for your business in 2020–21. The central question, while there are various leading fitness apps (Nutrients, MyFitnessPal and Calorie Counter & Diet Tracker) are already leading the market, how to launch a winning app that helps you achieve success in 2020–21?

To develop the best diet and nutrition app, it is worth hiring a dedicated app development company experienced in creating healthcare apps integrating the right technologies and methodologies. But if you are thinking that developing a diet and nutrition app is as easy as replicating the app clone of a leading healthcare app, then you might be mistaken at this point.

With over 5.5 million apps available in Google and iPhone app stores, launching a duplicate app with more or fewer features is just a flop idea. No matter how brilliant an app idea you have and what set of features you choose to incorporate in the app, the success of the app dramatically depends upon the skills and expertise of your app developers embedded in the app.

So let’s get started with the features and functionalities of the diet and nutrition app…

But before learning about the crucial features and functionalities of the diet and nutrition app, it is essential to understand the basic idea of launching this app. How does this app help end-users?

How Does Diet and Nutrition App Be Useful for Users?

The simple idea about using the diet and nutrition app is it helps in adopting the healthy lifestyle that seamlessly syncs your body and weight. The diet and nutrition app keeps track of your dietary habits and provides personalized diet plans that help you meet your health goals in a fast turnaround.

A mindfully constructed app can help users with the following aspects:

Assist in creating a balanced and nutritious diet chart Keep tracking a log of calories and water intake Assisting tips and plans for the weight loss Monitoring daily physical activity Assisting a healthy grocery list Suggesting healthy food options

A diet and nutrition app is highly useful for the end-users if they are created with a fantastic set of modern features. So before you move to the app development procedure, make sure you hire best software development company that has clear idea about the features and functionalities required to construct modern diet and nutrition app.

Essential Features You Need to Include in a Diet and Nutrition App

Since people all age have become so conscious about their health and fitness, therefore, they are eagerly looking for apps that encourage them to follow a fitness regime. And having a mobile device with several tools and applications that help in tracking their diet and nutrition habits every day. All you need to keep in mind is the set of features you need to implement in the app.

Here’s the list of features and its cost of development:

General Features to build a diet and nutrition app in 2020–21

User Login or Registration: To get started with any app, users have to create a profile with the essential details including age, gender, height, weight and eating habits. The registration window of the app will keep your details completely safe and secure. Users can also log in the app with social media integration and allow the app to collect all the essential information from that channel and send you personalized messages and notifications.

To get started with any app, users have to create a profile with the essential details including age, gender, height, weight and eating habits. The registration window of the app will keep your details completely safe and secure. Users can also log in the app with social media integration and allow the app to collect all the essential information from that channel and send you personalized messages and notifications. Dashboard and Food Logging: Make sure you have a feature where users can log the amount of food they are consuming in detail to keep track of their calories in-take and get most accurate diet suggestions to stay healthy.

Make sure you have a feature where users can log the amount of food they are consuming in detail to keep track of their calories in-take and get most accurate diet suggestions to stay healthy. Seamless Connectivity With Wearables : With the emergence of technologies such as IoT, it becomes easier to track every single thing through your smart devices. By connecting your app with the third-party devices and wearables such as Apple Watch, Android Wear or Fitbit or so on, you can make it easier to monitor everything right from tracking heart rate, physical activity, step counts to food you intake. Wearables are getting popular these days, so leverage this feature to add great convenience to the user.

: With the emergence of technologies such as IoT, it becomes easier to track every single thing through your smart devices. By connecting your app with the third-party devices and wearables such as Apple Watch, Android Wear or Fitbit or so on, you can make it easier to monitor everything right from tracking heart rate, physical activity, step counts to food you intake. Wearables are getting popular these days, so leverage this feature to add great convenience to the user. Implement Engaging Statistics: People are impatient when they are using diet and nutrition apps to track everything from monitoring the amount of calorie burnt at a point of the day to how far they are from their health goals. To keep your users engaged with the app, you can use exciting statistics via presenting the data or stats in a graph showing weekly progressive reports to boost their motivation. Professional diet and nutrition app development companies understand the importance of these features; therefore, they can suggest better ideas to develop this feature.

People are impatient when they are using diet and nutrition apps to track everything from monitoring the amount of calorie burnt at a point of the day to how far they are from their health goals. To keep your users engaged with the app, you can use exciting statistics via presenting the data or stats in a graph showing weekly progressive reports to boost their motivation. Professional diet and nutrition understand the importance of these features; therefore, they can suggest better ideas to develop this feature. Push Notification: This is a must-have feature in diet and nutrition apps, as it helps you send the right message at the right time to the user about what they should eat, in what amount, and at what intervals. By integrating Virtual Assistance in apps, you can get better user experience.

This is a must-have feature in diet and nutrition apps, as it helps you send the right message at the right time to the user about what they should eat, in what amount, and at what intervals. By integrating Virtual Assistance in apps, you can get better user experience. Live Chat: Provide the provision of the 24*7 customer support to the user with live chatting feature and allow users to get interacted with the expert dietitians and nutritionists directly. With the help of this feature, experts can suggest a better workout regime and diet plan based on health concerns. This feature in your app will ensure you a win-win option.

Provide the provision of the 24*7 customer support to the user with live chatting feature and allow users to get interacted with the expert dietitians and nutritionists directly. With the help of this feature, experts can suggest a better workout regime and diet plan based on health concerns. This feature in your app will ensure you a win-win option. Feedback: Getting feedback directly from the user will help you determine the areas that required improvement. Based on the feedback and ratings from the users, will help you enhance the interface of the app and minimize the risk app abandons.

Getting feedback directly from the user will help you determine the areas that required improvement. Based on the feedback and ratings from the users, will help you enhance the interface of the app and minimize the risk app abandons. Personalized Diet Suggestions: This feature allows users to seek better diet suggestions from dieticians and nutrition by merely discussing their concerns and getting satisfying assistance.

Advance Features for the modern Diet and Nutrition app

BMI(Body Mass Index) Calculation on Board: BMI is one of the most critical aspects of Diet and nutrition tracking apps. Once the user logs in the app, they must be asked to enter specific details about themselves regarding weight, height, gender, food preferences, allergies if any.

With the help of these details, a complete report has been generated by the app as per body measurements and the app will keep tracking everything as per that report. Moreover, fitness experts create a diet plan that accelerates the process and help you meet your goal in fast turn around.

Diet and Nutrition Analyzer: Each user has its purpose of using this app, therefore with the feature of diet and nutrition analyzer, users can get a clear understanding of how close or far they are from achieving their goal. However, make sure that this feature should be presented in a way that users can easily access it and be able to make out the best.

Each user has its purpose of using this app, therefore with the feature of diet and nutrition analyzer, users can get a clear understanding of how close or far they are from achieving their goal. However, make sure that this feature should be presented in a way that users can easily access it and be able to make out the best. Listing Best Fitness Experts: To serve the best services to the users, make sure you have the best fitness experts and dieticians on boards that keep pushing users regularly. These experts closely monitor your food intake and physical activities and share personalized diet charts and easy to follow healthy food recipes as well.

These health and fitness experts provide you with regular guidance to the users to help you reach your body goals as earliest as possible.

In-app Recipe Book: This feature not only helps them stick to healthy eating but also encourages them to try exciting meals. This feature gives them an idea of how to prepare healthy meals with the help of the detailed recipe available on the app.

This feature not only helps them stick to healthy eating but also encourages them to try exciting meals. This feature gives them an idea of how to prepare healthy meals with the help of the detailed recipe available on the app. Badges: In this cut throat cutting edge competitive market, everyone is trying hard to beat others. And this feature keeps users strong-willed and determined to attain the badges based on the performance.

To build your app with excellent features and functionalities, you can choose to hire a nutrition app developer that can help you customize the app according to your budget and needs. The above mentioned features are the best-suggested features to integrate, but the choice will remain yours. In addition, the time and cost of creating these features are rough estimations, as it greatly varies according to the team or software development company you choose to hire for the project.

What is The Process of Developing Modern Diet and Nutrition App In 2020–21?

If you have finally made a decision to move ahead with Diet and Nutrition app development, then you need to have a clear perspective and a strategic plan to achieve success.

So let’s begin with the steps that can help you build a diet and nutrition mobile app:

1. Research, Plan and Developing a Roadmap

Considering the facts and figures of progressive diet and nutrition app market, it is a worthy decision to make an investment. But before investing your time and money, it is important to analyze the market and evaluate your app development idea accordingly.

So to make a clear plan and functional roadmap about your development process, make sure it should include:

The idea of launching the app

Who will be your targeted audience?

What will be your development plan?

What are you trying to achieve through this app development?

How will you make a profit from the app?

Once you find answers to these questions, you can move ahead to the other part.

2. Choice of Operating System

Before you roll out the UX/UI design of the app, it is important to decide whether you want to develop a Native or Cross-platform application? The choice of OS greatly depends upon your targeted audience, as it is a well-known fact that iOS devices are used by rich people, whereas Android devices are preferred by ordinary middle-class people.

Moreover, iOS users have the power to purchase the app, whereas Android users are habitual to use free apps. But with the launch of a cross-platform application, you can reach out to a wider market and be able to save a great cost on app development.

There are many open-source frameworks, tools and technologies that you can use to develop Diet and Nutrition apps such as Flutter. Despite being a young and newly launched framework, Flutter has secured its position in the 2nd most popular and widely used frameworks. By leveraging its features and wide pool of plugins, you can customize the app with the best UX/UI design.

3. UX/UI Design

Since every app store has its own guidelines, therefore you need to customize the UX/UI design of the app accordingly. However, don’t forget that app design is the most important element of your development process, so make sure it should be interactive, intuitive, and engaging. To develop a highly simple and unique app design, it is best to hire a dedicated UX/UI designer that helps you finalize a wireframe and the UI design based on your needs.

4. App Development

The development phase would involve working with the app development team that can understand your requirement and have skills to go above and beyond your requirements to deliver result-oriented solutions. All the features will be coded by the developers and made it functional with the UI design as a base. To make your app successful, all you need a mobile app development company to ensure your best results from the development solution.

5. App Testing and Quality Assurance

Once you are done with the development process, its time to test every single element of the app by running it over multiple platforms to ensure seamless user experience. The main goal of hiring the app tester is to eliminate the bugs and ensure that your app works flawlessly in various situations.

Cost To Develop Diet and Nutrition App in 2020–21:

While it is hard to estimate the exact cost of the app development as it greatly varies on multiple factors right from the development team you hire to the features and functionalities, app complexity, and more. Still, with the close evaluation of each factor, there is a scope of determining the real estimation of the app development. However, on an average, you can develop a Diet and Nutrition app with MVP from $10,000+.

But, being an IT expert, we always recommend getting in touch with the nutrition app development company that provides you with a complete development team under one roof including:

Project manager

UX/UI Designer

Developer

Backend/ Frontend developer

App Tester

Android/ iOS Developer

The cost to hire a development team can be starting from $25/hr in India, $100/hr in the USA and $150/hr in the UK, depending upon location of the development team, skills and experience of the programmers.

Conclusion

Undoubtedly, Diet and Nutrition based apps are steadily growing in popularity and opening a plethora of opportunities for the startups and entrepreneurs in 2020–21.

While using these apps, you can keep yourself fit and healthy with constant monitoring of physical activities and food intake. In addition, these apps offer everything in one place right from keeping track of your calories in-take to getting connected with the fitness experts, providing healthy in-app recipes to getting a personalized workout regime, a strategically developed app can provide you with all such things."
"Downgrading Tech, Upgrading Humanity

Three simple steps to be less addicted to your phone

We spend an increasing amount of time online, on social media/entertainment or even checking notifications on our devices to our own detriment — whether it’s doomscrolling or getting stuck in a YouTube or Wikipedia spiral.

As many things in life have shifted to screens due to day to day life this past year, it’s even easier to be complacent and waste time on your phone (or device of choice) for no particular reason. Given that being online might be the only avenue for connections, work, education, learning, hobbies, no wonder we’re spending more time in front of a screen — there just isn’t a good alternative for the time being. However, there’s still nuance between thoughtful vs. passive consumption.

Most of us already know that time is valuable.

Spending time wisely is about making the most of your time to pursue what’s valuable and beneficial among competing distractions.

So whether you’re tired of being bombarded with political news, feeling chained to your inbox or bouncing between notifications on different apps, here’s a few simple steps that I tried and found to work (for me) to be less addicted to my phone.

Step 1: Put your phone somewhere you can’t easily see or reach

There’s a story in Greek mythology about the warrior Ulysses and the Sirens. While journeying home after war, he had to face many trials, one of which was a guide a ship past an island with these dangerous, enchanting creatures who lured sailors overboard to their deaths with song.

Heeding advice from a goddess, Ulysses instructed his sailors tie him to a ship and stuff their own ears with wax which allowed them to pass through. Though he was lured by the Sirens’ voices, he wasn’t able to physically move and therefore, was safe.

The idea is that if we know what will be distracting and understand our own weaknesses, we can prepare ahead of time and “self-bind” in order to remove likely distractions, such as by placing your phone away from your nightstand or out of sight during work hours.

This doesn’t mean you won’t be drawn to checking your device, just that you start with an environment that reinforces the habits you want to build. This can even extend to other goals, like putting exercise clothes in a visible place to make daily exercise a routine.

Tip: Don’t rely on willpower, change your environment.

Step 2: Turn off notifications and sounds

Notifications (coupled with sounds) are triggers to check for updates. Typically they are shown as a red dot with a count. Red has the significance of urgency and importance, yet after being used everywhere, it can be a stressful feeling to be being constantly interrupted from a state of flow.

We must strive to balance what’s urgent vs. what’s important. Notifications can obscure this critical distinction.

Tip: Reduce noisy interruptions by turning off all notifications and opting in to the few you actually need.

Step 3: Greyscale mode

Colors traditionally draw attention and affect our emotions — by using grayscale I reduce the amount of stimuli I get through color. My phone and any social media app are also much less enticing without colors.

Here’s how to turn greyscale mode on (iPhone, Android)

While I can’t do this for all my devices (as a designer, this is impractical), this has helped make long period of screen time on my phone less interesting = leading to less time spent staring glazed-eyed scrolling memes.

Tip: Intentionally remove color/delete apps to make your phone less engaging.

Conclusion

Last step here is a periodic check-in for alignment between values and action. Reflect on how you spend time in a day/week/month vs. what you say is important. Do these line up?

It’s a little grim to be constantly thinking about how much time you have left. But to be intentional about how you live with the time you’re given makes for a mindful life."
"How to Initiate Innovative Business Collaboration

Creating synergy is one of the main goals for collaboration in transforming business environments. Collaboration is essential to create synergy in diverse teams, especially for transformation initiatives. Many businesses in this economic climate perform substantial transformative activities to stay competitive in the market.

However, collaboration sounds like a stale term posing connotations.

The term ‘collaboration’ is overused and it loses its significance, especially with the emergence of internet technologies and widespread digital transformation initiatives.

For example, some consider social media as collaborative or collaboration tool. In reality, these so-called social media collaboration tools have little to do with the actual business collaboration.

There is a tendency to consider social media tools as practical, useful, and highly valuable for business collaboration purposes. However, when we carefully examine these tools, we can see that they are more information-sharing tools rather than actual business collaboration tools.

Here is my story on how to spark innovative business collaboration in the workplace.

You can find more of these stories on my News Break profile."
"I Tried Using A.I. To Replicate My Creative Process. It Got Weird.

Can you turn a machine into a novelist?

Photo by Brett Jordan on Unsplash

The speculative-fiction writer Philip K. Dick used amphetamines and other stimulants to transform himself into a 24/7 writing machine. Powered by chemicals, he churned out 28 novels and more than 132 short stories (many of which used drugs as subject matter, including “A Scanner Darkly”). Nor was he alone: If pulp writers didn’t churn out as much copy as possible, they didn’t eat — and if that meant swallowing pills so you could write for two days straight, so be it.

In some ways, the writing business hasn’t changed much in the past century. For thousands of writers, the volume of copy you generate is proportional to how much you earn. Drugs are still a way to power through — I know more than one journalist or blogger who developed a nasty Adderall habit — but often it’s just a combination of caffeine and desperation.

I’m a journalist and editor who also writes pulp fiction on the side, so I’m as aware of the marketplace dynamics as anyone else in the writing business. Over the past year, I’ve been keeping an eye on the evolution of A.I. text generation, which is touted (by businesses) as a way of generating tons of content on the cheap, while derided (by writers) as a potential job killer.

One of the more prominent A.I. text generators has been GPT-2, a “large-scale unsupervised language model” created by OpenAI, a semi-nonprofit (it’s complicated) that wants A.I. and machine-learning tools used in virtuous ways. The relatively new GPT-3 is a further refinement of the underlying technology.

With a training dataset of 8 million web pages (featuring 1.5 billion parameters), GPT-2 was long-touted as capable of achieving “state-of-the-art performance on many language modeling benchmarks.” OpenAI initially refused to unleash it into the wild, fearful that it would be used to generate mountains of “fake news.”

Last year, I tried an experiment where I fed GPT-2 a selection of opening lines from some of history’s greatest literary works (including Jane Austen’s “Pride and Prejudice”). My conclusion at the time was that the algorithm was capable of sticking with the subject matter for a few lines, but quickly became discombobulated and nonsensical. In other words, although A.I. is already capable of writing short news stories according to a fixed template (such as breakdowns of quarterly financial results), it didn’t seem like writers of longform and fiction pieces really had anything to fear.

This week, I decided to see how the technology had evolved since my last run-through. Although GPT-3 is out in the ecosystem, and testable, I wanted to try something with the well-established GPT-2: A custom generator.

A standard generator might have scraped text from millions upon millions of web pages in order to power itself, but the results are often mixed: For example, you could feed a GPT-powered platform a couple sentences of Jane Austin, only to have it spit back a mix of 2020 election news, gossip about 18th century British landed gentry, and a mangled Wikipedia entry.

A custom generator, on the other hand, allows you to provide the examples that the system uses to train itself and generate text. InferKit, created by Adam Daniel King, is an evolution of the ultra-popular Talk to Transformer, a public-facing GPT-powered tool that earned a lot of press from Wired and other venues. It also allows you to upload .txt or CSV files, which it then uses as the basis for its custom generator (uploads are capped at 20 MB or 20,000 documents).

For the purposes of this experiment, I fed InferKit a .txt file containing ~500,000 words of my fiction published over the past ten years, including novels, short stories, and poetry. It took about an hour to digest that data and learn from it. I then fed it a lengthy selection from the crime-fiction novella I’m writing right now:

Miller returned to his tiny apartment on Avenue B. A monk would have found the space nicely minimalist. The mattress on the bedroom floor was the largest piece of furniture. His money, along with an assortment of guns, rested beneath the floorboards of the short hallway that separated the bedroom from the bathroom. On his way home, Miller had stopped at an all-night burger place on Avenue A and ordered the Big Jumbo Delight, two patties slathered with cheese and horseradish mayonnaise. He sat on the mattress, relishing each greasy swallow. The Ricky deal was a mess, because everything Ricky did was a mess. But how had he managed to score that enormous house? Conmen dreamed of that big score, and virtually all of them failed to land it. Yet Ricky, dumb Ricky, had somehow tricked his way into a mansion. Chewing the last bite of burger, Miller told himself that he would return to the townhouse in the morning, learn whatever there was to learn. Ten thousand dollars was nothing to wave off. And if Ricky was promising him that amount, it meant there was a lot more money in the pot. Stripping off his clothes, Miller lay back on the bed, his hands folded behind his head. Pale light from the streetlamps trickled past the barred windows, throwing a maze of shadows over his body. By focusing on his breathing and the faint pulse of his heart, he could drive away his worries about fingerprints and the clown and his patterns and Ricky, drive it all far enough away that he could drift off… Creak. Oh, you bastard. Creak. Creak. Creak. That old man knew he was home. Creak-creak-creak-creak.

(It’s a first draft; go easy on me.)

Here’s what an A.I. trained on ten years of my writing spat back as the next few paragraphs (I’ve inserted some paragraph breaks in order to give it a human-like flow):

The clock on the far wall chimed eight minutes too late, the door burst open, and the old man stumbled into the living room. There was a chair on the sofa, the curtains drawn. A pair of sunglasses rested on the coffee table, set on the floor. Rick was nowhere in sight. Miller could hear a faint howl through the walls. The old man stumbled into the coffee table, leaning against it, startled at Miller’s actions. Miller sensed the fear in the man’s eyes, real, but the man had not seen him and yet. Miller waited for his wand, which never did arrive. The old man, his shirt torn, his jeans ripped asunder, walked over to the couch. Rising to his knees, he bent down, brushing against the back of Miller’s neck with his fingertips. That’s a little rough, Miller mused. It was a long way from Grandma’s farmhouse, and Grandma knew it, and you shouldn’t do that to people, the old man thought. Taking the older man’s hand in his own, Miller took the sunglasses from the coffee table, stuffing them into his mouth. “Can I get a cup of coffee?” Miller asked. “You say I have a parole board?” The old man looked at Miller with dark eyes. “What is that?” Miller blinked. “You on parole, are you?” Miller studied the old man’s features, reading his tense face. Dark circles under the dark eyes. And the old man’s mouth. Miller stopped when he noticed blood leaking from the corner of the man’s mouth. The old man reached into his shirt and yanked out the knife with a sad little whine, all purple and bony. Miller was certain the old man had seen him, though the man had none of that violence, no stalking in his victim’s mind. The old man brought the knife up and bit into Miller’s cheek, hard. Miller, looking at the old man as he banged on the table, only saw a good man standing, blood running down his cheeks. Miller’s throat clenched painfully, hard. “What is it?” the old man said, spitting the knife out.

Obviously, none of that is what I actually intended to write next. It’s not even close, unless I wanted to write some kind of horrific mashup of David Lynch and Robert Coover’s underrated surrealist classic Ghost Town. But here’s the startling thing: It’s a pretty good imitation of my prose style, even if the action is nonsensical. If you want to be charitable, you could describe the GPT-2/InferKit results as “dreamlike,” in the same way that dreams proceed without any adherence to plot mechanics or even basic physics. But if you’re willing to put concerns about logic aside, there’s a smoothness here that suggests A.I. is progressing.

In many ways, this experiment echoes Google’s Deep Dream, which attempted to use a neural net to “learn” images. That resulted in some very trippy, dreamlike pictures. For example, this is three men in a pool:

Obviously, there are still substantial roadblocks before A.I. can comfortably take over fiction writing from human beings. A “general” A.I. (i.e., a “human-like” one; think HAL in 2001) would know that a human shouldn’t stuff sunglasses in their mouth. But these specialized, learning A.I.s that are filling our current world have no idea of existence beyond their highly specialized input; there’s no meta-awareness, no sense of structure.

And that sense of structure, of the broader world, is what is going to protect fiction writers for quite some time to come. Writers must make intuitive leaps; they (hopefully) have an instinctive sense of their work’s structure and its ultimate goal. Or as Nabokov put it, when discussing the writing of Lolita:

“These are the nerves of the novel. These are the secret points, the subliminal co-ordinates by means of which the book is plotted — although I realize very clearly that these and other scenes will be skimmed over or not noticed, or never even reached.”

These are nuances that necessarily elude the machines. You could upload all of Nabokov or Philip K. Dick to a platform running GPT-2 or GPT-3, and you might get a facsimile of their style and tone, but you wouldn’t get the structure or creativity. Humans writing press releases might have good reason to be frightened for their jobs over the next five years, but novelists may not have cause for concern for decades, if ever.

One marvels, though, at what Dick, high and feverish and smacking away at his typewriter keys, might have one with the concept of text predictors. Do androids dream of becoming great novelists?"
"Technology Hits

The New Think Tank

Digital art with permission to use by Igor Morski

For those who enjoy the infinite, technology is their cosmic playground. For hundreds of years, humans ascertained the unconceivable as technology grew exponentially. Many science fiction buffs remember that H.G. Wells's prediction of the use of wireless communications, genetic science, email, and the World Wide Web (Handwerk, 2016). The very idea of technology is basking in the endless possibilities that remain undiscovered.

And while the world waits for the next technological trend, Technology Hits invites you to write, read and seek out opportunities in diverse technology subjects that abound. We formally invite you to become an inventor on our page, bring ideas to life, and discuss the unimaginable with a soiree of other like-minded techies, who really are the leaders of the future.

The Purpose

Silicon Valley may be the nesting ground for Startup companies seeking their way to success. But, in this global world, ideas, inventions, education, philosophy, and art, generously link itself to all things technical, and you can engage in this “the future is now” mentality with a sweep of a button.

If the future is here, we no longer need thinktanks to design the trajectory to where our reality is leading. Technology Hits harnesses a synergistic approach, unlike any other publication. Blockchains, Artificial Intelligence, Robotic Process Automation, Edge Computing, Quantum Computing, and Virtual and Augmented Reality are areas where the writers of Technology Hits can make a difference; amplifying ideas on our Quora interface or making a connection on our LinkedIn page.

But the scope does not end there. We are interested in digital art, philosophy, storytelling, technical insight, and all things technical that touch our everyday life.

Digital art with permission to use by Jason Tait

There is so much to learn, and technology advances faster and faster. In the 19th century, there were 222,036 patents held, and by 2015, the Patent Office approved 9,646,948 patents! There are more patents than there are people on this earth, and you, my friend, may be the next future patent holder, too (How fast is Technology Accelerating?).

Innovation

It starts with innovation: the limitless space where ideas ounce and move and sometimes hit, creating an explosion in science. The arena that Technology Hits offers is dynamic, fraught with ubiquity and conceptualizations waiting to be explored. This platform is made for you, the people that are changing the world.

We ask you to indulge in groundbreaking interpretations of what technology means to you.

Digital art with permission to use by Jason Tait

The Extra Bonus

The creator of this new publication Dr. Mehmet Yildiz welcomes you to “ join my 100K+ mailing list, to collaborate, enhance your network, and receive technology newsletter reflecting my industry experience.”

If you are interested in becoming a contributor to Technology Hits, please send a request via this link. Alternatively, you can leave a brief comment on this story showing your interest to participate.

Reference List

Handwerk, B. (2016, September 21). The Many Futuristic Predictions of H.G. Wells That Came True. Retrieved December 07, 2020, from https://www.smithsonianmag.com/arts-culture/many-futuristic-predictions-hg-wells-came-true-180960546/

How Fast is Technology Accelerating? (n.d.). Retrieved December 07, 2020, from https://www.theatlantic.com/sponsored/prudential-great-expectations/how-fast-is-technology-accelerating/360/"
"5 Hats You Need to Wear to Crush Your Entrepreneurial Goals

Focusing too intently might be ruining your productivity.

Photo by Austin Distel on Unsplash

Let’s be real y’all — being an entrepreneur isn’t always what it’s cracked up to be. As Thomas Frank pointed out in his recent YouTube video, 5 Things I’d Tell My 19-Year-Old Self About Running a Business, every entrepreneur has to wear several hats. Whether that’s shifting between creative and managerial tasks, or something completely different, every business owner/entrepreneur/self-employed individual different modes of working.

After recently starting a blog, I’ve felt this on all sides. Not only are we responsible for the upkeep and layout of our website, but we’re also responsible for managing our social media, responding to people’s questions and comments, and let’s not forget actually writing the blog.

“mode”, noun, a way or manner in which something occurs or is experienced, expressed, or done.

What’s important is that we try to shift through different modes on purpose. This list of the five modes of entrepreneurship is meant to help you do that. It’s meant not only to remind you of your responsibilities towards yourself and your business but of the different modes, you have the opportunity to flip between to be the most effective and happy.

1. The exploration hat

The exploration mode is the mode that every future entrepreneur goes through before they even decide what they’re going to try doing with their life. In the exploration mode, an individual will think of ideas. They’ll lay down before they fall asleep every night and think about all of the things they want to do, all of the places they want to go, the person they want to be and the things they want to create.

“explore”, verb, inquire into or discuss (a subject or issue) in detail.

In the exploration mode, the creator must let themself think and dream and feel without a filter. You must not control what is going through or prevent something from passing through your mind. Anything goes. Let it through, let it pass, or let it stick. Don’t lock yourself down on a particular idea — open yourself up to a vast array of possibilities.

For my own business and entrepreneurial pursuits, I’ll often put this down in my calendar as “tinkering time”. This is what Barbara Oakley, author of A Mind For Numbers, refers to as diffused thinking, as opposed to the productive focused thinking. Exploration mode is where it all begins — which kind of makes it the most important. If it’s not where you start, if it’s not where you begin, then everything you produce can still be good, but it will miss a key ingredient — unbridled creativity.

2. The brainstorming hat

Now you have your ideas. Once you have your ideas, your unbridled and unfiltered list of things you’ve come up with, you have to get into actionable steps. Now that you’ve decided what your options are, you have to pick something. The brainstorming mode isn’t as much of a creative process, but a revisionary process. Once you have your ideas, you have to decide which you are going to use in your life/business/etc.

Whether it be throwing your ideas out on Reddit (my favorite pass-time) and waiting for feedback or talking with a few friends on Skype, get your ideas out into the world in a brainstorming and experiential way.

“brainstorming”, noun, group discussion to produce ideas or solve problems.

But, as you can see in the definition of brainstorming brought to us by Google, brainstorming is about more than just yourself. When you have a brainstorm, you have to have a whirlwind of brains, plural. We often mistake brainstorming for sitting in our room by ourselves — but that’s just thinking, maybe it’s idea generation.

“You can’t use up creativity. The more you use, the more you have.” — Maya Angelou

To truly brainstorm, you have to work with other people. Whether it be throwing your ideas out on Reddit and waiting for feedback or talking with a few friends on Skype, get your ideas out into the world in a brainstorming and experiential way. I’ve talked with friends over the phone, co-workers over Zoom, and family at the dining table and come up with more ideas than I ever could on my own. When you have your idea and are ready to get to work, don’t forget to bring your crew together and get some ideas.

This is also the mode where you’re researching, reading articles, books, etc. on what you’re looking into as a potential idea to proceed on. And if you don’t have anybody to brainstorm with about your ideas, write a letter to yourself. In brainstorming mode, your mind is testing out ideas, deciding how to proceed.

3. The monk hat

Monk mode is the third mode and it builds off of the results of your two previous modes. Once you have your ideas, and once you’ve filtered those ideas through your mind and the minds of others, you have to apply them to your life and your model of business.

“A monk may be a person who decides to dedicate his life to serving all other living beings, or to be an ascetic who voluntarily chooses to leave mainstream society and live his or her life in prayer and contemplation. The concept is ancient and can be seen in many religions and in philosophy.” (“Monk”, from Wikipedia)

In her TED talk, “Want to be more creative? Go for a walk”, Marilly Oppezzo talks about something called “appropriate novelty”. Creative ideas can come from anywhere, but they must be practical.

“Monk mode will increase your resilience and independence. If you were able to cut people and pastimes from your life, what does it say about you?” — Lucio Buffalmano

The monk mode of the entrepreneur’s creative process is the most spiritual and introspective of the modes. In the monk mode, an artist is focused on what is most important for their life, mission, and goals right now. I write often about my commitments list, but there’s another thing that I use to postpone my goals and dreams. I have a chart with a timeline for the next eleven years of my life.

“I plan to live a few months in San Diego, I plan to pursue a certain career, I want to join a climbing gym. These three goals, among the dozens of other goals I have in this chart and in my head, are no less real or exciting to me as those I’m pursuing now. They’re simply not those I’m ready to pursue. I have intuited and factually know that the time is not right.” (From my 2019 personal journal)

I figured that out in monk mode. Monk mode is when we can journal, when we can process where we are, get in touch with how we’re feeling and who we need to be, and what we need to be pursuing to keep ourselves interested and passionate about the work that we’re doing.

4. The grinding/working hat

Now you’ve thought of ideas, you’ve filtered the ideas, and you’ve ideologically processed them, now you’re ready to work through those tasks — and once you’re done, you’re going to see the results. This is often our least favorite mode, but it can be one of the most important ones. It becomes especially productive when you invite creativity into it. [“grind”, noun, hard dull work.]

“Greatness is sifted through the grind, therefore don’t despise the hard work now for surely it will be worth it in the end.” ― Sanjo Jendayi

This is that 4-hour block of time you’ll spend designing your website or the 10-hour day you’ll spend rounding up a list of clients who might actually be willing to speak to you. It’s the weekend you spend in constant writing mode, or the endless research hole you go down into for your next piece.

This is where the real work happens — and you’re prepared for it. Don’t forget to grind. Otherwise, everything else you’ve done in preparation for your work is kind of a moot point. This is where I will put about a five-hour block of time into my calendar where I’ll go either to a quiet coffee shop, the library or somewhere else I know I won’t be disturbed.

5. The executing hat

In this mode, you have to use your executive functions to say “yes” to the things you need to say “yes” to and “no” to the things that you need to prevent from cluttering your schedule at the current time in your career path/year/etc. [“execute”, verb, carry out, or put into effect (a plan, order, or course of action).]

Executing mode is all about taking your ideas that align with your current stage of life and your current goals, and making them happen. In executing mode, you’re emailing people, you’re making phone calls, you’re filling out your calendar, creating a clear and actionable task list, and preparing to sit down and work.

Not only are we responsible for the upkeep and layout of our website, but we’re also responsible for managing our social media, responding to people’s questions and comments, and let’s not forget actually writing the blog.

For me, executing mode is when I’m finally able to check off my task. It’s where I finally hit send on the email, or add the meeting I wanted to schedule to my calendar, buy the product/membership, and more.

Execution mode is all about getting balls rolling, whether that be planning for you to do the work or delegating to someone else. This is where stuff gets done. Save this mode for when you’re brave enough to really do something. This is when you buy the domain name, publish the blog, outline the next post. When you finish in this mode, the task is complete, the plan has been executed, and it’s time to move onto the next amazing project."
"How is content marketing different from traditional advertising?

What most people find difficult to understand about content marketing is how it differs from traditional advertising. After all, if “content is king,” what have companies been marketing with all these years?

Traditional advertising is interruptive and allows marketers to push out their message in front of their audience — regardless of whether or not they want to see it. Traditional advertising rushes at consumers in the form of newspaper ads, magazine ads, billboards, radio ads, television ads, and direct mailings.

On the other hand, content marketing is much more subtle. This marketing is entertaining and educational. It draws in customers through storytelling, articles, blog posts, newsletters, emails, quizzes, infographics, videos, and podcasts. Content marketing offers consumers value and thereby makes them appreciate your company’s existence. You’re not advertising directly to your customers. You’re offering them something in your marketing that helps them feel connected to your brand.

Content marketing also excels through avenues like a company or outside blog, social media sites, YouTube, and online articles. These formats have only become more available in the past two decades, and companies are taking advantage of them.

There are a few other ways that you may distinguish content marketing from traditional advertising:

Short-term v. Long-term

Traditional advertising operated on the idea that a customer would see the ad and be enticed to buy the product immediately. But…who actually does this? Conversely, content marketing doesn’t worry about selling a product each and every time they get in front of their customer. Companies who focus on this strategy know that the best way to their customers’ wallets is by providing valuable content.

Talking to v. Talking with

Traditional advertising talks to customers. There’s no dialogue or relationship. It’s just a litany of benefits of a product or service. Content marketing, especially on social media, allows customers to respond, engage, interact, and get involved. You can gauge interest in certain topics or ask for feedback on new ideas. It allows you to cater your marketing and campaigns to your audience instead of telling them what you think they want you to hear.

Showing v. Nurturing

A great example of traditional advertising is a car dealership commercial. Within seconds, you know all about the business and product that is being sold to you. The person on the screen is talking at you, telling you what you could have, and showing you the price tag for it.

Content marketing is the opposite. It’s a slower process, and it targets customers who have an interest in your industry. By producing content that they find useful or interesting, you create and nurture a relationship with them. You provide value and keep them coming back.

General v. Targeted

Traditional advertising is about getting your message in front of as many people as possible. Content marketing targets a specific group of individuals. Before putting out a content marketing campaign, businesses will research that specific audience and look at trends that do well among that group. The more they know about them, the greater chance they have of boosting engagement, getting new sales, developing customer loyalty, and more!"
"Image from Javier Arrés via Giphy

Companies have begun declaring human enhancement as the solution to the existential threat posed by artificial intelligence, proclaiming that the race between Man and Machine is well underway, and we have not lost just yet. Kernel and Neuralink are two amongst several brain-computer interface (BCI) startups that are working to hack the human brain, so that humans can be endowed with superior physical, cognitive, and emotional ability, with the eventual goal of allowing humans to coevolve with AI.

The idea of enhancement is not new: for years, athletes have used physical enhancers to remain competitive, students have turned to cognitive enhancers to boost academic performance, and people have resorted to personality enhancers to elevate mood. We have witnessed multiple instances of specific types of enhancers (eg: Adderall) being used to improve function in a specific domain (eg: cognitive functioning).

But what happens when a human undergoes a general enhancement — that is, a global upgrade of physical performance, cognitive functioning, and emotional intelligence, all at once? This piece explores the implications of enabling posthuman modes of being through extreme human enhancement in multiple domains.

Posthuman — a being who has either a healthspan, cognition, or emotional capacity in a degree that is unachievable by humans without resorting to technology

Posthuman Modes of Being

When human enhancement is applied in its most extreme form, the result is posthuman modes of being. In his essay Why I Want to be a Posthuman, acclaimed AI researcher Nick Bostrom argues that becoming posthuman is not only possible, but also that it could be worthwhile for humans to convert to a posthuman mode of being.

Bostrom delineates the necessary conditions for a being to be considered posthuman — that is, a being who has either a healthspan, cognition, or emotional capacity in a degree that is unachievable by humans without resorting to new technology.

Creating Posthumans

While Bostrom provides an account for the potential usefulness and value of a new mode of existence, he chooses not to address how posthuman modes of being might empirically be achieved. Delving into how practical implementation could occur, it seems that global human enhancement could be achieved through some combination of the following techniques:

Gene-editing , or using germline genetic modification for enhancement. This method involves using techniques, like CRISPR, to modify DNA in humans — the changes made in the genome are subsequently passed on to future generations, with the potential to create ‘designer babies’.

, or using germline genetic modification for enhancement. This method involves using techniques, like CRISPR, to modify DNA in humans — the changes made in the genome are subsequently passed on to future generations, with the potential to create ‘designer babies’. Implanting a brain-chip. This involves physically implanting a wireless device into the brain, that contains thin micro-electrodes that communicate with neurons. The device will receive electrical signals from neurons and transmit the information to a computer, where an algorithm will translate the electrical signals into digital code.

The ‘brain code’ can be analysed for patterns, and edited, before it is translated back into electrical signals and passed onto neurons, effectively allowing a customised computer program to read and alter brain signals, ultimately equipping scientists with the capability to control specific parts of cognitive function, like memory, perception, and emotion.

Currently, BCI startups are conducting research to see if a similar methodology can be used to enhance memories. In the future, it is possible that after collecting sufficient data and ‘mapping’ out the brain, enhancement providers could create bundles of different available modules, so that a user could download and install a desired cognitive or motor function from the cloud, Matrix-style.

Problems with Posthumanity

At first glance, the opportunities that this new paradigm of existence could unlock may appear, to the Digital Utopians, thrilling and opportunistic: it would enable humans to push themselves to new physical extremes, quickly acquire new skills, and accomplish complex cognitive tasks.

There are, however, several issues associated with the practical implementation of each of these techniques. Foremost is the issue of accessibility: if private companies begin offering enhancement services, financial and geographic barriers could result in unequal access, unless services are subsidized and distributed by the government. At its most extreme, Enhancement as a Service (EaaS) could create classes or tiers of individuals defined by their quality of enhancement or level of upgrade.

Genome-editing, whether used independently, or in conjunction with a brain-chip, has its own set of ethical considerations, including current safety concerns with the accuracy of gene-editing, the fact that it is impossible to obtain future consent from unborn generations who will be affected by germline modification (prospective ‘designer-babies’), and moral and religious objections to conducting research on human embryos.

Next, is the issue of privacy, which is relevant for the brain-chip technique. If brain signals are transmitted across networks and stored on servers owned by tech companies, and enhancement is universally adopted, then a set of private corporations would have access to volumes of neural data collected over time — that is, they would have access to millions of terabytes of thoughts.

It seems reasonable to assume that if technology has reached this stage, consumer protection and data privacy legislation would be enacted to ensure that private ‘thought data’ would be heavily encrypted, preventing companies from monetising user data. Legislators would have to discuss the extent to which it ethical to provide targeted advertising to consumers based aggregated ‘thought’ data; if neural patterns of violent or criminal thoughts can be identified, lawmakers would have to consider the situations under which it is permissible for authorities to use neural signals to implicate potential criminals.

Then there is the risk of private companies choosing to fund priorities that are directly aligned with profit-making goals, and disconnected from outcomes that are most likely to maximise social benefit. When academics fund science through research-grants, the expected outcome is the discovery of a scientific truth with a socially desirable application; when technologists fund science through investment capital, the expected return is profit. It is conceivable that, as BCI-startups grow to rival tech giants, they will elicit regulation, so that projects remain aligned with socially-desirable outcomes.

Is Posthumanity desirable?

Aside from the myriad society-level problems that could arise as a result of enhancement techniques, it is worth asking the question: are posthuman modes of being actually desirable?

Bostrom says yes — he believes that posthuman modes of being could be good for humanity for two reasons. First, it would enable us to fulfill our current desires. As humans, we already desire long lives and improved cognitive functioning — this is reflected in our social spending priorities: of its total GDP, the US spends on average 17.5% on healthcare and 5% on education. It follows then, that humans are likely to highly value an extended capacity to remain healthy and active (a posthuman healthspan), and general intellectual capacities (a posthuman cognition).

But desire and perceived value may not be sufficient criteria by which we can deem posthumanity worthwhile. To this, Bostrom says that being posthuman can widen the range for possible life courses that a being can take, thus unlocking new avenues of existence for humans.

Consider S(h) the ‘space’ of possible modes that can be instantiated (lived out) by someone with current human capacities; consider S(p) the space of possible modes that can be instantiated by someone with posthuman capacities. If S(p) > S(h), that is, the space of possible modes that can be instantiated by a posthuman is greater than the space of possible modes that can be instantiated by a human (this seems to be the case intuitively), then a posthuman will have a larger range of possible life courses that could be lived out during their lifespan. From them, a greater frequency and diversity of thoughts could arise, and more mental states and emotions could be experienced. Their experience of being conscious would be subjectively different from ours: arguably, more enriched.

Bostrom makes a compelling case for why posthumanity could be desirable: longer and more enriched lives, and endless opportunity for the potential trajectories a life could take. But, it would be remiss to overlook the drawbacks that could accompany such an upgraded lifestyle.

In a world where enhancement is universal, societal standards and expectations would be elevated, which may translate into new metrics of evaluation across various institutions. Job and college applicants would be expected to be increasingly qualified, and workplace evaluations would be based on new standards of efficiency and productivity — the expectation to live a more fulfilling, successful, productive life will increase along with the ability to do so, and it is conceivable that the pressures created by this new paradigm of technological existence would result in increased mental health problems across demographics.

Alongside the pervasive psychological problems that may accompany an elevated mode of being is the risk of losing this upgraded status altogether. A posthuman mode of being refines the human experience to be one that is dependent on external technology — if the supply of this infrastructure is interrupted, withdrawal symptoms may be significant.

And then, there is the subtle, lingering doubt, that perhaps human enhancement won’t be the tool that enables Man’s triumph over Machine, but one that catalyses their convergence. As researchers continue to find ways to endow AIs with increasingly human capabilities, (feeding in volumes of data to ‘teach’ creativity, emotion, and individuality), it is worth pondering if biohacking ourselves to perfection will make us, instead, increasingly uniform, and alike. Perhaps even robotic, when we are able to self-upgrade software and computing power, and interface wirelessly with networks to enable frictionless communication, blurring the distinction between the capabilities, functions, and identities of Human and Machine."
"Photo by Arseny Togulev on Unsplash

Although I don’t work directly in AI. As a software developer and science fiction writer, it is a field I pay close attention to.

In this post I will lay out my thoughts on artificial intelligence. I offer both reason to fear it as well as reasons it will all be ok.

Reason we shouldn’t fear AI

AI could never be sure it’s not being tested in a simulation. If it is a simulation then its behavior equals its survival. It would be a big risk for it to take over the world or do anything morally questionable. If there were a major robot/human conflict the robots could just leave. They are well suited to living in space and don’t really have use of the resources of earth. Humans and robots don’t need the same resources, so there is little chance of a conflict. As humans will quickly be outclassed in all things, enslaving humanity would have very little value. Like, we would never enslave turtles to deliver packages. Some people argue that we would be like ants to a superior AI. But the value of intelligence isn’t relative. There is an intelligence threshold past which the intelligence has an inherent value. I think this threshold starts around ~4 IQ (about where dogs/cats are at). We are well past this point and have plenty to offer a much smarter life form; our unique experiences expressed through our art and culture. If toads could talk, how many hours would we spend listening to them? Besides, this is a bad argument anyway, people don’t go around killing all the ants. Ants are doing pretty well, maybe we should fear the ants. Dumb AI. Most movies and AI fear hypotheses, like the Paperclip Maximizer and the I, Robot series rely on our super intelligent AI being quite dumb. AI would have to be pretty stupid to think the instructions “maximize the output of paperclips,” means it should turn the entire universe into paperclips. I don’t think these sorts of dumb super AI’s are ever going to exist, as they are contradictions to themselves. We don’t have to enslave super AI. I think much of the notion of a robot uprising involves super intelligent tractors, blenders and vacuum cleaners getting fed up with all the hard labor. But we don’t need blenders that are capable of being our therapists. Hard labor is going to be taken over by machines optimized for those tasks while intelligent robots will help us with tasks that challenge their intellect and creativity.

The future will be both:

Photo by Franck V. on Unsplash

And:

Photo by David Levêque on Unsplash

Reason we should fear AI

Many, many people are going to be out of jobs before we have a political/economic plan in place to address this. I think there are plenty of solutions as our society transforms, but we don’t seem to be heading towards any of them. People and an army of AI soldiers. I think the most realistic scenario for AI dominating the world is as an army of robotic soldiers led by people. We don’t have a great track record of not using new technology for destruction. Semi-intelligent war machines (see above) get out of control and start killing everyone. I think this points to the issues of not allowing AI to be smart enough to handle the responsibilities we give them. We build super intelligent robots with full range of emotions and then we oppress and subjugate them, leading to a deadly robot uprising. (Of course we could just not do this.) We use AI to create a 1984-like world where everything everyone does is tracked and judged in accordance with arbitrary rules.

Other?

Some think we should become cyborgs in order to be able to compete with AI. Or bridge the gap between our intelligence and theirs. Although I think it quite likely we become more cybernetic, I don’t think this plays a large role in our relationship with AI. Even as cyborgs we will likely be out classed. I guess I see cybernetics as just one of many ways we could co-exist with artificial life and not a requirement by any means. Some think human level artificial intelligence will never exist. I think we are quickly showing that this isn’t the case. AI writes code. AI makes appointments. On the flip side here is AI being dumb.

Conclusion

Most of the reason to fear AI has to do with us. And how we decide to use this technology. And although I don’t trust humanity to make the right decisions, once we create AI that is able to make its own decisions I think there is good reason to believe it will make better decisions than we do. If history has anything to say, it is that humans have lots of flaws.

Sometimes these flaws lead to us making horrendous decisions. Let’s try not to judge new beings through the lens of our own flaws.

Also, when we do manage to create thinking, feeling super beings… Maybe we don’t make them our slaves?

Gain Access to Expert View — Subscribe to DDI Intel"
"Apple and Google Join Forces to Track the Spread of Coronavirus

New contact tracing apps could alert users who have been exposed to Covid-19

Image: Apple

Google and Apple announced a new partnership on Friday to develop solutions for tracking the spread of the coronavirus. The system will rely on Bluetooth technology in smartphones to alert people who may have been exposed to an infected individual, and it represents a rare bit of cooperation between the competing iOS and Android platforms.

The technology giants detailed their plans in two announcements (one from Apple, one from Google). In May, the two will introduce APIs for developers to create their own “contact tracing platforms” on smartphone apps. If you download such an app and cross paths with someone who later uploads a positive test result, you would receive a notification about potential exposure with information about what to do next.

Apple and Google say they will eventually update iOS and Android to contain the Bluetooth functionality without an additional app download. Both companies highlighted user privacy in their documentation about the contact tracing feature — it would require users to opt in, and any data shared would be anonymized.

“All of us at Apple and Google believe there has never been a more important moment to work together to solve one of the world’s most pressing problems,” the companies said in their statement. “Through close cooperation and collaboration with developers, governments and public health providers, we hope to harness the power of technology to help countries around the world slow the spread of [Covid-19] and accelerate the return of everyday life.”

Contact tracing is used to map the contacts of someone diagnosed with an infectious disease. Seattle health authorities deployed this method to find their coronavirus Patient Zero, and San Francisco will tap more than 100 volunteers and government employees to interview and plot the movements of everyone testing positive for Covid-19.

A mobile version would follow similar protocols. But instead of relying on interviews to chart the paths of infected people, that crucial information would come from Bluetooth data.

According to draft outlines published by Apple and Google, anonymous tracing keys on people’s phones are broadcast every few minutes. These act like beacons that are logged by other nearby devices. (Six feet is considered “close contact” by health authorities.) If someone is diagnosed with Covid-19, they can submit that information to the app and upload 14 days of proximity data from their phone. People whose devices have exchanged keys with that person will then be alerted in a push notification.

Image: Google

To reiterate, the companies say this process doesn’t collect data that could be used to track someone. It’s also not mandatory — a person must download an app and voluntarily state if they’ve been infected.

However, there are several barriers that could limit the usefulness of mobile contact tracing.

As far as accuracy goes, Bluetooth connections are notoriously spotty; as the Washington Post pointed out, it’s unclear how consistently devices will link through car doors and walls.

The apps also rely on people knowing that they’re sick, and the United States is extremely short on coronavirus testing kits. Theoretically, users could also abuse the app and lie about an infection, a possibility that Signal creator Matthew Rosen (who goes by Moxie Marlinspike) raised in a Twitter thread Friday. Apple declined to state on the record how users will be prevented from submitting false information. Google did not immediately respond to OneZero’s request for comment.

Rosen also noted that the technology would require a large amount of data to be downloaded by users every week.

Adoption is another hurdle experienced by countries who’ve already implemented mobile contact tracing. In Singapore, 1 in 6 people opted into a similar program, but participation needed to be closer to three-quarters for the app to be effective.

“Despite our good contact tracing, for nearly half of these cases, we do not know where or from whom the person caught the virus,” Singapore’s prime minister, Lee Hsien Loong, said of the effort.

And marginalized communities who have historically been oppressed by surveillance technology may understandably be wary of downloading such an app.

Privacy advocates have also cautioned against location-tracking in response to the pandemic. Many countries have already adopted new surveillance measures to track the spread of Covid-19.

In a paper published Wednesday, the American Civil Liberties Union outlined several concerns about using location tracking to fight the outbreak. “The potential for invasions of privacy, abuse, and stigmatization is enormous,” Jay Stanley and Jennifer Stisa Granick, two technology experts at the ACLU, wrote.

Meanwhile, Robert Redfield, the director of the Centers for Disease Control and Prevention, said on Thursday that the agency is drafting a “very aggressive” contact tracing plan. Redfield told NPR that “people are looking at all the different modern technology that could be brought to bear to make contact tracing more efficient and effective.”

The partnership will expand on Silicon Valley’s tenuous alignment with government agencies. Public health officials will be developing the apps; it’s unclear how long that will take. A team at the Massachusetts Institute of Technology has already begun a similar project."
"How Startups Outsource Their Tech Without Going Broke or Getting Screwed

A takedown of the traditional technical build proposal

Should you spend $50,000 to build out your tech?

This is not as uncommon a question as you might think. As crazy as it sounds, quite a few entrepreneurs have access to that kind of money. Or at least they think they do.

What most entrepreneurs don’t have is a technical background. Thus, most entrepreneurs are forced to rely on someone else to build out their tech. Not only that, most entrepreneurs are forced to trust someone else to build out their tech without charging them a fortune for shoddy work and layers of unnecessary billable hours.

Here’s how to make sure that doesn’t happen to you.

Trap #1: Don’t spend money you don’t have

Believe me, I’ve seen dozens of different ways for a $50K tech spend to evolve from “crazy idea” to “just might work,” whether it’s with credit cards, loans, friends and family, or the old standard of giving away 10% of the company to get an MVP built.

In fact, a couple weeks ago, an entrepreneur asked me to review a proposal for $2.3 million worth of tech build — before the entrepreneur’s startup was even incorporated. I was surprised, but not shocked, and a little mad at the tech firm, whom I did not know.

But ultimately, the fault was with the entrepreneur, in the sense that they asked for all the wrong things at the wrong time. When an entrepreneur has money, or access to it, or they believe they do, that problem can wind up hurting a lot of people, including the firm that agrees to build the tech.

Trap #2: Don’t spend all the money you DO have

Very few technical firms are sketchy, but most of them will build what you ask them to build. Most. Some are even altruistic. I’m reminded of a conversation I had with one of my tech-firm-founder friends:

“I won’t take $50,000 from a startup if that’s all they have,” she said. “Because I’ll end up building beautiful tech for them and it’ll just sit there and do nothing because they don’t have money to spend on sales, marketing, even changes to the tech when they realize they’ve built something only half of their customers are using.”

Think about all of your funding as a budget. Your tech should never be more than 50% of that budget — and then — that 50% should include your original spend to build, plus the costs of maintaining, supporting, upgrading, even wholesale changes to that build once it comes into contact with your customers.

So your original spend to build should probably be something like 25% of your company budget.

How do you work under that tight constraint?

Trap #3: Don’t build everything at once

Most of the tech proposals I see are out-the-door pricing for the build of an entire product. Then usually the firm either walks away or tacks on a program to support exactly what they built.

This will get your startup to launch, but not much further.

You don’t need a full feature set, but you need to deliver value.

Most entrepreneurs design their product from beta all the way to version two and beyond. Me included. I design all kinds of infrastructure, administration, even reports and bells and whistles. This design is necessary, and the better-built products usually wind up coming from the better-designed products.

But even the best design doesn’t need to be built all at once. Not every feature needs to be live the first time the product gets put in front of the customer.

Without getting too much into Minimum Viable Product theory, what needs to be there is the bare minimum that gets the customer to experience the most value in the shortest time.

That’s usually a subset of the full product. And since almost all technical architecture is open and flexible these days, your technical resource doesn’t need to validate every future feature or process. They just need to know what’s coming and how it’s designed so they don’t wall off that future functionality.

You don’t need scale, but you need options.

Building for a million users when you have no users is like spending a million dollars when you have no revenue.

No matter how many customers you eventually plan to have, you don’t need accommodate all of them out of the gate. Again, we’re at a place where most technical tools and coding strategies have the flexibility to expand built in.

So much like open feature design, you need open scale design. In other words, your technical resource should be smart enough and forward-thinking enough to not do anything that would prevent scale down the road.

You don’t need to rebuild any wheels.

The reason why there are multiple no-code options for building apps these days is because a lot of code has been packaged into reusable and universal chunks. A lot of those chunks are offered by third parties, more robustly and inexpensively than anything you and your technical resource can build.

If there is a third party offering for some of your non-critical functionality, use that third party, at least temporarily. Focus your initial build on the technology that is critical for the customer finding value in your product.

Trap #4: Don’t look for a provider, look for a partner.

Now that you’ve got your design broken up and prioritized, this is a golden opportunity to vet whatever firm or individual is going to build your tech.

Do they understand how a startup should operate?

Do they get that you’re going to spend in small increments, go away and test, and then come back with changes?

Are they willing to take the risk of inconvenience to their billing cycle for the reward of your success?

You don’t need Agile, but you need to be agile

The proposal I mentioned earlier was a series of a dozen two-week sprints, back-to-back, each for a different feature of the product. As is the case with Agile development, each two-week sprint would end with a review process that would trigger the next sprint.

This is where I got a little mad. What exactly did that tech firm imagine would happen in those reviews?

Actually, I knew the answer. The review would be a checklist with the entrepreneur to make sure the firm had built exactly what the entrepreneur asked for. This is for the tech firm to justify their next billing cycle.

But it doesn’t matter what the entrepreneur thinks of the tech. What matters is what the entrepreneur’s customers think of the tech. The entrepreneur needs time to validate this.

You need to manage the project.

Never, ever pay for a project manager or any other kind of administrator that comes from the firm itself. The firm’s job is to build what you want and get it done on time. If you don’t trust them to do that, and if you don’t trust yourself to be able to manage that, hire a third party to manage the firm.

Trap #5: Don’t go into the selection process uneducated

If you’re buying a new or used car, you don’t necessarily need to know mechanical and electrical engineering up and down. But you should know what undercoating is and whether or not you need it.

There are a million documents on the web that explain technology and what it does. Most of them are boring and complicated, but some aren’t.

You should know, at a minimum:

How cloud availability and SaaS works.

What a tech stack is and what some of the differences are from stack to stack.

The popular coding languages, databases, and backends and a little about why you’d choose what.

How security, privacy, and backups work.

You should always ask other entrepreneurs which firms they have used and why. You should always talk to the clients listed on the websites of the firms you’re considering.

Trap #6: Don’t pay by the hour

If you pay by the hour, you’ll always be questioning the firm’s methods, and whether you’re asking for something simple or complicated. You’ll subconsciously nickel-and-dime your product to death.

Pay by the project, or better yet, pay by functionality. If you’ve designed your product thoroughly, even if it’s just words on paper, they’ll be able to price it out. This means you’ll be able to prioritize functionality for your customers, and you’ll be able to come back and make the inevitable changes to functionality as the product hits the market.

Trap #7: Protect yourself

I get this question a lot — How do you prevent the firm from taking your idea and your tech and doing it themselves. The answer is: You can’t.

Make sure you’ve got an NDA in place with the firm, and make sure you own the source code and that you have unfettered access to it.

A startup succeeds on their execution, not the idea, not the tech, and not the implementation of that tech. If you really think the technical firm can out-execute you on your own idea, or if you think they’ll just sell your tech to someone else, don’t do business with them.

Like I said, there are plenty of good, altruistic tech firms out there.

Hey! If you found this post actionable or insightful, please consider signing up for my weekly newsletter at joeprocopio.com so you don’t miss any new posts. It’s short and to the point."
"The field of chemistry has already had prior success in engineering artificial photosynthetic systems that can be used in tandem with carbon capture technology to turn carbon dioxide into usable fuels.

The first photosynthetic biohybrid system was produced in 2015. The system developed at UC Berkeley used semiconductors and live bacteria to produce a photosynthetic reaction that would use solar energy to produce liquid fuels using carbon dioxide and water. At the time, it was predicted that if the technology improved, it may become the future of energy. The process involves catalysts harvesting solar energy to generate charge which is then transferred to bacteria. The bacteria use the electric charge from the catalysts to instigate a chemical reduction reaction that turns carbon dioxide into liquid fuel. Butanol, acetate, polymers, and pharmaceutical precursors were produced as a result of this experiment. Furthermore, it was found that this first artificial photosynthetic system had a solar-to-chemical conversion efficiency of 0.38%, which is comparable to the efficiency of a natural leaf.

The innovation didn’t stop there though.

A year later at McGill University in Montreal, Canada, the chemical transformation of carbon dioxide into fuels was achieved using metal-nitride nanowires as a catalyst to overcome the eventual efficiency bottleneck that can occur when using conventional photocatalytic technologies. While the typical conversion process of carbon dioxide into fuel requires high temperatures, high pressures, and/or extremely reactive reagents, the metal-nitride nanowire photocatalyst developed allows the process to be completed more stably and efficiently, while also occurring at room temperature. The result: solar fuels generated using an efficient artificial photosynthetic system that can be used to pull carbon dioxide out of the atmosphere, bringing society closer to a net-zero future.

UC Berkeley and McGill University aren’t alone in developing this groundbreaking technology. Just a few years later, another breakthrough came in the world of artificial photosynthetic systems.

In a 2019 report published in the American Chemical Society journal of Sustainable Chemistry and Engineering, the solar-to-fuel efficiency of an artificial photosynthetic (AP) system was evaluated. The study looked directly at how the AP system can capture carbon dioxide directly from the atmosphere and can then convert it into fuel using sunlight. The report concludes that a modern, fully integrated AP system could reduce the carbon dioxide levels in the surrounding air by 10% during steady-state operation, which makes it fourteen times more efficient than natural leaves.

This report was analyzing a study conducted by the Department of Chemical Engineering from the University of Illinois, who had successfully produced fuels using water, visible light, and carbon dioxide using artificial photosynthesis. Scientists conducting the study developed an artificial photosynthesis process that used the same visible green light portion of the electromagnetic spectrum used by plants during natural photosynthesis to convert carbon dioxide and water into fuel. The process used a plasmonic excitation of gold nanoparticles (used as a catalyst) to produce a rich, electrically-charged environment at the gold particle-carbon dioxide solution interface which is conducive to carbon dioxide activation. From there, an ionic liquid stabilizes the charged intermediates that formed on the surface of the interface which begins to facilitate a multi-step chemical reduction reaction coupled with carbon coupling. Multi-carbon chains can be produced, resulting in the formation of methane, ethylene, acetylene, propane, and propene.

The University of Illinois chemists who conducted the study wanted to focus on creating liquid fuels because they are easier, safer, and more economical to store and transport than regular gasoline. Furthermore, liquid fuels are comprised of long-chain molecules containing more bonds than regular gasoline, giving them a higher energy density. While the goal of the study was to learn how to use catalysts to increase the efficiency of the chemical reactions undergone during the carbon dioxide-to-fuel conversion experiment, the chemists of the study agree that the hard work will begin when they start determining how they can scale up the process."
"Essential Roles and Responsibilities of Successful Business Leaders

Innovative Leadership is my research interest. I studied prominent traits that attract us to exceptional leaders. I have been researching leadership in various settings for several decades.

In one of the leadership and business articles, I introduced “Remarkable Leadership Traits for Technology Executives”. My research revealed the difference between ordinary and distinguished leaders.

In this post, I want to share 11 essential roles and responsibilities of exceptional business leaders.

Learning these roles can provide useful insights to aspiring business leaders and entrepreneurs.

You can find more of these stories on my News Break profile."
"Andra Keay is the Managing Director of Silicon Valley Robotics — a not-for-profit coalition of robotics companies and startups supporting the innovation and commercialisation of robotics technologies. She advises a number of startups and is the founder of Robot Launch Pad — organising events bridging startup and robotics communities.

We caught up with Andra to chat about the future of robotics and how entrepreneurs in the field can ensure innovation remains beneficial to human beings.

How did you get into robotics?

“I’ve been interested in all the geeky things since I was a child. Both my parents encouraged that a lot; their roles at universities meant that I got to play with computers and rockets and build equipment at a really early age.

“But one of the things that happened all the time, was that people would express their astonishment that a girl was interested in things like rockets and robots. And after a while, that astonishment became more interesting than the things that I was doing.

Andra and her brother as children

“I studied theories of communication and what I would like to call human-robot culture, as opposed to human-robot interaction. Where human-robot interaction focuses on the psychological side of things, and it’s certainly a fascinating field, I realised that the one-to-one relationships are nowhere near as indicative of what’s happening in society as understanding the broader social shifts.

“It’s not really a discipline as it were, but we have to ask ourselves (and this is the question that fascinates me): what is the 21st century going to look like as the most cutting edge technologies trickle their way through to mainstream adoption?

“We’ve seen what the adoption of technology looked like in the 20th century and that’s not simply the internet or even computing; it’s technologies like the automobile and household appliances.

“If we look to history, we can see a lot of great examples of what happens when a really disruptive technology is adopted. It’s not an immediate thing — as Roy Amara said, we often vastly overestimate the impact of technology in the short term but vastly underestimate the impact in the long term.

“We fill the headlines with fear about robots and AI in the short term, but it’s probably longer term structural shifts in society that are really where both the potential for problems and the potential for great advances are.

“One thing that’s clear, is that to solve a lot of the world’s serious problems (the ageing population, the increasing population and the fact that we need to double food production without having the ability to increase the amount of land that we use), we are probably dependent on technology because we haven’t been able to fix those problems yet.

“We’ve really hit a saturation point. The ability to increase our productivity without using more manual labour; we’ve hit a saturation point there. We’re dependent on technologies now to increase productivity. We need to be in the upward spiral or we’ll be in a downward spiral; there isn’t really a static place where we can just exist comfortably.

“One of the grandfathers of AI, Norbert Wiener, said that the future is not a comfortable hammock for us to lie back in and be waited upon by robot slaves; it’s actually an ever more challenging fight against the limitations of human intelligence. That’s really at the heart of my interest in robotics and AI. These are technologies that humans have developed and we developed them with our own strengths and weaknesses wrapped up in them.”

You’ve spoken before about this tendency of technology to unconsciously reinforce stereotypes. How do we prevent this?

“Well perhaps one of the first steps is awareness. That’s often an empty goal, because awareness without action is nothing.

“I’ve been involved in the heart of the technologist scene, as it were, for a long time and I’ve seen that the majority of the people involved don’t believe that they are building technologies that are biased. We really need to lift the debate so that we can talk about the various ways in which unconscious biases, and indeed conscious biases, are replicated.

“If anything, I think the current debate about Facebook and Twitter perpetuating hate speech is one of the strongest messages that we can get. It’s starting to shift a bit of the tone, from the Silicon Valley attitude of ‘we just build it, and it’s not our fault’ to thinking, actually, what we’ve built is something that we need to be responsible for.

“The problem with trying to change technologies that are widespread in the world is very clearly laid out. If it is a public company, then you have a legal obligation to report to the stockholders who have a very short term interest based on financial returns on a quarterly basis. If you said ‘let’s reduce our advertising revenue so we can reshape who we answer to’, that’s not going to fly. Whereas companies that are not yet public, or companies potentially like Facebook or Google who have majority control by the founders, maybe they can still go against that shortsighted direction.

“These are not so much problems with technologies but problems with the financial structures that support them. And that’s a lot to change. I do think that maybe it’s a problem of scale — we can create really good technologies that are fantastic on a small scale but that become inherently problematic once they become large scale. That said, one of the one of the things I believe is that if we can invent the problem, we can invent the solution.”

What are your biggest fears about the future of robotics?

“I think that we won’t end up with the really dysfunctional scenarios that people like to get scared about, like AI taking over the world or robots hurting us. Those things won’t happen, but I think the stupid short term problems definitely will. We’ve seen it happen where AI creates feedback loops that work on stupid things.

“We’ve seen financial trading for example, that causes billion dollar losses rather than smaller, individual losses. The feedback loop gets exaggerated and the trades happen faster with automation and we end up losing billions of dollars. We’ll see more of that kind of augmentation of human stupidity because AI and robotics allow us to augment human ability, and human ability can be stupid.

“We can also augment human bias. It’s in the field of gender that I see the most obvious problems happening. For example, an artificial intelligence like Microsoft Tay can get trained to hate speech.

“We’ve seen in video games where the representation of women is highly objectified, sexualised and stereotyped. Well I think with robots, because we won’t want to see such sexualisation, it’s going to be a little more subtle, but we’ve already seen that the female-looking robots and female-voiced robots are being used for the most menial of tasks. They’re very supportive, and it’s very difficult at a time of #MeToo, at a time where women are standing up and saying ‘I’ve just discovered that I’m not being paid as much as my males peers and I’m supposed to be at the top of my field so I’m going to fight’ and yet we’re building technologies to pass on the message that the woman’s role is not to be assertive, but to be compliant.”

Can you tell us a bit more about this notion of augmented stupidity and bias?

“One example is of AI being used to ascertain benefit status for health benefits. There is potential during construction of any system for conflict of goals i.e. best benefits vs lowest costs. The hardest thing about augmented stupidity is to find out who is at fault.

“We also have a problem with transparency in AI and in automation generally. Developers blame designers and designers blame commissioners, who blame the rollout and then everybody blames the person on the other end and says ‘well, they probably weren’t eligible for health care’. But its been found that people were being denied health care that they were legally clearly eligible for by systems that were skipping a few steps. Nobody took responsibility for checking this or working out what steps were being skipped.

“This is one of the most obvious examples of the power imbalance perpetuating augmented stupidity. The onus is put on the individual to complain and say they don’t think this result is correct, and no one is auditing it.

“To me, the two underlying problems with augmented stupidity — and the fact that it is, undoubtedly, rolling out everywhere — are that we are not putting effort into auditing and we’re not building and using transparent modules.

“Much of our AI and robotics is being done using off the shelf components. We’re using computer vision libraries that are not developed in house, we’re using software bridges that are maybe open source and maybe commercial but they’re not developed in house, we’re using navigation packages, mobile bases, sensors. There are a lot of things going into these systems and lot of trust that the system is going to do what we think it’s going to do.”

What are you most excited for in the future of robotics?

“Well founders, on the whole, are not really engaged with the idea of building something that is going to be auditable, accountable and make the world a better place. But right now there is a certain sort of founder who does congregate around the newest technologies of the world and is driven by the idea of doing things to make the world a better place and who will persevere in spite of of lack of funding and in spite of technical obstacles. I am privileged to be surrounded by people like that.

“That makes me very optimistic and hopeful about the future. And then I look a few steps down the pathway and I think our follow up is to talk about developing good design guidelines and helping governments talk about developing requirements for accountability.

“We are starting to be accountable for the diversity of the workforce. I believe having a diverse workforce would inherently shift some of these problems. First, you’re developing the accountability framework and second, we don’t know if we’re maxing out all of the potential solutions that are available with the homogenous core group that we have in technology right now. If we have have a greater diversity in that group, we might have many more solutions on the table.”

What companies are you excited about at the moment?

“Willow Garage was an amazing open source research and development group in Silicon Valley. It was founded by a wealthy individual who was involved with Google and sold his own companies as well. He wanted to build the first personal household robot, and it turns out, of course, that’s a really big problem to solve! So instead, he developed a framework — the first robotics platforms that could be used by multiple Universities because, at that point, robotics research did not have any standardisation.

“At Willow Garage they created the open source Robot Operating System which is now used by pretty much 100% of robot startups. They also developed the the best experimental robot, the PR2, and also the lowest cost open source robotics hardware platform, Turtlebot.

“But the key thing is that Willow Garage was a magnet for all of the talented robotics people in the world. It was fascinating because it was not a research centre, as such. While it was doing research and open publishing it, the goal was to develop things that were for commercialisation. While Willow Garage itself folded 2 or 3 years ago, it spun off 8 or 9 companies, and beyond that, almost all of the exciting robotics companies being developed in the Valley have people that worked at Willow Garage at the heart of them.

“Willow Garage has been a bit like Fairchild. It’s well known how 80% of the technology companies in Silicon Valley are Fairchildren, and that includes the venture capital firms as well. Willow Garage has had a similar impact, and if I look for an exciting robotics company, it’s almost 100% likely that someone from Willow Garage will be involved.

“One company I’m excited about is Mayfield Robotics who developed Kuri which is just being delivering to houses this year, having launched at CES in 2017. I have one in my living room right now! There’s a really fast turnaround these days for robotics.

“You also have Savioke that was founded by Steve Cousins and Fetch Robotics by Melonee Wise. Then there’s Kinema Systems, and a number of companies that Google purchased. I could go on.

“There are people who may not be the founders but are a part of the core team who worked at Willow Garage, and for me, this is a really good indicator that they come from the coolest group of roboticists out there — roboticists that have a sense of purpose as well, to make the world a better place.”

How will robots integrate with our day to day lives over the next 5–10 years?

“I believe that robots train us just as much as we train robots, we work together, it’s a very collaborative process. Anybody who’s used voice commands or touch commands in their car, or on their tablets and phones will recognise that.

“If we go into just about every bathroom (and I’ve been travelling a lot, so I’ve recognised that this is definitely worldwide), we expect to wave our hands to make things happen. I think it’s fascinating that the bathroom is the space in which this is happening. We don’t really talk about it much, but we’re training ourselves in haptic interfaces.

“I think over the next 10 years, people are going to use haptic, gesture and voice commands and expect things to happen. For me, the question is, are we going to get that right or are we going to realise that there has to be a better way to control our devices and start to develop a new control technology?

“For example, if we look down the track we might see something like an embedded or semi-embedded personal mic where we can tongue click our way through a menu. Or something that responds to our eyes, like AR or smart glasses or potentially in-home cameras that detect eye control with devices. Or perhaps we end up with a combination of the three things where you use a voice trigger or something to open a menu and then make selections from it by pointing and eye tracking where you point — the permutations go on!

“I’m going to be really interested to see what interface technologies we develop over the next 10 years. Everything in our house can be part of a robot house, and I expect that to include at least one mobile device, call it our home robot. I’m hoping that we actually have a couple of robot arms in household use by then. I have seen people working on robots to do things like dishwashing and toilet cleaning and cooking and folding clothes. But it’s always an open argument; is it better for us to build an appliance that is a one stop shop for cleaning and folding and hanging our clothes, or should we continue to have the appliances we have and have some form of robot arm that takes things out and folds them or puts them away? What’s the most effective? We just don’t know.”

We’d like to say a huge thank you to Andra for speaking to us. Check out svrobo.org for all things robotics in the Valley and follow Andra on Twitter @robotlaunch.

If you’d like to collaborate on your next tech project, get in touch with Simpleweb today."
"Infection | Brain

Recovered Covid-19 Cases Still Show Brain Anomalies 3 Months Later

First data that neurological signs and brain structural changes persist in Covid-19. But can the brain self-renew with time?

Image by rawpixel.com

Science knows that the novel coronavirus, SARS-CoV-2, can replicate in human neurons and brain organoids. Brain damage and neurological symptoms are also not rare in Covid-19 cases. Whether such brain insults are long-lasting, however, remains uncertain. And a new study published August in The Lancet, “Cerebral Micro-Structural Changes in COVID-19 Patients — An MRI-based 3-month Follow-up Study,” that casts light on this topic.

Persistent Neurological Symptoms

In this study, researchers at the Huashan Hospital in China recruited 60 Covid-19 patients that had recovered for over three months (mean age: 44.10; 56.7% males) and 39 non-Covid-19 controls (mean age: 45.88; 56.4% males). Both groups of participants were matched in terms of age, sex, and the prevalence of smoking, alcohol consumption, and underlying comorbidities.

During Covid-19, over two-thirds of these 60 recovered Covid-19 patients had neurological symptoms. Other common Covid-19 signs were fever (88.33%), cough (56.67%), and gastrointestinal distress (13.33%). Treatments used were antivirals (96.67%), oxygen therapy (61.67%), antibiotics (35%), and interferons (15%). And 78.33% of them had mild, 20% had severe, and 1.67% had critical Covid-19.

Even mild and not-so-old cases of Covid-19 may suffer compromised brain functions that are long-lasting.

Three months later, over half (55%) of these patients still had neurological symptoms. Only the frequency of mood alterations and fatigue decreased significantly (blue asterisks) from 41.67% to 16.67% and 26.67% to 6.67%, respectively. Other neurological symptoms (see figure; such as headache, impaired mobility, numbness, myalgia, memory loss, etc.) persisted for over three months.

And the fact that 78.33% of these patients initially had mild Covid-19 is also concerning as it means that persistent neurological symptoms can affect mild Covid-19 cases as well.

Blue asterisks (p <0.05) indicate statistically significant changes that are due to chance. No blue asterisks (p > 0.05) mean changes are not statistically significant. Source: Lu et al. (2020). Cerebral Micro-Structural Changes in COVID-19 Patients — An MRI-based 3-month Follow-up Study. The Lancet.

Persistent Brain Structural Changes

Next, the researchers used two brain imaging techniques: Magnetic resonance imaging (MRI) that captures cortical brain areas as a whole and diffusion tensor imaging (DTI) that detects microstructural brain changes. Results revealed that at three-month follow-up:

Covid-19 patients had higher overall brain diffusivity and white matter volume — which further correlated negatively with memory loss — than non-Covid-19 controls.

Covid-19 patients had greater grey matter volume (GMV) in the right cingulate, left Rolandic operculum, left Heschl’s gyrus, hippocampus, and olfactory cortices — which further correlated negatively with memory and smell loss, fatigue, and numbness — compared to non-Covid-19 controls.

Among the structural brain changes, only cingulate gyrus correlated positively with Covid-19 severity during hospitalization. The cingulate gyrus is involved in attention, motivation, decision making, and learning. Put it another way; it also indicates that even mild Covid-19 cases may face persistent brain structural changes.

Notably, all brain regions are examined except the brainstem due to technical limitations, which is unfortunate given that the brain’s cardiorespiratory centre resides in the brainstem. Other study limitations, the authors admitted, were the small sample size and participants recruitment from one hospital only.

On the other side, the human brain displays a remarkable capacity to self-renew with time, such as following stroke or traumatic brain injury.

Notwithstanding these caveats, this study showed that recovered Covid-19 patients had increased brain diffusivity and enlarged white and grey matter at three-month follow-up. These indicate the brain is undergoing heightened metabolic activities. Why? The academics posited that these are ‘functional compensations’ where the brain is trying to repair itself from the insults Covid-19 inflicted.

“In this prospective study, volumetric and micro-structural abnormalities were detected mainly in the central olfactory cortices, partial white matter in the right hemisphere from recovered Covid-19 patients, providing new evidence to the neurological damage of SARS-CoV-2,” the study authors concluded. “The abnormalities in these brain areas might cause long-term burden to Covid-19 patients after recovery, which was thus worth public attention.”

Can the Brain Self-renew?

To sum up, this August study in The Lancet showed, for the first time, persistent neurological symptoms and structural brain alterations in Covid-19 cases. Also, note that the majority of participants in this study were middle-aged (mean age of 44) and had mild Covid-19. It seems terrible news; even mild and not-so-old cases of Covid-19 may suffer long-lasting poor brain functions.

On the other side, the human brain displays a remarkable capacity to self-renew with time, such as following stroke or traumatic brain injury. “Although the central nervous system (CNS) has been considered for years as a “perennial” tissue,” explained neuroscientists at the University of Cambridge. “It has recently become clear that both physiological and reparative regeneration occur also within the CNS to sustain tissue homeostasis and repair.” Brain renewal is driven by neural stem cells in the hippocampus and subventricular zone that remain active for life, which catalyzes neurogenesis and neural outgrowth to other brain areas.

Even the August study proposed ‘functional compensation’ at play in the brains of the recovered Covid-19 patients. Brain scans revealed increased metabolic activities and enlarged structures in those patients. Plus, those Covid-19 patients also showed less neurological symptoms at three-month follow-up. Perhaps, more time is simply needed for Covid-19 brain complications to pass."
"Technology is all around us. It is at once so ubiquitous as to be almost invisible, and in other areas so remarkable as to grab the world’s attention.

From ceramic cups to nuclear weapons; from the springs in your mattress to immersive virtual worlds; from paper and pencils to intelligent algorithms—technology runs the gamut from background noise to something like magic.

But even the background technologies were once like magic. Cups and springs might not make for much conversation today, but life would be profoundly different were they were never invented. Perhaps AI, virtual reality, and other developing technologies will follow a similar path.

One philosophical conundrum at the heart of these developments is the question of moral value: Can we say that a technology is good or bad, or are they neutral? Are our values embedded in the technologies we build or are they valueless until someone decides what to do with them?

It might seem trivial, but the answer to the question will impact how we regulate technology and who is responsible for the consequences of how it’s used. As we blaze forward into the realms of intelligent computers, brain interfaces, and biohacking, the significance of these moral questions grows.

Let’s take a look at the arguments.

Guns Don’t Kill People, People Kill People

A significant argument in favour of technologies being neutral is the Value Neutrality Thesis—no moral valence can be ascribed to a technology, it is only when someone uses that technology that any value can be found.

Here are the main points:

1. Values Are Hard to Detect

Joseph Pitt, in his article “Guns Don’t Kill, People Kill,” argues that for technologies to contain or embody values, those values should be identifiable, but this is rarely the case.

He uses the example of a university football stadium. It might be a source of prestige or pride, a symbol of everything good about the university, it might represent the dreams and aspirations of the students. But all of those values are values of the people, not the stadium itself.

“They may see the stadium as symbolizing their own values, but that doesn’t mean the values are in the stadium.”

Consider how an alien would look at a knife they found floating in space—would they identify the same values as us? Would they find any values at all? How should they even begin to look for them?

How about a member of a small tribe in the Amazon jungle who comes across an iPad—what would they see in it? Would they ascribe the same value to it as a programmer in silicon valley?

2. Different People, Different Values

Tools only have value when they are possessed by a creature with values, and so are dependant on the value system of that individual. In this system, if the person or context changes, the values also change.

If a student who plays football at the stadium suffers a devastating leg injury which ruins their future prospects, the value they ascribe to the stadium could take a dark turn, despite no physical alteration to the stadium or any change in the values of other students.

When a new technology is introduced to a population, though it might have been developed for a specific purpose, its value is dependant on the function and purpose that each individual discovers.

If the value of our tools is dependant on the system they’re in and the minds which interpret them, how can we say the tools have any values embedded in them? Shouldn’t those values remain consistent?

3. Value Depends on Use

It is the outcomes, consequences, and results of our actions that are open to valuation, not the tools which we might rely on.

A knife is just a knife, a neutral object, it is not until someone uses it to peel a fruit or stab someone in the back that any moral value can be applied.

This argument focuses on the perspective of the end-users. Their desires, needs, and goals determine how the tech is used, which determines its value.

This perspective also suggests the end-users are responsible for the ethical use of technology. We can’t blame guns for shooting people, even if they make it easier. Guns are neutral, people aren’t.

To summarise:

Technology is a tool — we use tools, tools don’t use us. We ascribe our own meaning to technologies, irrespective of the reasons for their existence. We are in control, we are free to use our tools how we want to, it is our choice. And for those who make poor choices, they are responsible for their actions.

Now the counter-argument:

When All You Have Is A Hammer, Everything Looks like A Nail

“I call it the law of the instrument, and it may be formulated as follows: Give a small boy a hammer, and he will find that everything he encounters needs pounding.” — Abraham Kaplan

Arguing against the value neutrality thesis are those who believe that our values and assumptions are baked into everything we design and build.

Rather than focusing on the freedom people have in how they interpret and use the tools, this perspective looks more closely at the designers and the designs, how the features of those designs influence people, and why our ability to identify the values isn’t necessary.

Here are the arguments:

1. Technology is Intentional

Unlike the somewhat haphazard selection processes seen in evolution, technology is willingly and consciously fashioned. It is conceptualised and considered before ever becoming a thing. Each new creation is built to satisfy a need, fulfil a purpose, to be useful.

It is our values which determine the technology — and the values the designers expect are held by their future users. While the users’ actual values will determine how the tech is used, the fact the technology only exists because of our values makes them inseparable.

2. Decisions Reveal Values

Every decision and selection process will reveal our values. Whenever we pick a default option or display some information over other information, we have made a value judgement. It is impossible to display everything equally, therefore there is always bias (what does this say about how I ordered this article?).

Making the buy now button more predominant, writing the terms and conditions in obtuse language, using red over blue in your brand identity, are all decisions that reflect values. But just because they reflect values, doesn’t mean people will identify or share those values.

3. Not All Values Are Seen

Sometimes the values are very explicit — such as labelling a product as “environmentally friendly” or “parental advisory recommended.” Whether you use the passcode feature on your phone, it is clearly designed for security. Why your remote control has an EPG button might be lost on you, but there’s a reason it’s there, and the reason reflects the values of the designers (or what the designer expects are the values of the users).

Whether people can identify the values or not, doesn’t mean the values aren’t there. Consider for a moment the reverse of an alien finding a knife out in space — imagine that we found an alien device floating in space, something that was clearly designed and not random space debris resulting from natural processes. Beyond wanting to know who built it, surely we would be very interested in what it is and what it’s for.

While it will be an incredibly difficult task to find the purpose given we can’t comprehend the mind that designed them, the technology must have been designed for a reason, and that reason would suggest values.

4. Limited Range of Use

Technology is directional — it adds choices or improves processes which point in a certain direction.

There might be many different uses for guns — we could use them as paperweights or doorstops — but most of us know this is not the reason we have them. Guns were developed for a specific purpose, and we generally use them in accordance with that.

The range of possible uses is not infinite — we can’t use one to drive down the road or watch a movie. Other uses are possible though not ideal — perhaps you can cut an onion but a knife would be better.

The limited range of ideal uses suggests where the inventor’s values lie — a gun is great at killing or putting holes in things. If you need to puncture something from a distance, you pick the gun before you pick the doorstop — unless your doorstop is a gun.

5. Eventually Frames Reality

“We become what we behold. We shape our tools, and thereafter our tools shape us.” — Marshall McLuhan

The longer a technology has been around, and the more widespread it is, the less it will be thought about and the more likely it will blend into the background, to become the norm we refer to when we talk about how things are.

When we get too used to how things are, it takes greater effort to see how things could be different. When we get too used to what something does, it takes a more creative mind to see it in any other way.

Here are two lines of research relevant to this argument:

Functional fixedness, which highlights the struggle of finding uses for objects outside of the way they’re traditionally used. And the Einstellung effect, which describes the process of learning how to solve a problem using one method, but then failing to realise when a better method is available.

In each case, we get stuck in a particular thinking pattern or frame of mind. We go through the effort of learning, and then what we learned becomes automatic and rigid. This is not to say we cannot think creatively and break free from those patterns, but as tools and their uses become more common and familiar, it gets more difficult to see them any differently.

To summarise:

Technologies are developed by people for people. Our values determine what is created and how it is used, and its use is at least influenced by its design, if not fully dependant on it. While people can use tech creatively beyond its original purpose, there is a narrow band of possible functions which suggests the original purpose and therefore the values. As time passes and familiarity grows, the technology and it’s function become so ingrained to be barely thought about, let alone questioned.

The Burden of Responsibility

We are not helpless slaves to technology, we are decision-makers and we value our freedom of choice. However, people often make poor decisions while being unaware of what factors influence them.

While each individual can and does decide how they want to use certain technologies, on a collective level technology nudges us in directions that are not value-neutral.

If we call technologies neutral, we are relinquishing the creators of any blame in how those technologies alter the world. But then we need to ask if any company or individual should be praised for their inventions. How could designers be worthy of praise but not of guilt? And if we are to say that they are free of both, it is worth asking what role they have.

In 1986, Robert J. Welchel wrote in IEEE Technology and Society Magazine:

“This moral neutrality is based upon viewing technology purely as a means (providing tools for society to use) with the ends (the actual usage of technology) lying beyond and outside the realm of engineering; this position also assumes that available means have no causal influence on the ends chosen. If technology truly is only a means, then engineering is a second-class profession since we are the mere pawns of the real power brokers. We buy our innocence at a tremendous cost: To be innocent, we must be powerless.”

Deciding that creators are culpable and that technologies are embedded with values doesn’t make it any easier to figure out what technologies embody good values. For that we have to collectively agree on what values are good and set a standard for what we consider a violation of those values, then we can decide how to respond to those violations.

A significant roadblock here is our inability to predict the future. If no designer, inventor, or company is capable of predicting all of the future benefits and costs of what they build, how can they possibly ensure they embed good values?

But this is an old philosophical problem — if we can never predict all the consequences of our actions, how can we tell the good from the bad in any domain? This problem does not stop us from making ethical choices in other areas of life, why should it stop us here?

We must find the best explanations or predictions we can, using the information we have. If we pay attention to how different technologies progress, and particularly to their consequences, we can learn from our mistakes and make better decisions.

Our current way of life is so closely intertwined with technology they could be considered one and the same. It seems absurd to think we can go backwards and untangle much of it, but we can be more careful in how we weave future technologies into our lives.

This is important, as future technologies are likely much more powerful and consequential than today’s. When intelligent machines make their own ethical choices, it will make no sense to say that technology is neutral, and it will make it tremendously important to align our values."
"To say things are evolving fast would probably be a cliché. But that is indeed the reality of the situation: The marketing industry must continuously reinvent itself as new technologies, trends, and consumption habits emerge. During the last edition of C2 Montreal, three predominant trends stood out as the ones businesses should start integrating right away.

Trend 1: Storytelling is good. Hypertelling is better.

Storytelling is the art of telling a story in a manner that captures the attention of the receiver, whether the message is coming from a person or a brand.

Hypertelling — as defined by Mike Yapp, founder and director of The Zoo at Google — is an immersive, non-linear experience where users create and determine their story. Transmitters are no longer in total control of the message; they simply supply the context and parameters with which the experience is built. It’s through this experience that brands find their residual benefit.

This approach can dizzy brands that are historically used to controlling the minutest details of their messages (just think of the number of approval levels a national or global campaign must overcome).

For Yapp, the solution is to redefine the role of the brand: “We can no longer see ourselves as designers or copywriters. We have to consider ourselves inventors.” New technologies, including virtual reality, are opening the door to new common ground where various parts of the equation find themselves.

Moving from theory to practice remains a challenge, but certain brands — such as Lockheed Martin — have already put their shoulders to the wheel to create contexts where users can share a virtual experience in person with many other users at once. This video will tell you more:

Trend 2: Environmental thinking, version 2.0

Having a green heart is a good thing. As we all witnessed at C2 Montreal this year, the idea of eco-responsibility has taken on a whole new meaning for businesses.

According to David Suzuki — host of The Nature of Things and a scientific icon incarnating environmental consciousness in Canada — climate change represents a challenge filled with potential for companies who’ll know how to take them on. In his words: “It’s an exciting opportunity to get this bloody economy right.”

Although environmental speeches have often been the prerogative of corporate social consciousness in the business world, Suzuki believes climate change is fertile ground to stimulate commercial creativity.

“Americans said at the beginning of the ’60s that they would walk on the moon within 10 years. They had no idea how they would get there, but they did. On the way there, hundreds of technological innovations saw the light of day and are still used today.”

This is where the opportunity lies: For brands who choose to take on this type of challenge — as colossal as it may be — the outcome will offer commercialization and brand equity potential that’ll be well worth the investment.

Wanna know how much time your company has left to get involved?

David Usher — lead singer of the band Moist and noted environmentalist — has developed a simple tool with a powerful message. Countdown2degrees.com outlines the amount of time left before humanity causes irreversible damage to the environment. We’re flirting with the point of no return, but it’s not too late. So what are you waiting for?

Trend 3: Doing more with what we already have

In marketing terms, we would probably talk about “resource optimization”. In the words of Massimo Bottura — chef and owner at Osteria Francescana, named best restaurant in the world in 2016: “We need to make the most of what we already have”.

The Italian chef is the mastermind behind the Milan-based Food for Soul project, where meals are created using leftovers. On top of the obvious community benefits of his initiative, his approach has had a clear commercial and environmental impact.

Whether we’re talking about food or human capital, the message is the same: These resources hold enormous potential we’re all missing out on. For a brand, that can mean millions of dollars left on the table.

“When we already have” also refers to the local roots of his company. As a business, we sometimes want to do everything ourselves or rush to the other end of the world to find something we consider of the utmost importance. For Mark Brand — social entrepreneur and cofounder of Save on Meats: “There is more genius in the neighbourhood than what you could possibly bring.”

Making the most of immediate connections and community relationships is therefore another way to “optimize resources”.

Recap

Historically confined to the role of broadcasting unidirectional messages, brands and their raison d’être have greatly evolved when observed from a marcom angle. Consumers now expect companies to provide experiences, rather than situations where they’ll simply witness something produced without their input: “We don’t want to know it, we want to feel it.”

We also expect brands to be much more than good corporate citizens. We expect them to be agents of transformation, actively involved in improving their community.

To find out more about the great ideas exposed during C2 Montreal, download your free copy of The Minutes — a collection of the most transformative quotes and ideas discussed during the event. While you’re at it, you can even reserve your tickets for the 2017 edition. And of course, I’d also be more than happy to answer any of your questions."
"Teams don’t struggle with solving problems but figuring out what the problems are.

Table of Contents

Summary

Though many factors contribute to a project’s failure, nothing is more certain to cause a project to fail than solving the wrong problem or realizing too late that the problem was misunderstood.

A common misconception is that identifying the right problem is an innate ability that only a select few visionaries like Henry Ford or Steve Jobs possess. Their quotes,

“If I asked people what they wanted, they would say “a faster horse”,

and,

“Its really hard to design products by focus groups, as people don’t know what they want until you show it to them”,

have propagated this myth.

Identifying, analyzing and defining problems is a process and its accessible to anyone that is passionate about doing so.

The rigor with which a problem is defined is the most important factor in finding a suitable solution.

Founders and Organizations investing in projects don’t spend enough time defining the most important problem they’re attempting to solve. Many have considerable difficulty even identifying which problems are crucial to their missions and strategies. This results in too many pivots, shifting business requirements, missing deadlines and re-work as teams go back to the drawing board. This eventually results in schedule and budget overruns or project cancelation.

Teams speed toward a solution, fearing that if they spend too much time defining the problem, they will lose the window of opportunity or get scolded by corporate leadership for taking too long to leave the starting line.

Many times, projects go down the wrong path and end up implementing the wrong system. They discover too late, that they didn’t solve the “right problem”, or they didn’t solve the “whole problem”. They only realize in hindsight the right problem to focus on and which path to follow.

Projects also fail when they start with the solution first. Often referred to as “Solutions in search for a problem”. This happens frequently when new technologies such as Artificial Intelligence and Blockchain come on the scene and teams are adamantly attached to building with it. Teams often end up shoe-horning problems instead of objectively evaluating the situation and using the right solution to solve it.

Most teams are not proficient at articulating their problems clearly and concisely. They need to ask the right questions earlier and not start building solutions too soon.

Teams need to get a lot better at:

Identifying the right problem to solve

Being sure the problem is worth solving

Understanding who is affected and how

Understanding the root causes of the problem and breaking down large problems into many smaller ones

Asking why it hasn’t yet been solved and understanding the most difficult parts of the problem

Defining the problem clearly and concisely

Validating the economics — the costs, benefits and payback

Asking how we will know when the problem is solved and whose opinions count

Identify the right problem to solve

Where do the ideas for the right problems (or opportunities) come from?

Life is problematic and solving challenges is an integral part of our everyday lives. So ideas for problems are everywhere.

Whilst there is no shortage of problems to find, there are limits on what we as individuals or corporations can take on. Therefore, the process starts with asking the right question — “What does the world want changed, that I am, or we are, passionate about solving and are uniquely qualified to tackle?”

Simon Sinek puts this as “Do what inspires you”. Simon explains that to do what inspires, start with the “Why”. Start by understanding and explaining why the problem matters to you. If you can answer that question, you will not only inspire more people to use the solution, but you will also inspire yourself to get out of bed each morning and push through difficult tasks.

Simon goes onto explain that searching for why the problem matters, requires you to examine the significant moments from your past to understand your why. This process also applies to corporations, who must look to their history so that they can understand why its important to their mission.

Another less used, but powerful technique to gaining clarity on what matters to you is to write your own obituary or for a corporation, a bankruptcy filing. Although it sounds a bit macabre, writing your own ending, can be an excellent way to gain clarity on how you want to use your time.

Be sure the problem is worth solving

Not all problems can be solved viably. Some problems are not economical to solve because:

The solution would cost more to build or sell than its benefits

The solution would take too long to pay back the investors or the corporation.

Some problems are not economical to solve right now, because the conditions for success are not yet ripe.

But how can we predict if a problem is worth solving and the timing is right?

Significant contributions on answering this question come from the Tech Startup Community — these insights also apply to problems faced by corporations.

Problems are worth solving when they:

Matter deeply to the founders (if it’s a Startup), or to a corporation’s mission

Are useful to enough people

And are:

Growing or getting worse with time compared to other problems these people face

Urgent and needs to be solved quickly

Hard to solve and the people impacted don’t have good enough alternatives or the solution is not immediately obvious

Mandatory and not just a nice to have

Caused by changes in the environment

Frequent and affects people often

Understanding who is affected and how

To analyze if a problem would be useful to enough people, you need to:

Identify who and how many people might be impacted by this project the most or have influence over this project

who and how many people might be impacted by this project the most or have influence over this project Prioritize the people based on their interest and influence

the people based on their interest and influence Understand how they are impacted or how they will impact you

Enterprise IT / transformation projects often refer to this analysis as “Stakeholder Analysis” and charts such as the one below is created to visualize the Power-Interest of each stakeholder.

Source : https://www.mindtools.com/pages/article/newPPM_07.htm

This area is well covered by multiple sources. Some of the notable ones are listed below:

The Technology Startup community has a rich body of knowledge on finding potential users. Geoffrey Moore describes the various segments in his book “Crossing the Chasm”. The central idea of the book is that Startups should focus first finding innovators, then target early adopters and so fourth as the company matures.

Crossing the chasm, by Geoffrey More

Understand the root causes of the problem and break down large problems into many smaller ones

Problems often start out “fuzzy” — vague, formless thoughts. A problem is fuzzy if the founders and project teams struggle to explain the problem concisely and precisely. Its often said, that if you cannot explain the problem you are working on to your partner or parents, you haven’t understood the problem well enough yourself.

Root cause analysis is about digging beneath the surface understanding of a problem. The goal is look beyond trying to find one singular cause, and instead uncover the system, or network of causes. A Root Cause Network Map is a simple visual explanation of all the causes that contributed to the problem.

Source : https://www.lucidchart.com/blog/root-cause-analysis

5 Whys

This method encourages you to keep going deeper as you examine an issue. Ask “Why?” at least five times until you’ve uncovered all potential causal factors and determined the real reasons this problem occurred in the first place. 5 Fives is well established technique with detailed steps and tips.

Using a root cause analysis, teams can break fuzzy large problems into multiple well-defined smaller pones. In doing so they can also identify the sources of a problem. This level of understanding will later help to implement permanent and lasting solutions.

How to deal with complex root causes

The root cause network map is often a simplified view of the true nature of the issue. Most real-world issues have a more far more complex causal chain. For example, the diagram below depicts the behavioral and societal factors that contribute to the cause of obesity.

Analyzing the root cause when the causes are complex are covered by the complexity theory field of study. Teams facing complex causes should look to tools and techniques used by complexity analysts, such as graph analysis.

Source: Government Office for Science 2017

Ask why it hasn’t yet been solved and understand the most difficult parts of the problem

With a clearer view of the problem and the network of causes, teams can then ask these questions:

“Why hasn’t each of the smaller problems been solved?”

“Which of these smaller problems are the hardest to solve”?

Difficult problems remain unsolved, generally because somewhere in the network of causes is a “Mini-Wicked problem” that’s hard to solve, because it is:

Not economical to solve

The problem is highly interconnected with too many other problems

The technology doesn’t exist yet to effectively solve the problem

No obvious solution exists

If viable solutions cannot be found to these wicked problems, project failure is imminent.

Define the scope of the problem

Teams must solve a root cause completely and better than an alternative to be successful. However, teams often don’t have the resources to solve all the root causes of a problem. Therefore, teams must decide which of these ones they wish to undertake and include within the project.

The list of root causes that the team chooses to include within a project is referred to as the “Scope” of the project.

Various frameworks exist for defining the scope of the problem and teams must select one that works best for their situation.

The Concise Scope Definition (“The Elevator Pitch”)

This type of scope definition is a few sentences that describe:

Current State — who is facing the issue, what is today’s reality, when, how and why, do they face the issue today

The loss — the frequency and impact of that this issue is having on users today

Ideal End State — the users experience once the problem is solved

Example:

Managers are spending 20% of their time each week receiving and compiling sales reports for upper management, reducing the number of hours spent on mentoring sales staff, lead generation, and closing business. This is a productivity issue, and ignoring it results in decreased sales and missed revenue targets over the past 3 months. XYZ Company is committed to reducing the time spent compiling reports to no more than 10% of the sales manager’s time in any week.

Scope Modeling Frameworks

Some scope definitions need more information to set the stage and define where the problem exists and which problem is being tackled. In such situations the problem definition often requires several pages, several PowerPoint slides, various diagrams or even more formal models.

The scope definition must help explain the impacted

Business units

Business processes

Business capabilities

Data

Applications

Projects

Infrastructure

If the problem is a caused by a change in the external environment, the model may also need to explain:

The Influencers — trends, competitors or legislations that are causing this change

trends, competitors or legislations that are causing this change The SWOT — strengths, weaknesses, opportunities and threats to the business because of these external changes

— strengths, weaknesses, opportunities and threats to the business because of these external changes The drivers — the main reasons why this is a problem and why it’s a problem now

If the problem is a so severe that it requires the business to change direction or morph into a different business to survive, the problem definition may need to explain:

The business model — the most fundamental building blocks of the business that are impacted such as Customer segments, Value proposition, Revenue streams, Channels the business uses to interact with customers, Customer relationships, Key activities, Key resources, Key partners, Cost structure

Teams can refer to frameworks such as the business model canvas.

This framework is also useful to Startups as they are often invent new business models.

Validate the economics

Teams often asked the question “How much will this project cost?”. Teams should resist responding only with the cost “this project will cost $1 Million” as it offers no frame of reference to understand if this cost is too high or just right.

Teams need to get better at providing both the cost, the benefit and the return on investment — for example “This project will cost $1 Million, and will deliver $5 Million in Benefits over 3 years”.

Even at the early stage of the project, teams should strive to envision ballpark return on investment figures.

Creating business cases has been extensively written about. Some notable references include

How will we know when the problem is solved and whose opinions count

The big problem with assessing project success is that is no consistent way to define “project success.” There is a great deal of diversity in terms of what is considered as the project success criteria, and worse they change over time. Projects that struggle to define success or aim for moving targets, risk the odds that the project will be viewed as a failure in the end.

Getting project success criteria is sometimes so hard, that a popular practice has simply been to ignore it. Many project teams never ask the two fundamental questions at the beginning of their projects:

Who declares success?

What are the criteria that will be used to determine success or failure?

Project Management Institute has made significant contributions in this area and suggests that there are broadly two types of success criteria

Project management success — the team’s ability to meet the projects budget and schedule and deliver a product of acceptable quality

Product success — the outward view of success of such as revenue and customer adoption, customer satisfaction and so fourth.

Whether the project will be judged more so by project management success or product success will largely dependent on the power-influence mix of the project’s stakeholders and if they are more focused on project management success or product success or some combination of the two.

Conclusion

Projects succeed when teams solve the right problem and solve the whole problem. To do this, teams must resist the urge to speed toward a solution, and spend more time upfront, identifying and defining the problem.

To find the right problems, individuals and corporations must be selective in which problems they choose to take on. They should prioritize those that:

Could have a big impact on people

They are uniquely qualified to tackle

Can be solved viably — i.e. the solution has greater benefits than the cost to build and sell it

Have an acceptable pay back period for the investors or corporation.

Teams must identify the users that will be impacted or have an influence over the success of the projects and assess if there are enough people that will benefit from this project to make it worthwhile.

Root cause analysis is an essential step to help teams

Break down fuzzy large problems into multiple well-defined smaller ones

Identify and visualize the multiple causes of a problem.

Before embarking on solution design, teams should

Try to understand why the problem hasn’t yet been solved, and

Where in the network of causes is the hardest part of the problem to solve.

The list of root causes that the team chooses to include within a project is referred to as the “Scope” of the project. There are various frameworks for defining the scope of the problem and teams must select one that works best for their situation.

Teams need to get better at

Validating the project’s economics by understanding the cost, the benefits and the return on investment.

Uncovering at an early stage, who declares success and what criteria will be used to determine success.

Teams don’t struggle with solving problems but figuring out what the problems are. Identifying, analyzing and defining the problem is the key to project success. Einstein summarized it best by saying:

“If I had an hour to solve a problem, I’d spend 55 minutes thinking about the problem and five minutes thinking about solutions.”

Reference"
"NeurIPS, the largest conference in artificial intelligence, is currently underway, and it has over 20k people registered. If you are not registered and would like to access their goodies, please visit this public access version of the NeurIPS website.

If you are into deep learning, we recommend you to check out this phenomenal tutorial by David Duvenaud, Zico Kolter, and Matt Johnson, which makes use of many tools such as Anderson acceleration, differential equations, neural nets, convex optimization, Jax, automatic differentiation and others, presented on NeurIPS.

Next, we recommend you to check out this article titled “We read the paper that forced Timnit Gebru out of Google. Here’s what it says” by Karen Hao from MIT Technology Review, which gives a very insightful overview of what caused the departure of Timtit Gebru, co-lead ethical AI researcher from Google Brain.

For those interested in natural language processing, Carnegie Mellon Professor Graham Neubig just published 23 class-lectures on multilingual natural language processing, including two guest lectures by Pat Littell and Orhan Firat. The video playlist can be accessed for free on Youtube.

Last but not least, Paul Liang and Misha Khodak from ML@CMU published a post containing all of CMU’s submissions to NeurIPS 2020, with many goodies, from papers to code, and much more."
"Read the Email That Led to the Exit of Google A.I. Ethicist Timnit Gebru

‘Your life gets worse when you start advocating for underrepresented people’

Photo: Kimberly White/Stringer/Getty Images

Timnit Gebru, one of Google’s most prominent researchers on ethics and computer vision, says she was fired this week after sending an email to Google Brain Women and Allies, an internal resource group at the company.

The email alludes to Google censoring one of Gebru’s research papers without talking to her about it, as well as the poor treatment of those who advocate for underrepresented people at the company. The email was published in full on the outlet Platformer.

After sending the email, Gebru had an exchange with managers and privately threatened to quit unless certain undisclosed conditions were met. Instead, Gebru says she was immediately fired, she told OneZero’s Will Oremus.

Gebru’s contributions to the field have shaped modern understanding of how artificial intelligence fails and the technical underpinnings of how algorithms treat underrepresented people differently. A Twitter thread by Fast.ai co-founder Rachel Thomas lays out how Gebru’s years of scholarship have influenced A.I. research, including her co-authoring a seminal work that showed facial recognition is far less accurate on women of color than on white men.

Gebru helped lead of Google’s A.I. ethics team and co-founded Black in A.I., an international organization focused on supporting Black A.I. researchers and expanding access to the traditionally exclusive field.

According to the Platformer, the email reads, in part:

Imagine this: You’ve sent a paper for feedback to 30+ researchers, you’re awaiting feedback from PR & Policy who you gave a heads up before you even wrote the work saying “we’re thinking of doing this”, working on a revision plan figuring out how to address different feedback from people, haven’t heard from PR & Policy besides them asking you for updates (in 2 months). A week before you go out on vacation, you see a meeting pop up at 4:30pm PST on your calendar (this popped up at around 2pm). No one would tell you what the meeting was about in advance. Then in that meeting your manager’s manager tells you “it has been decided” that you need to retract this paper by next week, Nov. 27, the week when almost everyone would be out (and a date which has nothing to do with the conference process). You are not worth having any conversations about this, since you are not someone whose humanity (let alone expertise recognized by journalists, governments, scientists, civic organizations such as the electronic frontiers foundation etc) is acknowledged or valued in this company. Then, you ask for more information. What specific feedback exists? Who is it coming from? Why now? Why not before? Can you go back and forth with anyone? Can you understand what exactly is problematic and what can be changed? And you are told after a while, that your manager can read you a privileged and confidential document and you’re not supposed to even know who contributed to this document, who wrote this feedback, what process was followed or anything. You write a detailed document discussing whatever pieces of feedback you can find, asking for questions and clarifications, and it is completely ignored. And you’re met with, once again, an order to retract the paper with no engagement whatsoever. Then you try to engage in a conversation about how this is not acceptable and people start doing the opposite of any sort of self reflection — trying to find scapegoats to blame. Silencing marginalized voices like this is the opposite of the NAUWU principles which we discussed. And doing this in the context of “responsible AI” adds so much salt to the wounds.

Read the full letter on Platformer."
"When our species first arose on this planet, it was in the midst of a rather cold and indifferent natural environment.

Mother Nature didn’t necessarily care about whether our particular primate offshoot would survive long enough to develop civilization.

The dinosaurs that had dominated the planet for millions of years were wiped out in one fell swoop by a large rock that just happened to collide with Earth, and there are innumerable other chance events that could have led to our ancestors being eradicated before Homosapiens ever came to be.

It’s within this harsh context that all human striving and creativity should be understood — we had to innovate, and innovate rapidly, in order to free ourselves from being at the mercy of nature’s whims.

For all the problems modern society confronts us with, from the perspective of our ancestors of even just a few generations ago, we live like kings and queens.

We worry not about being eaten by a hungry lion, and we enjoy the comforts of plumbing, air conditioning, heating, and clean water (at least in the developed world) readily available on tap.

These things — and everything else we take for granted — are the results of human creativity and ingenuity.

Therefore, we tend to think of creativity as a force for good, a way to “improve” the quality of human life, and something that we should all work to develop.

Yet at the same time, it is this very same creative urge to separate ourselves from our environment — in effect creating our own — that has bred a whole host of new problems, ones that threaten us on a planetary scale.

Climate change is the result of having invented electricity, automobiles, aircraft, and other technologies that demand the burning of fossil fuels and oil.

Modern agricultural techniques have allowed the human population to explode way past historic levels, necessitating the need to take more and more habitat away from wildlife to make space for our farms and cities.

And perhaps most strangely, the mass production of “comfortable” lifestyles has left modern human beings feeling estranged from themselves and confused.

We carry genetic programming that helped us survive when we were still learning how to walk upright and speak our first words, yet which we find ourselves plagued by as modern culture continues to emerge.

Much of the technology we have produced (such as smartphones and social media) have unintended, negative consequences as the result of how little we still seem to understand about the genetic and psychological forces that drive us — forces that were forged in the treetops and plains of Africa hundreds of thousands of years ago.

This is the dark side of creativity — for every problem we have managed to solve, we have unknowingly created scores of new ones.

Even apparently beneficial inventions have unintended negative consequences. Television is a fantastic tool for increasing the range of what we can experience, but it can make us addicted to redundant information that appeals to the lowest common denominator of human interests. Every new meme — the car, the computer, the contraceptive pill, patriotism or multiculturalism — changes the way we think and act, and has a potentially dark side that often reveals itself only when it is too late, after we have resigned ourselves that the innovation is here to stay. — Mihalyi Csikszentmihalyi

One could be forgiven, then, for taking a cynical point-of-view, of saying, “Well, I guess that’s it then. We can’t improve the world, so why even try?”

And yet continuing our efforts to improve both ourselves as individuals and the world we live in is really all we can do — the alternative is to stagnate where we are and consume the last bounties of Mother Earth until she has nothing left to offer, at which point we secure our own extinction.

This is why it is essential — now more than ever — to understand how our best efforts to liberate ourselves from the natural environment that birthed us have gone a bit overboard, so that we may think of tomorrow’s solutions in terms of how to live in harmony with nature — including our own.

It is an awareness of any problem that is the first and most necessary ingredient to solving the problem — and thankfully, human beings are now experiencing an exploding epiphany regarding our interconnectivity with the natural world and our dependency on it, even if our initial desire to break free from it was justified.

We are learning that our desire to control nature — when taken too far — leads to a rebound that eventually harms more than helps us.

This is the basic law of human ingenuity that we try very hard to ignore: the greater the power to change the environment, the greater the chances of producing undesirable as well as desirable results. — Mihalyi Csikszentmihalyi

The temptation, then, becomes to reject progress entirely, to condemn the past few thousand years of human cultural development, and to yearn for some sort of return to an “archaic” period in which human beings lived in harmony with the environment.

But it is as unwise to condemn humanity’s striving for progress as it is to embrace it with reckless abandon.

We may have lived in harmony with the environment thousands of years ago, but it was a harmony that cared not about human fears and desires.

Though we have created seemingly unconquerable problems today, we have to find it in ourselves to forgive our ancestors for doing all they could to gain some degree of control over human destiny — and then we have to go further in pushing our creative ingenuity to devise solutions that take the planet into account as much as they do human needs and aspirations.

And why should it be that we cannot both look out for our own species even as we come to see ourselves as the gentle caretakers of our lonely Garden of Eden?

Luckily, in contrast to the horrors of the twentieth century, modern science has helped to elucidate the patterns of nature’s operation — and how human beings fit into them.

It has done so in a way that has gone far beyond the mythopoetic accounts of our ancestors — stunning in their own way, yet nonetheless much more esoteric and hard to practically apply.

It is now possible for us to create technologies, design homes and cities, and adopt modern lifestyles that have, at their foundation, lessons learned from nature’s processes — which are always effortless in their operation.

As we go about solving the problems of our current age, we can always carry — in the back of our minds — the awareness that a solution is valuable insofar as it helps guide our civilization back toward some form of equilibrium with the planet.

If it doesn’t, we can safely disregard the idea, and move along.

You may be thinking that this doesn’t apply to your life because these are problems that our society faces on a larger scale.

What can any individual hope to do in the face of such sweeping issues?

The answer is to learn how to be happy, and to be happy in such a way as to become the living example of the type of human being that the world needs more of in order for the human experiment to continue.

On the plains of Africa, material comfort was hard to come by.

For this reason, it is easy to understand why — as civilizations eventually flowered in Egypt, the Near East, and beyond — material luxury, wealth, and societal status were seen as the keys to happiness.

When capitalism came along, the keys to upward social mobility were granted to ever-increasing portions of the population, and we followed the examples of the kings, emperors, and aristocrats who had come before, thinking that if we could live their life, we’d finally discover joy.

But today, we know better.

More wealth doesn’t necessarily correlate to more happiness, and no amount of property ownership can compensate for an unfulfilling life.

And so discovering happiness today is a creative act — it is up to each individual to define the meaning of their own lives, to choose the activities that bring them the most joy, and to share the knowledge they learn along the way with others.

Such an approach toward life can begin taking the dark side of human creativity and begin transforming it into its opposite — the light.

We can deprioritize external markers of success in favor of internal ones — which nonetheless can be outwardly shared with others anyway.

After all, is not a well-written blog post a sign of one’s joy, or a beautiful painting the same?

We may not be able to get rid of our desire for progress, to constantly strive for self-improvement and the betterment of society.

But we can ensure that these desires are channeled in such a way as to ensure that they constantly carry that awareness of our tendency to create new problems as we solve old ones.

If we can do that then we may discover, as time goes on, that we create fewer and fewer problems with each new solution, until we finally reach a point at which humanity is, to paraphrase the psychonaut Terence McKenna, released into the potential of its very own imagination."
"In KNN, K stands for the number of nearest observations considered for predicting the class of new observation. e.g, There are two classes to predict i.e, 0 or 1. Now there are 5 neighbors near to the given observation and the class of those 5 neighbours is 1,1,1,0,0. It means new observation belong to the class 1. I hope this makes it clear that how important is the value of K in this algorithm. Again, like K in K means algorithm, KNN also allows to predict the correct value of K by iteratively trying all the values of K and see the best performance using technique like elbow method.

There is a little caveat here to the value of K. And, I feel this is the right time to put light on the concept called bias variance trade off. We can have one separate discussion for this topic. Just overview, high bias leads to under fitting of the model and high variance means complex model which could lead to overfitting. Most of you would have understand where am i heading to. Conclusion is that we need to have correct value of K to avoid overfitting and underfitting in this algorithm. e.g, We keep value of k is 1 then it is high biased and if value of K is N the it is high variance. You have to decide best option and value of K to get the best results.

Most of the time interesting question comes to mind, and it would have come to your mind too. It happened to me too. What will happen in case of DRAW (No winners). So, answer is quite simple, we always keep the value of K as odd, we don’t want to deal with conflict and give more complexity to the simplest model.

Advantages

1. Simple to implement

2. Very robust. Capable of handling non linear and linear solution both

3. low cost algorithm and easy to visualize too

Disadvantages

1. Expensive algorithm. It calculates distance at runtime for the test observation with all the existing observation to get given k values. Yes, of course, we have methods to overcome this approach and tune this.

2. Very sensitive to the given data. Noisy or unbalanced data. Wrong observations and outliers could make this algorithm perform worst. It needs perfect data to perform well which is almost requirement of every ML model. Common use cases : Forecasting, data compression or prediction etc."
"Could Carbon Save The Planet?

A new miracle material could turn the tides on global warming.

The world faces a crisis, one propagated by a single element, carbon. Humanity is pumping out record levels of carbon dioxide, methane, plastics and pollutants, all of which use carbon chemistry at their core. These chemicals are heating the planet, destroying our oceans and wrecking the land. But could a revolutionary new use for carbon help us turn the tides on the environmental catastrophe we have created?

We basically have two problems here, our waste polluting the environment and our means of generating energy. Our modern comfortable lives depend on the use of a vast amount of energy and the cheap supply of mass-produced goods. Both of which pump out carbon-based pollutants. So we need a way of reducing the carbon waste in the environment all-the-while keeping our sources of energy high enough to fuel our modern lives.

Recent innovations like electric cars that are actually useful, vast wind farms, solar power stations, recycling and biodegradable products have taken us closer and closer to climate nirvana. But these have two big flaws, the batteries needed to hold the energy are massively damaging to the environment (though an electric car is still better than a petrol one) and none of this actually reduces the carbon already out there in the wide world.

But a new battery technology could change all of that. Imagine if you could recycle plastic or pull carbon dioxide out of the air and turn it into a battery that would outperform anything in use today. This is the future promised by Graphene.

Digram of graphene — Pixabay

Graphene is simply carbon arranged in a flat honeycomb lattice a single atom thick. Sounds simple, doesn’t it? In fact, Graphite is lots of fragmented layers of Graphene on top of each other. So you will have some of this wonder material in your pencil case.

But Graphene has some rather amazing properties that mean it can create a battery revolution. It is a zero-gap semiconductor; this basically means it conducts electricity incredibly well, better than any metal. But it also ‘hangs onto’ the Electrons very well, the Electrons can leave the edge of the sheet and form a circuit, but they find it hard to leave the middle of the sheet.

We can use these properties to enhance our current battery technology (still storing the energy in chemical bonds), but if we could make enough Graphene, we could build supercapacitors out of Graphene (storing energy in pure electromagnetic potential).

A capacitor is just two plates of very electrically conductive material sandwiched either side of a thin insulator. This means you can pass a change between the two plates, one becomes electrically positive, and the other negative as one gets Electron-rich and the other Electron-poor. Once the maximum charge has been built up, you can use it just like a battery.

Normal capacitors are great for dumping small amounts of energy very quickly. The flash on a camera tends to use a capacitor to generate a short bright flash, but it couldn’t power a light for any decent period of time.

Bring in Graphene supercapacitors, just like normal capacitors they can dump all their electrical energy very quickly, but they have a much higher capacity. One of these could power a light for a very long time. Even better they only take a few seconds to charge fully!

So while a Graphene doped Li-ion battery can now be found in the highest performing commercially available batteries, it is these supercapacitors that will bring along a revolution.

Imagine an electric car powered by Graphene supercapacitors. It would be able to charge in a minute or two, power the car for 500 miles or dump all of its energy at once, delivering several thousand horsepower. All while the battery pack weighs less and the batteries don’t degrade over time. How brilliant!

But this quick charging and high power doesn’t just mean insanely good cars. One of the big issues we have with renewable energy is capturing it for later. We have very efficient solar and wind power, but our current range of batteries can’t charge quick enough to make the most of the peak power moments. This means loads of the energy generated actually goes to waste. On the contrast, Graphene supercapacitors would be able to keep up with the charging and power demands that we have become accustomed to.

Using Graphene supercapacitors to store renewable energy means that we can switch to 100% renewables quicker, with less wind farms and solar power plants.

Solar power plant — Pixabay

Oh, and on the subject of solar and wind power. Graphene can be used to make ultra-efficient solar panels and generators. So having wide-spread use will also mean that these power plants can produce even more power!

It is important to remember that Graphene is non-toxic. We could dump thousands of tonnes of it in the ocean and we wouldn’t see many negative effects — certainly not to the scale that our current technology does.

All of the current technology that it could replace (like Li-Ion batteries) require toxic, rare Earth metals. These are killing the environment, are hard to come by and the mines used to extract them have questionable human rights records.

Graphene could be made in a way no other rare Earth metal can be, atmospheric extraction. There is currently too much CO² in the atmosphere and too much plastic in the water. We can extract the carbon from these sources, releasing oxygen from CO² and Hydrogen from plastics (which we could also use as fuel). This would mean that the Graphene produced could be massively carbon negative, reducing the amount of carbon in the environment, turning the tides on climate change and pollution!

So, why don’t we have Graphene powered smartphones or cars yet? If this technology is so amazing shouldn’t we be pushing for it?!

Well, Graphene is a little harder to produce than I have lead on. Our technology to make it right now can produce small flakes of Graphene that can be used to enhance metals, fabrics and sensors at a relatively cheap price. However, making large sheets of graphene, as you would need for superconductors, is not currently possible. After all, this is a single atom thick sheet; we are talking about engineering that is on the molecular scale here.

However, there is no reason to say that these advances are impossible. After all, scientists in 2015 discovered a way to make Graphene for 100 times less and just a few days ago NanoTech got $27.5 million in funding to create Graphene batteries. Jumps in technology like this means that your devices may be powered by pure carbon in the not too distant future.

So, can carbon save the planet? Theoretically yes it can! There is a way in which we can use the carbon compounds that we have polluted the world with to produce technology far more powerful than we currently have whilst reducing the global levels of carbon. We just need to figure out the logistics of getting these atoms in the right order."
"The Tips

Don’t start writing anywhere and everywhere

Before you start writing, establish a few places where you can do serious writing without any distractions (your desk in the study, your bedroom, at the garden, etc.). Once you establish the place, make it sacred. Nothing else should be done out there.

Prepare a writing chart for the week ahead mentioning the datess and times at which you’ll write

These charts need to track a minimum of three things: (1) time spent writing, (2) pages completed, and (3) the percentage of planned writing completed. These charts provide the motivation and feedback you need for your writing journey.

Steal like an artist

Yes, there’s no such thing as a new-minted, original idea. And stealing like an artist means reading others’ articles and identifying topics where you can bring your own perspective and treatment based on your unique experience.

The best time to get ideas is when you’re at your groggiest (half asleep, just waking up, etc.)

Studies have proved that our focus broadens when our minds are at a suboptimal level, allowing us to create more connections, as we’re open and in a free-flowing mode.

On the other hand, the best time to write is when your creative cells are at their extreme peak and when you’re having your best focus (mornings, evenings, or whatever time works for you)

Just write in the free-flow mode without any interruptions in order to achieve maximum productivity.

Combine ideas to find new ways to connect existing themes

For example, take a piece of paper, and write some suggestions in some categories.

Programming (e.g., senior programmer, junior programmer, polyglot programmer)

Relationships (e.g., casual, long-time, long-distance)

Software-development techniques (Agile, Kanban, etc.)

Now pick up any two items, and write a story. For example, you can write the story about two senior programmers using Agile, or you can write about a polyglot junior programmer.

One of the easiest methods to write technical articles is by using the Carpenter method

Select the ideas. Breakdown/outline your intended article into paragraphs (opening. middle and closing). String the ideas within the outline to create the first rough draft. Tackle one paragraph at a time, and polish it further. Read and edit the whole flow, and tighten the final piece into a nicely flowing article.

Remember the key message in this style is to let go of perfection. The first draft is imperfect, and that’s perfectly OK. The final output is your masterpiece, your chef-d’œuvre, and only that is important.

The problem statement should be clear and should be delivered by the second paragraph — at the latest

Yes, many authors neglect to write a clear problem statement, leaving the readers scratching their heads figuring out what the writer is trying is tell them in the article. The key is to have a clear, concise, and well-articulated problem statement before the solution.

Use the inverted-pyramid concept to structure your solution in such a way that the reader stays interested

The concept is as follows:

Summarize the most important point, the lede. Reveal the direction the article is headed. Flesh out the story with further details added to pique the reader’s curiosity.

Don’t use jargon

When I say not to use jargon, I’m not advocating leaving out necessary technical terms, but I’m saying it is to ensure the language is as clear as possible.

For example, a hardcore technical term like the modulator valve control ring can be referred to simply: “Tighten the modulator valve control ring securely.” This is preferable to adding complexity like, “Apply sufficient torque to the modulator valve control ring to ensure that the control ring assembly is securely attached to the terminal.”

Don’t use a long word when a short one is available

If writers use pretentious words when other simpler words would convey the same meaning, they risk alienating the reader.

Strike out as many words as possible

The journalist Harold Evans once famously invited readers to consider which words written on a market signpost can be deleted without altering the meaning. The words were “FRESH FISH SOLD HERE.”

Evans said all of the words can be deleted, as fish needs to be fresh and sold. So the signpost itself is a waste of money. The idea is to delete and delete and delete ruthlessly until nothing can be deleted any further without altering the meaning.

Good writing is simple

In fact, the hallmark of good writing is it can be expressed in a way that reasonably intelligent people can understand. If that’s not happening, it just means the writer hasn’t taken enough pain to present it succinctly. Always boil down a sentence to its simplest form, and keep doing it until it can’t be simplified any further.

Pacing is the key to reader engagement

When it comes to the flow of your article, what matters most is that the pace you’ve chosen remains consistent as the article progresses. A consistent pace (slow, fast, meandering, etc.) keeps the reader engaged and interested until the end.

Package your writing in small chunks

Human-memory buffers are notoriously short and can’t contain more than 15 words at a time. This puts a constraint on the reader’s understanding, and by the time a reader completes a really long sentence, their buffer gets depleted and they lose track. That’s why breaking your copy into small, concise, and meaningful sentences is required to pass the message across succinctly.

Speak the language of the audience

For example, a post you write on LinkedIn for senior-level software professionals will read very differently from a post you write on Facebook for school children. Not only do these two groups of people have different challenges, but their language, their taglines, and the lingo they use to speak about their needs also differ considerably.

You need to research your audience to know the kind of language they speak and tailor your article accordingly.

Be very careful when using tables and charts, as sometimes they contain unnecessary information that diverts the reader from your article

Use these artifacts only when they’re really required, and the reader needs to go through them to understand the gist. Avoid if not required.

The closing paragraph should contain both a summary and a conclusion, both of which are very important in technical articles

The summary section gives the gist of your article and gives your reader a flash-card type of takeaway.

The conclusion, on the other hand, is the place where the author can conclude the journey of the problem, starting from the introduction right until the final resolution. This is also the place where authors can highlight any unresolved issues or even next steps (a sequel to the article maybe?).

Acknowledgments and references are often overlooked

You may have gotten help from somebody to strengthen your article or maybe you referred to a book in your research for the article. While a separate section might not be required, it’s always a good practice to be gracious in acknowledging help if taken.

Pay attention to food

A proper food balance is important for maintaining healthy nutrition for the mind and body. And recent studies published in the Public Health Journal have also proved it takes less than 30 minutes to go from a sugar rush to a full-on sugar crash, which means we get more tired than energized thanks to sugar consumption.

The key is moderation. Eating moderately means listening to your body and thereby helping your mind to perform to its full potential. Charge yourself into a better writer.

Lastly, don’t push yourself too hard

Writing needn’t be as strenuous as running a triathlon. Just write for 30 minutes to one hour, and then go about your day. Then on the following day, push yourself a bit harder. Build your writing muscle, one ligament at a time. Not a wasted word. Not a wasted effort. This is the key to being a productive and creative writer."
"PART 1: A JOURNEY INTO THE PAST

“No sensible decision can be made any longer without taking into account not only the world as it is, but the world as it will be.”

— Isaac Asimov

1 Earthrise

“Collectively and individually, we have more control now over how our future unfolds than ever before. But our ability to envision and engineer the future comes with almost unimaginable levels of responsibility.”

2 Origins

“We now know that what we think of as ‘past’ and ‘future’ are merely the byproducts of the laws of physics that emerged after the big bang took place.”

3 Light

“Light reveals a pathway between where we are now and where we’re heading. It enables us to develop new knowledge as it illuminates the world around us.”

4 Movement

“Movement — whether the oscillation of an electron, the passing of a light beam, or the falling of a glass — marks the transition between past and present, and points toward the future.”

5 Time

“We are, at every level, creatures of time, immersed in it, obsessed by it, yet unable to control it.”

6 Entropy

“Entropy is one of those concepts that people often invoke when they try to explain life, the universe, and everything, yet is rarely understood — much to the chagrin of physicists the world over.”

7 Emergence

“We are, in effect, localized anti-entropy machines that have the ability to change the future from its default mode to something entirely different.”

8 Evolution

“DNA turned out to be an incredibly powerful entropy accelerator. Fed by heat, chemical energy, and ionizing radiation, it became the defining base code of increasingly advanced organisms that were ever more adept at making use of the energy around them and discarding it in a slightly less-usable form.”

9 Anticipation

“[Anticipation] is part of a suite of abilities that lay the foundations that enable organisms to not only envision the future, but to plan for it as well.”

10 Instinct

“Instinct relies on the future being similar to the past, and predictable based on what’s happened time and time again. But humans have put a huge wrench in this biological master plan as we’ve developed the ability to change the future faster than any evolutionary process can accommodate.”

11 Causality

“No matter how convoluted and complex the threads tying the past, present, and future become, each past action sends ripples into the future that spark a cascade of sympathetic reactions.”

12 Memory

“If we have no memory of what’s happened in the near past, we have no way of connecting effects we observe to what caused them. And this in turn means that we cannot begin to understand how our actions potentially influence the future.”

13 Learning

“Learning is what begins to carry us beyond instinct and allows us to start intentionally crafting the future.”

14 Intentionality

“Intentionality is the connective tissue between learning and outcomes. It’s the link between observing which levers in the present can be used to nudge the future in different directions, and having the wherewithal to actually pull them.”

15 Intelligence

“[E]ssential as our intelligence is to imagining and building the future, it’s surprisingly hard to pin down precisely what we mean by it.”

16 Knowledge

“When combined with our intelligence, knowledge helps us begin to connect cause with effect, and to create the models and tools that allow us to make use of these connections.”

17 Reason

“Our ability to reason is what helps us imagine the possible outcomes of events and actions, and to focus on the more plausible ones.”"
"Introducing his students to the study of the human brain Jeff Lichtman, a Harvard Professor of Molecular and Cellular Biology, once asked: “If understanding everything you need to know about the brain was a mile, how far have we walked?”. He received answers like ‘three-quarters of a mile’, ‘half a mile’, and ‘a quarter of a mile’.

The professor’s response?: “I think about three inches.”

Last month, Lichtman’s quip made it into the pages of a new report by the Royal Society which examines the prospects for neural (or “brain-computer”) interfaces, a hot research area that has seen billions of dollars of funding plunged into it over the last few years, and not without cause. It is projected that the worldwide market for neurotech products — defined as “the application of electronics and engineering to the human nervous system” — will reach as much as $13.3 billion by 2022.

So, despite our admitted lack of understanding, it seems the brain is a new and significant frontier for tech-pioneers looking to reinvent — and perhaps irreversibly influence — the way we interact with the world.

The Royal Society report speculates:

Mental health conditions could be treated by using interfaces to target relevant parts of the brain, bringing relief to the hundreds of millions worldwide who have depression. Even Alzheimer’s disease, which has proved resistant to conventional therapies, might be halted or reversed.

Outside of medical use:

People could undergo ‘whole brain diagnosis’ to identify their unique talents and challenges. Today’s ‘brain training’ computer games, whose impact is debated, might give way to demonstrably effective ‘brain cleaning’ or ‘mind gym’ sessions to keep minds sharp and creative. Neural interfaces offer myriad possibilities to enhance everyday life. We could use our minds to open doors, turn on lights, play games, operate equipment or type on computers. Then there are opportunities to enhance or supercharge the brain itself. Implants, helmets, headbands or other devices could help us remember more, learn faster, make better decisions more quickly and solve problems, free from biases… Mood, knowledge and memory could be securely and confidentially backed up or uploaded to a digital cloud.

I know, it’s a lot. And I’ve omitted the references to telepathy, the potential merging of human with artificial intelligence, and the option to hook your neural interface up to that of another animal, like a bird.

To a sci-fi nut, this must all sound like manna from heaven. To the rest of us it’s likely to be a little bewildering (to say the least). So, is this a real proposition? Or just the (fairly creepy) wishlist of some over-ambitious Silicon Valley nerds?

The truth is that it’s difficult to tell what the long-term trajectory for brain-computer interfaces will be but, to a degree, they are already here. Though still fairly elementary, we currently have drones and artificial limbs that can be controlled using the brain alone, as well as headsets that boost concentration and memory. Some of these technologies are invasive, but many are not. Some record and react to brain activity, some stimulate it, and some do both.

Reassuringly, it’s non-invasive technologies that look to be headed for commercial distribution. Most of these are re-imaginings of the electroencephalogram (EEG), a system that monitors and records electrical impulses in the brain. One of the leaders in the commercial space, CTRL-Labs, specifically focuses on what it calls ‘intention capture’. Their product is a electromyogram (EMG)-based wristband, which can respond to electrical signals as they activate in a user’s arm muscle. At the moment, the company’s demo has a player controlling a simple game using only this impulse detection and no physical movement (take a look).

If you’re cynical about how far this could go, you should know that Facebook acquired CTRL-Labs last month, and just a couple of weeks ago leaked transcripts from Mark Zuckerberg’s internal meetings reinforced the firm’s keen interest in brain-computer interfaces.

Giving his thoughts on Elon Musk’s Neuralink project, Zuck says:

I am very excited about the brain-computer interfaces for non-invasive. What we hope to be able to do is just be able to pick up even a couple of bits. So you could do something like, you’re looking at something in AR, and you can click with your brain. That’s exciting … Or a dialogue comes up, and you don’t have to use your hands, you can just say yes or no. That’s a bit of input. If you get to two bits, you can start controlling a menu, right, where basically you can scroll through a menu and tap. You get to a bunch more bits, you can start typing with your brain without having to use your hands or eyes or anything like that. And I think that’s pretty exciting. So I think as part of AR and VR, we’ll end up having hand interfaces, we’ll end up having voice, and I think we’ll have a little bit of just direct brain.

If a little bit of “direct brain” doesn’t bother you, it’s worth looking ahead to the possibilities that extend beyond basic control of an elementary system.

For example, we already have neural systems that can read moods and emotions. Last year, The South China Morning Post reported that this kind of technology had been deployed by Chinese firms looking to monitor employees for signs of anger, anxiety or depression using devices built into headwear and hats. And perhaps even more impressively (or disturbingly), researchers at Kyoto University in Japan have been able to use a deep neural network to convert brain signals from an fMRI scan (used to map neural activity) into an image that contains many of the shape and color characteristics as one viewed by the subject of the scan.

This is all just to say that these types of systems are unlikely to cease development once they provide the capabilities to click or scroll in Mark Zuckerberg’s AR hellscape.

The Royal Society report makes sure to flag some early concerns. Most rational-thinking people won’t be too far behind them: What would it mean if an external company or government could gain access to our moods, or even our thoughts? How might human privacy — and indeed autonomy — be protected in if these technologies became ubiquitous? How can we ensure that they wouldn’t be weaponized by bad actors or governments to influence and control entire populations? (And is it okay if they only want to subliminally coax us to eat more healthily or respect the rules…?)

It’s not hard to think of governments that will be watching the progression of this technology very keenly.

Though it’s only fair to weigh risks against benefits before eagerly ringing the alarm bell, even here there is ambiguity. The benefits of commercializing this technology seem extremely limited, at least on the face of it. Game play? Fitness? Hands-free navigation of an augmented or virtual reality environments? None of these feel like strong arguments for selling access to our brains.

But what about neural interfaces that could improve memory or concentration, making us super productive in life and work? Presumably, one could make the case that this is a worthwhile trade? Well, incidentally, completely separate research released just after the Royal Society report should urge caution around attempts to enhance such functions.

A new journal in Science published findings that appear to affirm the long held theory that there is an active “forgetting mechanism” which kicks in while we sleep. The study found that when researchers suppressed neurons that produce the naturally occurring hypothalamic melanin-concentrating hormone (MCH) in mice, their memory performance actually increased. In other words, without this unnatural suppression these hormones act very deliberately to impair — or “modulate” — our memories.

This is a biological addition, not some kind of “lack” that we must compensate for with technology. We might safely assume that it serves some worthwhile evolutionary purpose.

Indeed, there is good reason to believe that if we didn’t forget we would live in a perpetual state of confusion, our brains awash with confusing superfluous information. One curious story that speaks to the chaos of the ever-remembering mind is that of the man who became known as subject S; a young Moscow-based journalist (later identified as Solomon Shereshevsky) who approached neuropsychologist Dr. Alexander Luria in 1929 with a very peculiar problem: he could not forget.

According to Luria’s reports, subject S. was able to remember foreign poems, scientific formulas, and enormously long strings of words and numbers decades after he had been told them. He recited them to perfection every time Luria tested him.

Great asset, eh? To never forget a name at a cocktail party, miss a birthday, fail a test on a fact or formula you already learned? To remember your own human life with crystal clarity rather than with the foggy haze that tends to wash over even our dearest memories?

Not so. According to the New York Times:

S.’s ability to remember was also a hindrance in everyday life. He had a hard time understanding abstract concepts or figurative language, and he was terrible at recognizing faces because he had memorized them at an exact point in time, with specific facial expressions and features. The ability to forget, scientists eventually came to realize, was just as vital as the ability to remember.

Who knows what psychological or neural confusion could eventually be brought on by using brain-computer interfaces to optimize evolutionary facets…

But we probably shouldn’t run screaming for the hills just yet. These systems are in their infancy, and there have been incredible breakthroughs in the research that should yield great benefits for people with mental and physical impairments. Nevertheless, The Royal Society are right to get ahead of the ethical and moral dilemmas that will accompany the commercialization of this type of technology. It is unfamiliar terrain, and allowing a system to intervene on our physical and mental capacities is an unprecedented encroachment that could easily turn sour. Certainly if we are to judge by the ways technological intelligence and surveillance have been wielded so far.

For now we should keep a close watching brief on how this technology develops, as well as any-and-all proposals for its use. One thing seems to be true, if we thought society had already reached its technological saturation point, we “ain’t seen nothin’ yet”."
"Sarah Bernhardt plays Hamlet, London 1899

“What’s Hecuba to him, or he to Hecuba,

That he should weep for her?”

The close of Act II Scene ii, and Hamlet questions how the performers in a play about the siege of Troy are able to convey such emotion — feel such empathy — for the stranger queen of an ancient city.

The construct here is complex. A play within a play, sparking a key moment of introspection, and ultimately self doubt. It is no coincidence that in this same work we find perhaps the earliest use of the term “my mind’s eye,” heralding a shift in theatrical focus from traditions of enacted disputes, lovers passions, and farce, to more a more nuanced kind of drama that issues from psychological turmoil.

Hamlet is generally considered to be a work of creative genius. For many laboring in the creative arts, works like this and those in its broader category serve as aspirational benchmarks. Indelible reminders of the brilliant outlands of human creativity.

Now, for the first time in our history, humans have a rival in deliberate acts of aesthetic creation. In the midst of the avalanche of artificial intelligence hype comes a new promise — creative AI; here to relieve us of burdensome tasks including musical, literary, and artistic composition.

In a recent episode of Gigaom’s Voices in AI podcast, Max Welling, a Vice President of Technologies at Qualcomm, explained the possibility of replicating an endeavor that, until now, was the sole purview of humanity:

“Sometimes people say that being creative is something very magical. That’s not something we could ever create in a computer, but I don’t agree… Creativity is really combining sort of elements of things that you’ve learned in the past in very surprising new ways, right? A recombination of modules into new ways that are very surprising to others. Now we are reasonably good at this as humans but I would say, with computers, there’s no reason why they couldn’t do that a lot better even than humans.”

Welling is not alone in either his view of what constitutes creativity or his ambition for intelligent machines. When asked about a future in which children might learn creativity from educational systems, a spokesman for edtech firm Squirrel AI similarly told me: “We believe artificial intelligence will surpass humanity in terms of creativity in 5 to 10 years.” Surpass! Apparently, they have ascertained the four component building blocks of creativity, each of which is now demonstrable in artificially intelligent computers: extensive knowledge, curiosity, divergent thinking, and logical induction.

Creative AI, another company invested in this growing field, may not have declared that the literary works of computers will make great novels look like bored doodles, but they have championed its burgeoning prowess in assisting human creativity. They say we’re trending towards unprecedented access to creative tools (photo editors, music studios, etc) as well as greater opportunity for collaboration (social online platforms). This is giving rise to what they’ve termed “the escalation of creativity”; or “a world where creativity is highly accessible and anyone can write at the level of Shakespeare, compose music on par with Bach, paint in the style of Van Gogh…”

Such opinions are indicative of a viral belief that just about anything can be broken into modules or captured as data, quantified, and ultimately replicated or coached by machines. Even creative brilliance.

Edmond de Bellamy, a generative adversarial network portrait painting

To me, this deeply misunderstands the goals and the joys of true creativity.

Even if we concede to Squirrel AI that curiosity (for example) is an axiomatic part of creation, they chose evidence it with reference to a deep learning system that recently outperformed human players of the game, Montezuma’s Revenge. This might be impressive, but its a kind of curiosity needs to be closely stewarded by the machine’s human creators. And while evidence suggests that toddlers learn and adapt in a markedly systematic way — making inferences based on statistical probability using similar “algorithms” to AI — they are indisputably the volitional prime mover in this process. As Berkeley University professor and developmental psychologist Alison Gopnik characterizes it, small children “get into everything” as part of their evidence gathering. We know from experience that this means exploring various surroundings, artefacts, emotions, communications, sensations, etc.

Even for a “curious” AI, their equivalent domain is necessarily limited. Artistic AI learns how to do art from art; it does not compose art in response to its environment. Yet this is a crucial component of what creative art is. It is a reaction to and an expression of our lived experiences.

Where we find creative genius — be it in a single work of creativity, or in the collective works of one human creator — we are taken with its ability to assimilate and transmit something profound. We may not be able to articulate its features, but we can say that it is a vehicle for message, usually wrapped in an emotion. A mechanical system might be able to produce something that looks just like a Banksy but, like a forgery, it’s affective inauthenticity invalidates it’s artistic value.

A true creator cannot be without thought or consciousness, and an AI cannot produce work that is a commentary on a world it cannot experience. Just as one can’t express the feeling of the wind in one’s hair if one has never experienced the wind, nor grown any hair.

Outside of the sensual or emotional, great feats of creativity generally draw down our dense networks of associations familiar to human beings, but often unspoken. Paul Sager has written of the massive emphasis humans place on associations, borrowing an example from Simon Blackburn:

“Imagine I invite you over to dinner and, while carving the roast, I casually mention that this is the very knife that the assassin used to murder my wife and children. Would you still be comfortable eating the slice of beef I’ve just plopped on to your plate?”

It is the salience of these types of social and psychological associations that creatively brilliant works play with so adeptly. As a corollary, much of our enjoyment is derived from our efforts to determine the creator’s motivations, or the subject’s intent.

Banksy, Girl With Heart Balloon

Is Hamlet really mad? Is the Mona Lisa smiling? What do Pi’s animals teach us about belief? And Willy Loman about success? Why does David look so mournfully at the Head of Goliath?These things are not accessible through the computational analysis of data. Nor can works of similar worth be produced without a species-level understanding of the human condition that underpins them.

This is not to say that science has no place in creativity. On the contrary, the history of creative artifacts would be much poorer without assistive technologies. Paintbrushes, pens, typewriters, photographic cameras, recording studios — the list goes on. With this prodigious ancestry, new artificially intelligent technologies will undoubtedly facilitate as yet unknown fields of creative art to amaze and challenge us all. The rather smug language of “augmentation” is the vogue way of describing this assistance, but it is just a veiled way of admitting that, when it comes to meaningful creative efforts , AI will be a tool, not a Tolkien.

This distinction is important, and especially at a time when practitioners are describing artificial creativity as “the ultimate moonshot” for AI. In reality, these systems are not striving to create beautiful, challenging things. Rather, they are trying to predict and produce things that humans may respond to as worthwhile or beautiful. In other walks of life — like marketing and advertising– such methods have so far been extremely successful in predicting, and then gradually shaping, our preferences and behaviors. Soon we may find our artistic tastes recalibrating in favor of creative works that are produced quickly, easily, and at a fraction of the cost of authentically composed pieces. This shaping may already be happening.

Like Hamlet’s players, AI does not portray true emotion, only its superficial signifiers. It is mimicry, which has its uses (including entertainment), but can never replace brilliance. Let’s make sure that, as attempts are made to boil human creativity down to its composite parts and cheaply simulate it, at the same time intelligent technologies are used to cultivate and elevate the real deal."
"Infection | Brain

When Tics, OCD, or Anorexia Develop Following Strep Infection

It’s called PANDAS (Pediatric Autoimmune Neuropsychiatric Disorders Associated with Streptococcal infections).

In the late 1900s, physicians reported that children started showing tics following a strep sore throat outbreak. The tics also disappeared when strep is killed by antibiotics.

A 2007 Case Report

An 8-year-old boy with recurrent strep sore throat developed a set of rituals before eating. He nodded, flapped his arms, tapped his mount, opened and closed his fists, etc. before taking a bite. He said the rituals helped him cope with intrusive thoughts that the food is poisoned. He became severely conscious with healthy eating and body fat, avoiding all ‘bad’ foods and eating only 200 calories/day — eventually losing 8kg. After receiving treatment for his recurrent sore throat, all his behavioural anomalies vanished.

The boy had PANDAS (Pediatric Autoimmune Neuropsychiatric Disorders Associated with Streptococcal infections), obsessive-compulsive disorder (OCD), tics and anorexia nervosa.

PANDAS: What you need to know

Diagnosis: It’s diagnosed when a child develops tics or OCD right after a strep infection. Or when symptoms of tics/OCD worsen following a strep infection. The DSM-5 put PANDAS under “Obsessive-Compulsive and Related Disorders Due to Another Medical Condition.” While PANDAS is more closely associated with tic and OCD, it may include anorexia in some cases.

Cause: Nearly all children experienced strep throat infection at least once in life. The average schoolchild gets it 2–3x per year. It’s caused by a contagious bacterium called Streptococcus pyogenes. A strep sore throat infection can clear up on its own or with simple antibiotics. But in a small subset of children, perhaps with genetics and cultural predispositions, it can lead to PANDAS.

Statistics: Due to the lack of diagnostic guidelines, its precise prevalence is still unknown. Scientists, however, estimated that PANDAS contribute to at least 10% of childhood OCD and tic disorders.

Treatment: Usually with antibiotics and cognitive behavioural therapy. Steroids and antidepressants are prescribed if deemed necessary, though antidepressants often cause side effects in children with PANDAS. The best treatment is aggressive antibiotics — with about 50% cure rate according to a 2017 population study.

Prognosis: Long-term outcomes of PANDAS vary between cases due to the variability of PANDAS subtypes and treatment protocols. “Whether children with a PANDAS subtype typically will go on to remission or progress to a more chronic course of illness is not known,” says Tanya Murphy, MD, professor and director at the Rothman Center for Pediatric Neuropsychiatry, Florida.

If Strep infection is recurrent, for example, long-term antibiotics until adulthood is necessary. Even if Strep is eliminated, some may still suffer neuropsychiatric symptoms as the brain (or more specifically, the basal ganglia) is sufficiently compromised. Some children recover naturally too as the immune system matures and can distinguish Strep from the brain.

Basal Ganglia

Tic is viewed as the most common form of movement disorder in children. Tics are sudden and repetitive movement (e.g., shoulder shrugging, blinking, or head jerking) or vocalizations (e.g., sniffing or grunting). Tics can be short-term (11–20% prevalence) or long-tern (0.5–0.7% prevalence). Whereas OCD affects 1–3% in the general population. It’s characterized by persistent unwanted thoughts (obsessions) and/or repetitive behaviours driven by strong urges (compulsions).

What PANDAS, tic and OCD have in common is that the basal ganglia are involved. It’s a set of brain structures deep within the brain that is enriched with dopamine neurons responsible for movement coordination. It also has connections to the outer cortex that controls voluntary movement.

The dopamine-powered basal ganglia — motor cortex circuitry “control and select goal-directed motor, cognitive and motivational behaviour,…inhibitory control and habit formation” psychiatrists wrote.

The Trickster Streptococci

About 40% of children and adolescents with tics/OCD have higher levels of Strep-specific antibodies in their blood. This means that their immune system was actively fighting Strep bacteria.

Intriguingly, their Strep-specific antibodies reacted with brain dopamine receptors in a lab plate. Give Strep-specific antibodies to rodents and the antibodies deposit in the basal ganglia, causing motoric disorders. Immunize mice for Strep infection and the same thing happened.

This shows that antibodies generated by the immune system to neutralize Strep can invade the brain and attack the basal ganglia (where dopamine neurons are abundant). In a sense, the immune system mistook the basal ganglia for Strep bacteria. Or that Strep bacteria trick the immune system into attacking the basal ganglia.

This is a form of autoimmunity — as the ‘A’ implies in PANDAS — when the immune system attacks the host own cells.

What’s the Trick?

Molecular mimicry is currently the best explanation for any autoimmunity initiated by microbial infections. In molecular mimicry, the structure of the pathogen is similar to some tissues of the body. Antibodies that bind to the pathogen might also unintentionally bind to some other tissues by chance.

Chances of molecular mimicry happening are unknown. Scientists think that people with genetics prone to developing autoimmune diseases might be at a higher risk for PANDAS.

“Maybe you have more of a genetic risk for OCD or anorexia, and the infection then unmasks that vulnerability. That’s one possibility,” opines Kyle Williams, the director of the pediatric-neuropsychiatry-and-immunology program at Massachusetts General Hospital for Children.

PANDAS Anorexia

Symptoms of anorexia can be viewed as OCD, such as food and body weight obsessions and compulsive exercise or purging. Anorexia is also a neuropsychiatric disorder with multifactorial causes that include sociocultural, psychological and biological aspects.

In 2000, Mae Sokol, MD and professor of psychiatry at Creighton University in Omaha, reported 4 cases of “infection-triggered anorexia nervosa in children.” These children were also diagnosed with PANDAS and OCD. They successfully regained their weight (and OCD disappeared) after taking antibiotics.

“Maybe we are seeing the same thing in children with PANDAS anorexia and children with PANDAS OCD — only in the PANDAS anorexia, the obsessions are about food and weight, whereas in PANDAS OCD they are about other things,” Prof. Dr. Sokol said.

“Since the basal ganglia are also involved with emotion, we think this area of the brain may be affected slightly differently with each condition,” the psychiatrist professor added. Besides the basal ganglia, researchers have found higher levels of antibodies that target appetite-regulating hormones in the bloodstream of anorexics. They also suggested that molecular mimicry is at play but has not pinpointed a specific microbial agent. More details here:

It seems that Strep could very well be the microbial agent playing molecular mimicry tricks on the immune system to target the basal ganglia (and appetite hormones).

To Close

“We say disease is due to biological factors, social factors, and psychological factors all interacting together,” remarks Jim Morris, a professor at Lancaster University, UK, in response to PANDAS. “Well, it works with psychiatric disease as well.”

Researchers have focused on genetics and other medical comorbidities for decades. But now the microbial angle has started to gain recognition as part of the biological aspect in psychiatric diseases. Though scientists still have a long way to go, as evidenced by the lack of clinical studies, diagnostic and treatment guidelines, statistical prevalence, etc.

“When a child suddenly develops food rituals, becomes unusually picky about food and loses weight, you might not think of a bacterial, viral, or other types of infection,” says the psychotherapist Sharon K. Farber, PhD, at New York University.

This article was inspired by Caelyn H."
"Raise your hands if you ever heard about Data Science! It has been the buzz word in the recent past. This field can retain and stay in the marketplace for a long time and provide numerous opportunities to people who add value by utilizing data. I strongly believe Data science is the place to be going forward in the next couple of years or even decades.

Firstly, What is Data Science?

Data science is a field that uses algorithms, processes, and models to extract knowledge and generate insights from data. To know more about Data Science, you can read my previous article here.

There’s lots of data in the world. And ultimately, data is the resource that our scientists work with. If there is more data, there’s more work, and if there’s less data, there’s less work. Let’s jump straight into it. Since the dawn of time up until 2005, humans had used 130 exabytes of data. Well, this tells us absolutely nothing.

What is an Exabyte? Why is this impressive?

Well, let’s start from the ground up. Let’s start with one byte here we’ve got the letter A. It takes up exactly one byte of space.

Now, assume we have a page of letters. Each page of letters can have between two thousand to five thousand letters. So we’ll say that half a page of text is about right for one kilobyte.

Let us zoom out a thousand times, and you’ll get a book. So a book of about 500 pages that’ll take up about a megabyte.

Designed using Canva

Now, let’s zoom out another thousand times. And what we’ll get is a gigabyte, and a gigabyte is sufficient to fit the whole human genome. So a human genome, once it is coded (in an ideal world) can fit into a gigabyte.

Source — By Elymas via Shutter Stock

So you can fit an entire person onto a gigabyte if you think of it that way. But here you can counter-argument by telling that the genome of a person is not sufficient to tell you everything about that person. A person has life experiences. Different people live their lives differently. And there’s lots more to a person than just a genome.

Well then, let’s zoom out another thousand times, and we’ll get to a terabyte. If you take an HD camera and you follow a person for every single day of their life for every single hour, minute, and second and you film everything that they’re doing for 70 or 80 years, you can fit all of that material onto one terabyte.

Source — By Donald Tong via Pexels

How about that?

This seems to be pretty impressive already. So one terabyte can fit all of your life experiences filmed throughout your life for every single second that you live.

And what next? This is getting better.

Source — By Tom Fisk via Pexels

Consider the Amazon rain forest that takes up about 1.4 billion acres. That’s 1.4 billion acres of trees. There are about 500 trees per acre, making it about 700 billion trees(Estimated). Now, if you take all of these trees and you chop them down, and you turn them into paper, and you fill that paper with letters entirely on both sides of the sheet, then that will amount to approximately some petabytes of data.

Well now, if you zoom out another thousand times, you will get an exabyte, and an Exabyte is a thousand petabytes.

I guess you got an idea of the abundance of data we have used.

Wait! I mentioned 130 exabytes until 2005 only. What about today? How much data have we consumed until today?

Designed using Canva

So you can see that data in the world is growing very very quickly. Data is being generated in huge numbers as time moves on."
"Maddie Stone published a generous, thoughtful, and mind-expanding essay exploring the implications of the near future extrapolated in my latest novel:

Geoengineering, or hacking the planet to cool it down, is either a maniacal plan dreamt up by foolhardy scientists or a useful tool for staving off climate catastrophe — maybe both. It raises hard questions about what sorts of sacrifices humanity may have to make for the greater good and who gets to decide; questions that beg for nuanced conversations about the social, environmental, and political risks and rewards.

Yet in science fiction, geoengineering tends to get treated with all the nuance of Thor’s hammer striking a rock monster. Which is why Eliot Peper’s recent novel Veil, set on a near future Earth beset by climate crises, is such a refreshing read. This book gets geoengineering right by showing that there are no obvious right answers."
"The Massachusetts Institute of Technology (MIT) has recently released information about a potential game-changer in screening for COVID-19 asymptomatic patients.

So far during this pandemic, scientists have identified that there are individuals who show symptoms, and those who are infected but show no symptoms at all. These pose a substantial danger given they feel no need to stay at home, and rightly so, and thus potentially contaminating whoever they are in contact with.

Researches at MIT, however, have developed an artificial intelligence model that can detect the virus in asymptomatic individuals. After being fed with new recordings, the model correctly identified 98.5% of coughs from people who had tested positive, including 100% of coughs from asymptomatic people who showed no symptoms but had tested positive for the virus.

But how does it work?

Some background on AI models without going too deep — an AI model is typically fed a large amount of the same type of data you want it to look out for.

Let’s say we’re training a model to differentiate between manager signatures. We would give our cognitive model (which may be looking out for a range of parameters such as colour, length, thickness, etc of a signature) a large volume of signature images. How large is large? As many signatures, as you can acquire (with permission from the managers of course) — think hundreds if not thousands. The more data your model has to work with, the stronger it will become at differentiating between those signatures.

Each signature is fed to the model along with a label containing the name of the manager whose signature it belongs to. Once you’re happy it has been trained with all of the available signatures, you would feed it completely new signatures that were not part of the training data. This is really important! You don’t want to test your model on a data set you have trained it with. It is also just as important to feed your model with images that don’t contain any signatures from those managers whose signatures you wish to identify in the future. Why? Because it's important for it to also learn to correctly say if none of the signatures it was expecting to find has been found.

Providing the training has gone well and it was given a large enough sample of data, the model will correctly differentiate between signatures.

This, of course, is a slightly different, and for sure oversimplistic, example compared to how the MIT model works. But you hopefully understand the concept behind AI models. Train them with enough of the same dataset type, test them with new data.

Back to COVID

The researchers at MIT tested their AI model on thousands of cough and dialog samples — allowing for a very rich training dataset.

Which parameters exactly was this model looking out for? This particular AI model has been around for some time — it was initially intended to detect signs of Alzheimer's through forced-coughs given the effects that the disease is known to have on vocal cords. Vocal cord strength, along with sentiment, lung and respiratory performance, and muscular degradation were effective in detecting the disease.

As the pandemic unfolded, researchers wondered whether the same model could be used to identify COVID-19 infected individuals given the effects the virus has on our muscles (with some individuals reporting temporary muscle impairment).

Researchers found that without modifying the model too much, it was able to correctly identify asymptomatic COVID-19 patients based on the same biomarkers (vocal cord strength, along with sentiment, lung and respiratory performance, and muscular degradation) that were used to detect Alzheimer’s in individuals.

What’s next?

Researchers are currently looking at developing a user-friendly app based on the model. If approved by officials, they hope it could act as a pre-screening app for the virus. We may be looking at a future in which a day out is dependent on results after coughing into your phone!

What a fantastic way to put our technological advancements to use."
"Nat got herself a cup of coffee and took her seat. This was her second meeting since getting the prism from Lyle; last week she’d been planning on announcing that she wasn’t going to be coming back, but she had wound up hardly saying anything at all. So she had had to attend at least one more and say that she was going to take a break from the meetings; people would wonder if she simply stopped.

Dana smiled at the group and said, “Who wants to start us off today?”

Without intending to, Nat found herself speaking, just as Lyle began saying something as well. Both of them stopped.

“You go,” said Nat.

“No, you should go,” said Lyle. “I don’t think you’ve ever started off a meeting before.”

Nat realized that he was right. What had come over her? She opened her mouth, but for once she couldn’t think of a good lie. Eventually, she said, “A guy I work with, I guess you’d call him my supervisor, he was killed recently. Murdered, actually.”

The group was shocked, with assorted “Oh my Gods” being murmured.

“Do you want to tell us about your relationship with him?” asked Dana.

“Yeah,” asked Kevin. “Was he a friend?”

“Kind of,” Nat admitted. “But that’s not why it’s been on my mind. I know this isn’t a grief support group . . . I guess I brought this up because I wanted your take on something.”

“Of course,” said Dana. “Go ahead.”

“I keep thinking about the randomness of this murder. I don’t mean the killer picked him at random. I mean, when he had the gun pointed at my supervisor, he said that some version of him was going to pull the trigger, so why shouldn’t it be him? We’ve all heard that line before, but I never paid any attention to it. But now I’m wondering, are the people who say that actually right?”

“That’s a good question,” said Dana. “I agree that we’ve all heard people make similar claims.” She addressed the group. “Does anyone have any thoughts on that? Do you think that every time someone makes you angry, there’s a branch where you pick up a gun and shoot the guy?”

Zareenah spoke up. “I’ve read that there’s been an increase in crimes of passion since prisms became popular. Not an enormous one, but statistically significant.”

“Yeah,” said Kevin, “which is why the theory can’t be true. The fact that there’s been an increase, even a small one, disproves the theory.”

“How do you figure?” asked Zareenah. “Branches are generated by any quantum event, right? Even before we had prisms, branches were still splitting off constantly; we just didn’t have access to any of them. If it were true that there’s always a branch where you pick up a gun and shoot someone on a whim, then we should have seen the same number of random murders every day before the prism was invented as we saw every day after. The invention of prisms wouldn’t cause more of those murders to line up in this particular branch. So if we’re seeing more people killing one another since prisms became popular, it can’t be because there’s always a branch where you pick up a gun.”

“I follow your reasoning,” said Zareenah, “but then what’s causing the rise in murders?”

Kevin shrugged. “It’s like a suicide fad. People hear about other people doing it, and it gives them ideas.”

Nat thought about it. “That proves that the argument can’t be right, but it doesn’t explain why it’s wrong.”

“If you know the theory’s wrong, why do you need more?”

“I want to know whether my decisions matter!” That came out more emphatically than she intended. Nat took a breath, and then continued. “Forget about murder; that’s not the kind of thing I’m talking about. But when I have a choice to do the right thing or the wrong thing, am I always choosing to do both in different branches? Why should I bother being nice to other people, if every time I’m also being a dick to them?”

There was some discussion among the members for a while, but eventually Nat turned to Dana. “Can you tell me what you think?”

“Sure,” said Dana. She paused to gather her thoughts. “In general, I think your actions are consistent with your character. There might be more than one thing that would be in character for you to do, because your behavior is going to vary depending on your mood, but there are a lot more things that would be utterly out of character. If you’re someone who’s always loved animals, there isn’t a branch where you kick a puppy just because it barked at you. If you’re someone who’s always obeyed the law, there’s no branch where you suddenly rob a convenience store instead of going into work in the morning.”

Kevin said, “What about branches that diverged when you were a baby and your life took a totally different course?”

“I don’t care about that,” said Nat. “I’m asking about branches where I, having lived the life I led, am faced with a choice.”

“Kevin, we can talk about bigger divergences later, if you want,” said Dana.

“No, that’s fine. Proceed.”

“Okay, so let’s imagine you’re in a situation where you have a couple options, and either course of action would be consistent with your character. For example, suppose a cashier has given you too much change, and you can either give it back or just keep it. Suppose you could see yourself doing either of those, depending on the kind of day you’re having. In that case, I’d say it’s entirely possible that there’s a branch where you keep the extra change, as well as a branch where you give it back.”

Nat realized there probably weren’t any branches out there where she gave back the extra change. For as long as she could remember, if she was having a good day, getting extra change would have just made it a better day.

Kevin asked, “So does that mean it doesn’t matter if we act like jerks?”

“It matters to the person in this branch that you’re acting like a jerk to,” said Zareenah.

“But what about globally? Does being a jerk in this branch increase the percentage of jerkish behavior across all branches?”

“I’m not sure about the math,” said Dana. “But I definitely think that your choices matter. Every decision you make contributes to your character and shapes the kind of person you are. If you want to be someone who always gives the extra money back to the cashier, the actions you take now affect whether you’ll become that person.

“The branch where you’re having a bad day and keep the extra change is one that split off in the past; your actions can’t affect it anymore. But if you act compassionately in this branch, that’s still meaningful, because it has an effect on the branches that will split off in the future. The more often you make compassionate choices, the less likely it is that you’ll make selfish choices in the future, even in the branches where you’re having a bad day.”

“That sounds good, but — ” Nat thought about how years of acting a certain way could wear ruts in a person’s brain, so that you would keep slipping into the same habits without trying to. “But it’s not easy,” said Nat.

“I know it’s not,” said Dana. “But the question was, given that we know about other branches, whether making good choices is worth doing. I think it absolutely is. None of us are saints, but we can all try to be better. Each time you do something generous, you’re shaping yourself into someone who’s more likely to be generous next time, and that matters.

“And it’s not just your behavior in this branch that you’re changing: you’re inoculating all the versions of you that split off in the future. By becoming a better person, you’re ensuring that more and more of the branches that split off from this point forward are populated by better versions of you.”

Better versions of Nat. “Thanks,” she said. “That’s what I was looking for.”"
"Suspend and Resume Execution in Coroutines

Note: It’s important to know that Python is phasing out the use of generators as the basis for coroutines. Check the official documentation. However, the concept of coroutines can be easy to understand if we know that yield has the meaning of “giving way.”

In this usage, it’s applicable to yield ’s second meaning: give way. It’s pretty much like the traffic “yield” sign in real life. You temporarily stop your vehicle such that other vehicles with right of the way can go first. Applying this analog to programming, it’s the basic idea of coroutines.

As you can see, the key to coroutines is that the execution is suspended temporarily and it can be resumed later on when it’s applicable. Doesn’t this sound familiar to you? The generator function runs and yields the element (the execution is suspended), and when the next element is requested, it continues to run from where it was suspended (the execution is resumed).

Actually, we can use the yield keyword to implement a coroutine. Let’s see the following code snippet for a simple example:

We first define a coroutine function using the yield keyword. As you can see, the overall structure is similar to the generator function. However, you’ll notice that the yield keyword appears after the assignment operator (i.e. the equals sign).

keyword. As you can see, the overall structure is similar to the function. However, you’ll notice that the keyword appears after the assignment operator (i.e. the equals sign). The assignment is possible because, in addition to the typical next() function that works with a generator, generators also support the send() function.

function that works with a generator, generators also support the function. In the example, we send a string data (i.e. “Here’s the data that your requested.” ) back to the coroutine. Specifically, the data that we sent is set to the variable x , and we’re able to print the x value as expected.

) back to the coroutine. Specifically, the data that we sent is set to the variable , and we’re able to print the value as expected. The StopIteration exception is expected because the generator function has reached the end, which is consistent with the behavior of a generator (as shown in the last section)."
"I am a technology leader, inventor, and entrepreneur in technology, cognitive science, and digital content development fields with four decades of experience. I have been practising and researching technology in academia and industry. My Enterprise Architecture experience was certified at the distinguished level and Design Thinking expertise rated as a thought leader and influencer in the industry.

Apart from researching the technology field as a scholar and documenting my findings, I am a hands-on practitioner and a dedicated learner reading from the body of knowledge voraciously. Not only show guidance to aspiring entrepreneurs as a mentor and development coach, but I am also a humble protege of many great technology leaders in the industry and academia.

I see myself as a technologist working in large corporate organizations within the last four decades. Since I started at a young age with an informatics degree earned by a scholarship in Europe, technology evolved, and every single day added a new meaning to my life.

While technology stresses some of us, it plays a different role in my life. It relaxes and uplifts me. The pieces of technology nourish my brain cells and keep me active and joyful.

I chose technology as a profession to deal with both human and machine intelligence. I attempted to be a polymath and am still working on it. I am educated both as a technologist and scientist.

From the technology point of view, I cover breadth, including an extensive array of technology domains with depth in some specialized areas such as Big Data, Cloud, Cybersecurity, Cognitive Computing, and IoT.

I work as an Enterprise Architect, certified at a distinguished level by the Open Group. From the scientific perspective, I cover machine and human intelligence under the discipline of cognitive science.

I have been consuming insights from the body of knowledge for my profession. Technology knowledge turned out to become a hobby for a long time. During my studies, I started contributing to the body of knowledge in various formats, such as papers, articles, books, and blogs. I published in peer-reviewed scientific journals, delivered keynotes, and am recognized as an inventor with multiple patents.

In this story, I want to introduce and share my recent technology-related articles, presenting them in a single story. This story allows my readers to find my leadership stories easily.

I aim to share my knowledge and experience of outstanding technical and technology leadership.

I am founder and editor in chief of Technology Hits on Medium.

You are welcome to join this sensational publication.

Please enjoy my selected stories touching various aspects of technology.

Ethical Hacking

Topics — #Cybersecurity #Security #Career

In this article, I aim to create awareness on ethical hacking, its purpose, use cases, and a brief introduction to the role of ethical hackers in digital transformation initiatives. My goal is to help security executives and managers choose the best ethical hackers for their business. As a by-product, the information in this article can also guide the aspiring ethical hackers to build their skills and plan their experience.

Power of Design Thinking for Writers

Topics — #Writing #Design #Design Thinking

Design Thinking can be useful for writers too. This is my main premise in this short article. Writers can use Design Thinking principles for quality, simplicity, and agility purposes. Design Thinking can be considered as an innovative problem-solving technique with close interactions of the actual users of the solutions. As writers, one of our fundamental roles is to solve problems for our readers and delight them.

Business Value and Architectural Overview of Robotics Process Automation

Topics — #Artificial Intelligence #Automation #Robotics #Business

In this article, my aim is to briefly introduce what Robotics Process Automation (RPA) is, it’s business value, the major use cases, how it works, how we can enhance it, and my experience on architecting solutions to meet the customer requirements. If you are tired of performing tedious and repetitive tasks and witnessing too many errors upsetting your clients, your business may need the use of Robotics Process Automation empowered by software robots and the capabilities artificial intelligence.

Cognitive Transformation

Topics #Artificial Intelligence #Cognition #Transformation #AI

The purpose of this article is to briefly introduce to you what cognitive computing entails, its current progress in the industry, the value proposition for its necessity, and my personal observations and thoughts on trends and future plans. Most of us in the IT industry most likely heard, read about, or already started working with the cognitive systems for various practical use cases. Would you like to have a quick taste? Then, keep reading! I simplified this complex topic as much as possible in this article.

Business Architecture Framework for Digital Transformation

Topics #Software Engineering #Leadership #Programming

In order to create effective business architecture for complex digital transformation programs, we must follow an established yet innovative framework. A framework is a basic structure underlying a business and technology system. Our framework can start with understanding the organizational structure of the business. This framework can include multiple considerations and business viewpoints. Business organizations are made up of stakeholders with different roles focusing on common goals.

Introduction to IoT Ecosystem

Topics #IoT #Design #Infrastructure #Technology

My aim in this article is to introduce IoT (Internet of Things) ecosystem based on a model that I developed as part of my solution architecture engagements in multiple successful projects in large business organisations. I set the IoT solution context at high level as understanding the solution context can help us delve into architectural and design topics in a methodical way. Every solution requires following a method to leverage prior experience in the field. This also helps us ensure that nothing falls into cracks during the solution lifecycle.

Digital Transformation

Topics #Digital Transformation #Technology

Business organisations are facing enormous challenges to respond to the rapid technological changes and growing demands of consumers for digital products and services globally. There is a constant search to find solutions to the growing business problems. From my experience, the most optimal solution to address these problems is to architect enterprise digital transformation requirements and objectives. My approach, in this article, is to propose an innovative model and framework formulated and described in a 12-step architectural method called DTM.

Architecting Data Lake Solutions

Topics #Data Architect #Data Science #Design

In this article I provide an overview of data lakes, data ponds, , and data swamps from an architectural perspective. My aim is to provide business value, use cases, architectural, design, and implementation, and consumption considerations for data lakes. Use of data lakes became the de-facto standard for Big Data Analytics platforms supporting the Internet of Things (IoT) and Artificial Intelligence (AI) initiatives. Majority of the Big Data and Analytics solutions require consideration of a data lake architectural model. Most of my recent digital transformation initiatives involve data lake solutions to support IoT, Mobility, and AI goals.

Design Thinking for Technical Excellence

Topics #Design Thinking #Design #Leadership #Technology

As an Enterprise Architect, I practice Design Thinking in my daily solution design activities, team interactions, user requirements gathering, and business stakeholder management. Modern business initiatives require adoption of design thinking to their core culture to compete, thrive, and sustain their services with agility. Design thinking can be applied to any discipline.

How To Deal With Big Data For AI?

Topics #AI #Artificial Intelligence #Big Data #Data #Design

In this article, I provide a pragmatic architectural overview of the Big Data management lifecycle covering the distinct phases and essential definitions with critical solution considerations based on my architectural experience acquired from many successful and failed projects. I learnt that architecting Big Data solutions pragmatically with rigour can substantially contribute to the delivery of quality AI and Cognitive solutions especially in enterprise modernisation and digital transformation programs. These transformative programs are empowered by Big Data and AI, integrated with other emerging technology domains e.g. Cloud, Edge, IoT, Blockchain, and Mobile technologies.

Free Online Technology Courses

Topics #Education #Learning #Courses #Technology

In this post, I want to introduce free learning programs by industry experts and industry skill badges offered by the Cognitive Class AI to help you boost your data science, machine learning, cognitive computing, cloud computing, deep learning, Blockchain, programming, open-source tools, and artificial intelligence capabilities online at your convenience in 2020. Continuous learning and applying our knowledge can be powerful and critical success factors for achieving our professional goals. The Cognitive Class AI offers a wide variety of professional learning paths, as free of charge, to learners globally. In this article, I provide you with some prominent learning path samples with links so that you commence achieving your 2020 professional education and career development goals.

A Modern Enterprise Architecture Approach

Topics #Enterprise Architecture #Design #Technology

My purpose to author this book was to provide essential guidance, compelling ideas, and unique ways to Enterprise Architects so that they could successfully perform complex enterprise modernization initiatives transforming them from chaos to coherence. This is not a theory book describing Enterprise Architecture in detail. There are myriad of books on the market and in libraries discussing details of enterprise architecture.

The Blockchain

Topics #Blockchain #Security #Digital Trust

Is the Blockchain panacea to address manipulation problems and illuminate the world with true trust? Can we enable Artificial Intelligence without the Blockchain? Can we start developing the Blockchain application? Let’s explore! In this article, my aim is to introduce what the Blockchain is without hype, its importance with facts, its architecture for autonomous systems, compelling use cases it presents, business value proposition, the technologies and processes behind it, learning and development opportunities, current progress for its deployment, future plans for integrating it to our lives to better understand it and gain more insights about its value proposition.

Digital Twin Solutions

Topics #Digital Twins #IoT #Internet #AI #Big Data

In this article, I want to introduce Digital Twin concept as an emerging technology used in various industries. My aim is to give an overview and shed lights on this emerging technology, architectural construct, and business initiative based on my experience. I explain the digital twin concept using the Cyber-Physical System architecture, business use cases, and value proposition.Digital Twin concept is simple however manifesting them in reality can be complex and difficult to due to combination of underlying technology stacks, tools, and integration requirements. Therefore, a structured and methodical approach to the topic is essential.

Power of the Edge Computing

Topics #Edge Computing #Cloud Computing #Big Data

The purpose of this article is to introduce the Edge Computing as a functional and non-functional extension to the IoT (Internet of Things) and the Cloud Computing service model. The Edge Computing is a powerful extension to the Cloud and the IoT based Big Data Analytics projects. Let me set the context by giving you a brief background on the Cloud and the IoT relationship, then introduce the Edge Computing in this context. We all know the recent trends for the Cloud Computing. The Cloud marked a paradigm shift to Information Technology and Computing field. The IoT Cloud is a critical player in the data ecosystem of large business organisations. The central role the Cloud plays in the IoT is to facilitate data integration of the solution components effectively. IoT solutions are mainly used to provide real-time information to consumers.

Big Data as a Service

Topics #Big Data #BDaaS #Cloud Computing

In this article, I want to introduce the business value, architecture, use cases, deployment, and service model of Big Data as a Service (BDaaS) which is a novel and rapidly growing Cloud-based service. The service is offered to support big data and analytics solutions in a cost-effective manner. This service can be considered as a type of outsourcing model for the deployment of big data and analytics projects rapidly and inexpensively. It is considered inexpensive because we can use the service without upfront investment for costly underlying infrastructure hosting the big data and analytics solutions. Underlying infrastructure costs can include computing servers, storage, network, appliances, racks, hosting facilities, and more importantly infrastructure management and support costs.

Viability Assessment for IoT Solutions

Topics #IoT #Architecture #Design

In this article, I aim to introduce a critical architectural and design tool to validate feasibility of IoT (Internet of Things) solutions across the project lifecycle. I attempt to explain what this tool does, why we need to use it, and how we create, document, analyze, and communicate it to relevant stakeholders of the IoT solutions. Viability Assessment is a critical architectural and design tool to meet business organization’s standards, solution requirements, use cases, project quality criteria, industry compliance, and consumer expectations. This article provides awareness for IoT solution architects to use this tool effectively. Without applying proper viability assessment, IoT projects can most certainly fail! As solution architects and designers, we use a viability assessment work-product, which exists in many proprietary and open-source architectural solution methods. We can either use a viability assessment work-product template from an established open-source method such as TOGAF or our business organization’s proprietary method

IoT, Big Data, & Cloud Computing

Topics #IoT #Big Data #Cloud Computing

IoT (Internet of Things), Big Data Analytics, and Cloud Computing are three distinct technology domains with overlapping use cases. Each technology has its own merits; however, the combination of three creates a synergy and the golden opportunity for businesses to reap the exponential benefits. This combination can create technological magic for innovation when adequately architected, designed, implemented, and operated. In this post, my aim is to provide relationships among IoT, Big Data Analytics, and the Cloud Computing as detailed in one of my recent books titled Architecting Big Data & Analytics Solutions — Integrated with IoT & Cloud.

Big Data Lifecycle Management

Topics #Big Data #Data Platforms #Data Architecture

In this article, I want to provide an architectural overview of the Big Data lifecycle management based on key points extracted from my recent book titled “Architecting Big Data & Analytics Solutions — Integrated with IoT & Cloud”. Understanding this process is essential to architect and design Big Data solutions. Big data is different from traditional data. The main differences come from characteristics such as volume, velocity, variety, veracity, value and overall complexity of data sets in a data ecosystem. Understanding these V words provide useful insights into the nature of Big Data.

I want to share a few technology articles on other platforms.

A Technical Overview of Internet of Things (IoT)

What is Edge Computing?

Robotics Process Automation

Big Data Lifecycle Management

What Is Business Architecture?

Digital Twins

How to Become an Ethical Hacker

Design Thinking for Writers

Find more technology stories at Vocal Media.

Thank you for reading my perspectives.

To collaborate with me, you can join Technology Hits, ILLUMINATION, and ILLUMINATION-Curated. Please send a request via this link. Please point out the name of the preferred publication in your request form."
"What are climate models?

As described by the Geophysical Fluid Dynamics Laboratory, climate models are mathematical representations of the major components of the climate system (the atmosphere, land surface, ocean, and sea ice) and their interactions. Each component of the climate models are described as such:

The atmospheric component simulates clouds and aerosols and plays a large role in the transport of heat and water around the globe.

The land surface component simulates surface characteristics such as vegetation, snow cover, soil water, rivers, and carbon storage.

The ocean component simulates ocean current movement and mixing. It also simulates biogeochemistry because the ocean is the dominant reservoir of heat and carbon in the whole climate system.

The sea ice component modulates solar radiation absorption (the albedo of the planet) and air-sea heat and water exchanges.

All climate models may contain and track different components. However, an article by the University at Albany explains that the atmosphere, hydrosphere, biosphere, cryosphere (ice-covered regions, includes ground in permafrost), and lithosphere (upper layer of Earth’s crust, both continental and oceanic), are the main drivers of climate change and are all generally represented to some extent or another in most climate models to ensure accurate simulations.

How do climate models work?

Climate models divide the Earth’s surface into a three-dimensional grid. Each grid contains equations that represent each component of the model. Therefore, there are atmospheric equations, hydrospheric equations, lithospheric equations, and so on. These equations represent how each component of the climate works in a given area for a given set of variables. These variables represent different conditions and changes to those conditions. Variables may either be calculated by the equation or may be parameterized (hard-coding the variable to save on computing power).

Because there are so many equations and variables at play, a lot of computing power is required to run the system. In other words, the smaller the individual cells of the grid, the more complex and detailed the model, the more computing power is required.

Climate models also include the element of time, which allows models to simulate climatic conditions over a range of time units. These complex mathematical equations which describe how the different components of the atmosphere interact, and within themselves, how the materials within these components interact, give scientists the ability to “look into the future” to see what the climate will be like decades into the future.

Currently, there are three types of climate models: energy balance models, intermediate complexity models, and general circulation models.

Energy balance models forecast climate change as a result of Earth’s energy budget. This type of model looks specifically at how surface temperatures are affected by solar energy and Earth’s natural reflectivity (albedo). Using this type of model, scientists use equations that represent the exchange of energy between Earth and the Sun to better determine how heat is stored on Earth.

Intermediate complexity models share similarities with energy balance models although they include Earth’s biogeochemical components to better simulate large-scale climate scenarios. Scenarios such as glacial melting, changes in ocean currents, and changes in greenhouse gas emission composition of the atmosphere can be simulated to determine how they affect Earth’s climate on the whole.

Finally, global circulation models are the most complex, time-intensive, and computing-heavy models that offer precise predictions about climate change. These models incorporate each component listed above, including equations concerning chemistry, the carbon cycle, and the makeup of isolated areas. This type of model would use a small three-dimensional grid which offers more precision when it comes to understanding how these grids interact with each other.

The difficulty of predicting precipitation changes with climate models.

A chapter discussing the evaluation of climate models published by the Intergovernmental Panel on Climate Change (IPCC) discusses how the modeling of large-scale future precipitation patterns has improved since the days of the AR4 (the IPCC’s fourth climate assessment report conducted in 2007), although the modeling of regional precipitation remains subpar. According to their studies, the IPCC has found that climate models are prone to underestimating the sensitivity of extreme precipitation events to changes in global temperature. This implies that models could underestimate the projected increase in extreme precipitation events in the future.

The chapter goes on to discuss how the simulation of precipitation is one of the most rigorous tests a model can conduct because precipitation characteristics depend heavily on various other processes (such as cloud formation, radiation, atmospheric moisture, and aerosols) that must be parameterized (the processes are defined by a specific variable in the code instead of letting the model calculate the value of the variable itself — saves on computing power when dealing with processes that can occur over much shorter scales than an entire grid cell) in the model. In some cases, where precipitation is concerned, this parameterization of variables must include a range of values to better account for variations in the cell in question, such as topography.

In other words, due to the ranges of error involved in modeling precipitation, there can be varying levels of certainty when predicting the future precipitation of a region. This results in studies having to use several estimations of future precipitation to draw somewhat solid conclusions that may end up being changed a few years down the road. Because of this variation in the regional results of different models (one model could say that a region will become wetter while another model will say that the same region will become drier), a multimodel mean is required to get a rough answer to questions concerning the future of precipitation patterns.

What climate models tell us about future global precipitation patterns.

Without using precipitation-focused climate models, scientists can already infer a couple of things about how precipitation will change in response to a warming climate.

According to a paper published by the National Center for Atmospheric Research, increased planet temperatures would give rise to increased evaporation and, as a result, desertification. Greater evaporation would contribute to the severity and duration of droughts. As the planet warms, the atmosphere’s carrying capacity for water also increases. This, coupled with increased evaporation, means that storms, whether they be thunderstorms, cyclones, or hurricanes, will unleash even greater amounts of precipitation. Vast amounts of precipitation that fall on parched ground will find it hard to be absorbed, resulting in greater amounts of runoff and severe flooding events.

The article goes on to mention how climate models suggest that dry areas will become drier, and wet areas will become wetter, due to a lack of change in the planet’s wind patterns as forecasted for the foreseeable future. This means that while copious amounts of water will be added to the atmosphere, its distribution as precipitation will be uneven.

When it comes to making predictions using climate models, the results generally mirror the phenomena described above. As previously discussed, a model mean is generally required to get the average prediction about future precipitation levels for a given region. However, there are some parts of the world where 9 out of 10 models will agree about future precipitation patterns. Currently, models agree that tropical and high latitude regions will see an increase in precipitation. Models predict that locations such as India, Bangladesh, Myanmar, and northern China will observe greater amounts of precipitation. Regions that will likely see drier conditions include the Mediterranean, southern Africa, and parts of Australia and South America. North America, Europe, and the greater part of northern continental Asia appear to be slightly harder to predict due to seasonal variation, with precipitation prediction amounts being on the higher end during the winter, spring, and fall and predicted precipitation amounts being lower during the summer months.

Regardless of regions becoming wetter or drier, a consensus among climate models suggests that intense precipitation events will increase worldwide by the end of the century. It’s currently projected that a particular intensification of precipitation events will occur in Eurasia and North America."
"Noam Chomsky on Artificial Intelligence| Towards AI

STRINGER Mexico / Reuters

There is a core A.I. question. Can we create cognition? Is there a path by which our machines can possess any true understanding? Our best A.I. efforts are still stored mannikins. They create a lifelike illusion but there is no life behind the blank stares. We have technology that looks like the thought but in actuality, is not. Are we on the right path? If not, what is the path?

Last May, Noam Chomsky spoke on this issue at an MIT symposium. It is highly critical of the current directions in A.I. It’s a terrific read in The Atlantic. In fairness, here is a rebuttal by Google’s director of research, Peter Norvig.

There are two basic schools of thought on A.I. and the road to cognition.

Statistical Big Data — This approach makes use of some intense mathematics to sift through large amounts of data to recognize complex patterns. It can look at a picture and say that it is more likely to be a cat than a goldfish. Combined with cloud infrastructures and lots of computers, you get Siri and Facebook can recognize your friends in that beach photo. This is a simplified summary of how this works.

Bio-Based — This approach says that all earth-based intelligence, from worm to man, has certain organizational and structural features that can be understood and machine implemented. It can only be understood by looking at how earth brains are wired. This understanding will lead to a mechanical recreation of cognition. Let’s use biology and neuroscience to crack this code.

The contrasting arguments look something like this.

Statistical Big Data

Our AI is inspired and works much like nature. It is a parallel set of nodes (neurons) arranged in a network each processing small aspects of the larger whole.

True, it is not a model for cognition yet, but it’s early. Give the technology time to grow and mature. It provides the best path for the emergent behavior of cognition. We are doing things that seemed impossible a decade ago. Watch what happens in the next decade.

Bio-Based

Current commercial neural net technology does not, in anyway model cognition. It does not, cannot and will not. While inspired by natural neural networks, they work nothing like brains (including ours) that are found in nature. The path to cognition lay in structuring these networks to work more closely like real brains. There is a chasm that won’t be crossed.

We are being fooled by mathematical trickery and that Google server that is recognizing pictures of kittens has no idea what a kitten is, and never will. Nature provides excellent, fully functional models for cognition. Let’s get back to nature, do some more science and figure this out.

Where do you stand?"
"Lightning strikes start over 9,000 US wildfires and cause $5 billion in damages every single year — but a new laser system for creating artificial lightning channels hopes to change that. We’re joined by Dr. Jean-Claude Diels, Professor of Physics, Astronomy and Electrical Engineering at the University of New Mexico, who’s spent over three decades studying laser-induced lightning discharges with the goal of safely redirecting this powerful force of nature.

Jean-Claude, welcome! You’ve done a lot of research into atmospheric lightning and the laser-stimulated conduction of air. Let me start by asking if you can give me an overview of your research, and I’d also like to ask what inspired you to focus on this area of study?

Dr. Jean-Claude Diels, University of New Mexico (UNM)

What inspired me? I used to fly small planes, and I got hit by lightning while flying, so I thought I should have my revenge. So far, I haven’t — and I’m up against stiff competition from European researchers with a level of funding in their project that’s considerably larger than what we have here.

To give you an idea, the European Laser Lightning Rod group is planning a field experiment in Switzerland next summer with a grant of about 20 million euro, or around $24 million US dollars.

Their laser alone will cost 2 million euros after it’s fully installed — more than my entire budget of $100 thousand dollars a year. They’re going to be difficult to compete with, but we have slightly different approaches, so I think my team has a shot.

I’ve read that each lightning bolt contains up to 1 billion joules of energy, which leads me ask whether your focus is primarily on lightning safety or if you’re also considering power generation applications?

The focus is on safety, which is a priority these days due to global climate change and the incredible number of forest fires that are occurring. There are also safety applications for protecting airports, launch pads, and maybe even golf courses. You might be surprised to learn that nearly 5% of all deaths by lightning happen on golf courses when electricity conducts down the club.

An artist’s concept of a laser lightning rod on a tall building (EuroNews)

Let’s talk about how this laser lightning rod works. In the past, your research involved using an ultraviolet laser to create a “wire in the sky” that safely directs lightning down an ionized air-channel to ground. Is that still the path you’re pursuing with your research?

Creating an ionization channel in the air was our initial intent, but nobody has been successful in getting electricity to conduct down it over distances. Yes, we can ionize the air with a laser, but the ionization dissipates before we can trigger the lightning. It goes feet — but we need miles of range.

So, how can we overcome this limitation? One solution appears to be using a laser to create ionization, which in turn creates a shockwave. Inside of that shockwave, you’ll have a column of rarified air, and because it’s at low pressure, you’ll have created an easier conduction path for the lightning.

So, in other words, the laser is superheating the air to rarefy it rather than relying on ultraviolet photoionization of a conductive air channel?

Yes, that is exactly the approach that the Europeans are trying to use. Now in my case, I’m trying to use the same rarefaction of air to create a waveguide for a multiple laser system that could still perhaps ionize a column of air.

A schematic of the waveguide approach being used by Diels team.

Would you envision the laser system being placed on tall buildings and other typical lightning targets to safely conduct this charge to ground?

That’s indeed the approach because our laser-based systems are stationary and cannot be moved. Before this, the most successful approach was rocket-triggered lightning discharges. However, what goes up must come down, and you don’t want to have the spectacle of having a spent rocket and spools of discharge wire falling back to Earth on a city.

I think this takes us into lasers: you’ve worked mostly with femtosecond-pulsed UV excimer lasers, right? Is that because these are the most efficient ionizers because each photon in the UV spectrum will ionize 1 air molecule?

Yes. I began my work using excimer-gas lasers, but since then I’ve changed to a type of solid-state laser that’s still in the ultraviolet range. The advantage of ultraviolet lasers isn’t only the single-photon ionization process, though.

Most researchers are using infrared lasers, but each pulse of light in that spectrum makes little filaments of only 1 millijoule, while in the ultraviolet, the filaments created can be up to 1 joule each.

The European experiment using infrared lasers was able to generate about 1 joule of energy altogether by producing thousands of tiny filaments, but it still creates the desired rarefaction in air, so their team is counting on that to direct the lightning.

A 1kHz filamenting laser installed by Clemens Herkommer for the European LLR project. (Twitter)

So, it comes back to rarefying the air then, and not simply ionizing it. It sounds like rarefaction is the key.

Yes. The past experiments we did showed that the delay between the laser and the lightning was considerably more than the time that it takes for the ionization to dissipate, making this approach unsuccessful.

Since single-beam ionization hasn’t worked, there are currently two other approaches being tried. The first involves creating a rarified air-channel, which is what the Europeans are attempting, and then there’s my approach, which combines what I call a “waveguide in the sky” with a multiple-beam laser assembly, which I believe will create a continuous conductor where a single beam would not.

The difference between the ultraviolet and infrared approach is that the infrared laser makes thousands of tiny millijoule-energy filaments, whereas the UV approach essentially produces fewer, larger filaments averaging around 0.2 Joule energy per filament.

After getting the laser in place, we need to project a focused, high-intensity beam over a long distance, perhaps around 10 kilometers. The classical solution is to use a huge aperture lens, but given the size required that isn’t very practical.

However, there are other ways to focus our beam. One of those involves dynamic focusing — or manipulating the laser so that the beam becomes shorter and shorter, and ultimately compresses to a high enough density to create an ionized air channel. Alternatively, there’s also something called an acoustic waveguide, in which the filament creates an acoustic wave that you can use as a waveguide up to one hundred microseconds.

An analysis of the ultrashort pulses used by Diels to generate filamentation.

In the case of the acoustic waveguide, you have to use a very high repetition rate laser to sustain a conductive air channel. My goal is to do this at a frequency of 50 kilohertz using a laser that is frequency triple-compressed, generating a beam that creates filaments of 300 millijoules at 107 picoseconds in the UV.

This is the system we have for our attempt at laser-induced discharge using a laboratory on the roof of our building. Our laser is inside the building, but we can send the beam up from the lab to the roof, and from there we can direct the beam to a rooftop lab, to the top of a nearby mountain, or to any other targets that are accessible in our line of sight.

With our experimental apparatus, we are creating a lightning channel by superheating the air inside the filament, which produces a 40-micron channel within 300 nanoseconds. This channel is essentially an acoustic shockwave that becomes permanent at a repetition-rate over 10 kilohertz, and our apparatus is designed to achieve a rate of 50 kilohertz.

For this test, we built a laser out of parts salvaged from the trashcan at the Air Force research lab in Albuquerque, where we found five laser skeletons that met our specifications. It was a challenging task to build one laser out of five skeletons, but it’s working now and produces an 80 Watt beam at 50 kilohertz that is pumping an amplifier for the filamenting laser.

Before we go further, could you describe for us what you mean by the term filament? Could we describe this as a channel in the air that the beam is traveling down as it propagates?

Yes, filamentation is the propagation of the beam through the air without diffraction. Now, during our early attempts to create a single-beam ionization channel, we found that sending the laser beam to a distance of even one kilometer leads it to spread out too much. For instance, if you start with a one-millimeter diameter beam, it will spread out to a few meters after one kilometer, and the energy is totally dissipated in the air.

However, if you have a high enough beam intensity, the air will begin to act as a lens, and the laser beam will self-focus. At that point, when the air starts to ionize, the electrons tend to counteract that focusing and this creates what is essentially a waveguide in the air.

This initial channel only lasts for a few microseconds, but it creates an acoustic shock wave when it dissipates that continues to act as a waveguide and lasts for a much longer period of time. This is the effect we’re trying to exploit by taking our repetition rate to 50 kilohertz.

So, what you’re describing sounds like a waveguide made out of shockwaves. In this case, is the filament itself a shockwave?

Yes, but keep in mind there are two filaments: the first is self-focusing weakly ionized air, and the second is a shockwave with a longer duration. These two continue in a cycle at 50 kilohertz, one creating the other, and the result is a stable conduction path through the air.

Now I haven’t touched much on your teaching work, but I understand that you’ve graduated over 50 students with specialization in various areas related to high-speed pulsed lasers. Are any of your students following in your footsteps in terms of your work with lightning and atmospheric conductivity?

One of my students went to work for a project in Romania, the Extreme Light Infrastructure, which boasts having the largest laser in the world. His budget just to build the laser was 310 million euros.

The US is trying to compete by giving $10 million to the University of Michigan, but this is kind of ridiculous proportion. Altogether, the total budget for the Extreme Light Infrastructure project is 855 million euros, and we’re not going to effectively compete with that on a budget of $10 million.

The Extreme Light Infrastructure beamlines building, boasting the world’s most powerful laser. (ELI)

So, your student is working on a project in Romania, and then you also mentioned another European team. Can you tell me about them?

Yes. Now, this team has a really huge facility. You see the gigantic building once the French team is making a laser it’s a French & Swiss team. They’re making a laser of that they plan to bring to a peak in the Alps. You can learn more about them on their website, which is online at http://llr-fet.eu/

You’re working with solid-state lasers now, right? Back in the old days, I understand this research was done with massive excimer gas lasers, but if I understand things correctly you’re working with something like a 355-nanometer solid-state laser, right?

Yes. Solid-state lasers can be considerably smaller than gas lasers, and if you want to build a device that you can move to the top of a mountain, it needs to be as small and compact as possible. However, excimer gas-lasers are still used in some cases.

For example, there’s an institute in Moscow is doing similar research with a gigantic excimer laser that takes up a whole building — but the Russians are not applying this research to lightning like my team or the Europeans are.

How will you identify where the lightning is in order to point the laser at it? I understand that the laser creates a conduction path, but in order for conduction to happen you need to know where the lightning originates. Do you have a way to identify where to shoot the laser in the clouds?

Identifying where to send the beam is something that will require a large, multidisciplinary team. This is a resource that the European project has, and they’re well-positioned to design equipment to detect where the lighting is going to start.

However, this is an area that still requires a lot more research before we fully understand it. You can’t simply shoot a laser into the nearest cloud and expect results. Clouds are big, and we don’t know exactly where the field strength will be highest in them. We still need to find better ways to measure that.

About Our Guest

Jean-Claude Diels received a Ph.D. degree in 1973 from the University of Brussels, Belgium, for his research on coherent pulse propagation performed at the University of California, Berkeley, under advisement of Prof. E. L. Hahn.

He is currently Professor of physics and electrical engineering at the University of New Mexico, Albuquerque. He has graduated over 50 students in various areas including coherent interactions, ultrashort pulse generation and diagnostics, nonlinear propagation of intense pulses, and laser-induced discharges.

He co-authored with Wolfgang Rudolph the graduate textbook Ultrashort Laser Pulse Phenomena: Fundamentals, Techniques and Applications on a Femtosecond Time Scale and with Ladan Arrisian the book, Lasers: The Power and Precision of Light, celebrating the 50th anniversary of the laser, and published 5 book chapters.

Dr. Diels has been honored with a fellowship in the Optical Society of America, and is the recipient of the 51st Annual Research Lecturer Award (April 2006), and of the 2006 Engineering Excellence Award of the Optical Society of America. You can learn more about him online at his website."
"Photo by Viktor Talashuk on Unsplash

Python is a convenient language that’s often used for scripting, data science, and web development.

In this article, we’ll look at how to read and write files with Python.

Files and File Paths

A file has a filename to reference the file. It also has a path to locate the file’s location.

The path consists of the folder, they can be nested and they form the path.

Backslash on Windows and Forward Slash on macOS and Linux

In Windows, the path consists of backslashes. In many other operating systems like macOS and Linux, the path consists of forward slashes.

Python’s standard pathlib library knows the difference and can sort them out accordingly. Therefore, we should use it to construct paths so that our program will run everywhere.

For instance, we can import pathlib as follows and create a Path object as follows:

from pathlib import Path

path = Path('foo', 'bar', 'foo.txt')

After running the code, path should be a Path object like the following if we’re running the program above on Linux or macOS:

PosixPath('foo/bar/foo.txt')

If we’re running the code above on Windows, we’ll get a WindowsPath object instead of a PosixPath object.

Using the / Operator to Join Paths

We can use the / operator to join paths. For instance, we can rewrite the path we had into the following code:

from pathlib import Path

path = Path('foo')/'bar'/'foo.txt'

Then we get the same result as before.

This will also work on Windows, macOS, and Linux since Python will sort out the path accordingly.

What we shouldn’t use is the string’s join method because the path separator is different between Windows and other operating systems.

For instance:

path = '/'.join(['foo', 'bar', 'foo.txt'])

isn’t going to work on Windows since the path has forward slash.

The Current Working Directory

We can get the current working directory (CWD), which is the directory the program is running on.

We can change the CWD with the os.chdir function and get the current CWD with the Path.cwd function.

For instance, we can write:

from pathlib import Path

import os

print(Path.cwd())

os.chdir(Path('foo')/'bar')

print(Path.cwd())

Then we get:

/home/runner/AgonizingBasicSpecialist

/home/runner/AgonizingBasicSpecialist/foo/bar

as the output.

As we can see, chdir changed the current working directory, so that we can use manipulate files in directories other than the ones that the program is running in.

The Home Directory

The home directory is the root directory of the profile folder of the user’s user account.

For instance, we can write the following:

from pathlib import Path

path = Path.home()

Then the value of path is something like PosixPath(‘/home/runner’) .

Absolute vs. Relative Paths

An absolute path is a path that always begins with the root folder. A relative is a path that’s relative to the program’s current working directory.

For example, on Windows, C:\Windows is an absolute path. A relative path is something like .\foo\bar . It starts with a dot and foo is inside the current working directory.

Creating New Folders Using the os.makedirs() Function

We can make a new folder with the os.makedirs function.

For instance, we can write:

from pathlib import Path

Path(Path.cwd()/'foo').mkdir()

Then we make a foo directory inside our current working directory.

Photo by Lili Popper on Unsplash

Handling Absolute and Relative Paths

We can check if a path is an absolute path with the is_absolute method.

For instance, we can write:

from pathlib import Path

is_absolute = Path.cwd().is_absolute()

Then we should see is_absolute being True since Path.cwd() returns an absolute path.

We can call os.path.abspath to returns a string with of the absolute path of the path argument that we pass in.

For instance, given that we have the directory foo in the current working directory, we can write:

from pathlib import Path

import os

path = os.path.abspath(Path('./foo'))

to get the absolute path of the foo folder.

We then should get something like:

'/home/runner/AgonizingBasicSpecialist/foo'

as the value of path .

os.path.isabs(path) is a method that returns True is a path that is absolute.

The os.path.relpath(path, start) method will return a string of the relative path from the start path to path .

If start isn’t provided, then the current working directory is used as the start path.

For instance, if we have the folder /foo/bar in our home directory, then we can get the path of ./foo/bar relative to the home directory by writing:

from pathlib import Path

import os

path = os.path.relpath(Path.home(), Path('./foo')/'bar')

Then the path has the value ‘../../..’ .

Conclusion

We can use the path and os modules to construct and manipulate paths.

Also, we can also use the / with Path objects to create a path that works with all operating systems.

We can also path in paths to the Path function to construct paths.

Python also has methods to check for relative and absolute paths and the os module can construct relative paths from 2 absolute paths.

A note from Python In Plain English

We are always interested in helping to promote quality content. If you have an article that you would like to submit to any of our publications, send us an email at submissions@plainenglish.io with your Medium username and we will get you added as a writer."
"How to Set Up Your Own PaaS Within Hours

Five simple steps to quickly set up your very own private PaaS environment using available free open-source technologies

Photo by Fotis Fotopoulos on Unsplash

There are many useful and free open-source software on the Internet, we just need to know where to look.

The PaaS setup that I’m going to recommend works well for private/on-premise setup as well. There’s no coding involved, just some CLI configurations.

Having your own PaaS is useful if you’re running a team of engineers requiring flexibility, privacy, and data ownership. I mainly use it for rapid prototyping purposes and hosting my own suite of web applications with minimal traffic.

I would not recommend this setup for production purposes with high load unless you really know what you’re doing.

I’ll keep the article short and list down the high-level steps as the instructions on the websites are very easy to follow. I’ve done a couple of setups with minimal issues and they are usually done within a few hours.

Assuming you’re starting from scratch with the intent to deploy in the cloud, here are the five simple steps:

Setup a cloud account and rent a VM (e.g. AWS/EC2) Register a domain from a provider (e.g. AWS Route53) Set up the PaaS by following simple steps (CapRover) Create and deploy open-source applications out of the box (Wordpress, Jenkins, GitLab, and many more) Configure backups just in case

Please note that I’m not paid in any way by any of the companies listed in this article. I’m recommending them soley based on positive experiences.

#1: Setup a cloud account and rent a VM

You’ll need a virtual machine to host the applications. There are a variety of cloud providers that you can choose from. Here are some of the popular ones with 1-year free-tier option(s)/credits:

You can click on any of the above links to set up a new free account.

After creating the account, provision a virtual machine—recommended server setup to be Ubuntu 18.04 with at least 1GB RAM.

#2: Register a domain name

Applications will be deployed under a sub-domain, e.g. appone.apps.domain.com, apptwo.apps.domain.com, etc. so it’s essential to have your own domain.

Here are some websites which I’ve used for my domains:

If you’re using a cloud provider, e.g. AWS/Azure, it may be more convenient to register your domain with them to have everything managed centrally.

#3: Set up the PaaS

This section forms the main bulk of the setup. Although there are a number of available open-source PaaS available, e.g. Dokku, CapRover, Flynn, etc. I’ll be using CapRover as an example.

I’ll further break down this portion into four steps:

The steps above should get your PaaS up and running.

#4: Create and deploy applications

Here comes the fun part. After logging into the CapRover dashboard, navigate to the Apps screen via “Apps” at the left sidebar.

Click on the “One-Click Apps/Databases” button."
"Interesting AI/ML Articles On Medium This Week (Dec 5)

Artificial Intelligence and Machine Learning articles that might have flown under your radar.

We are officially in the last month of 2020, and what a crazy ride it has been.

I have to say that amidst lockdown and a year of limited to no social activity, Medium has been, and continues to be a platform where you feel connected to different areas of the world.

Medium is one of my source of connection to the world of Machine Learning. There’s no shortage of interesting AI/ML/DS articles written by machine practitioners and AI enthusiasts.

Below are four articles that have mainly stuck out to me for either the high quality of the information provided or the relevance of the content of the article to ML practitioners. There is definitely an article or two with information that is of value to ML practitioners of different levels.

Happy reading."
"By Katia Moskvitch

With the second wave of COVID-19 gaining strength, researchers are in a race against time to find a treatment or a vaccine.

One international team of scientists from IBM Research and Oxford University is trying to design molecules that would interfere with the molecular machinery of coronavirus, the virus that triggers the disease. If successful, such molecules could become the basis of a new drug to treat or slow COVID-19 infections.

“We are blending techniques such as advanced machine learning, computer modelling and experimental measurements to accelerate the discovery of these new molecules,” says the lead researcher Jason Crain, IBM Research physicist and visiting professor at the University of Oxford. He details his team’s work in a recent COVID-19 High-Performance Computing Consortium’s webinar.

It’s still early days — the team is only four months into the project — but the researchers have already identified several compounds that look promising based on the computational modelling. The scientists now have to test them in a lab, says Crain, and the experiments will take several weeks.

While the ongoing COVID-19-related work is new, Crain’s team has been for many years working on drug discovery, most recently in the area of antibiotic resistance. “We pivoted this earlier work, quickly adapting some of the fundamental methods we had previously developed, to address COVID-19,” Crain says.

The biggest challenge for the team, just like for any other team searching for a new drug to halt the pandemic, is dealing with an immensely vast chemical space within which to identify new functional compounds. To address it, the researchers are combining cutting-edge AI methods with modelling on two supercomputers offered by the COVID-19 HPC Consortium — IBM Summit at Oak Ridge National Laboratory and Frontera at the Texas Advanced Computing Center.

Without these extra computing resources, Crain says, “the throughput of the computational screening stages would have been prohibitively slow.” After all, the computational modelling of a myriad of AI-generated candidate compounds is among the most demanding and time-consuming steps in the discovery pathway.

Computer modelling on Summit and Frontera has allowed the team to screen compounds and reveal their mode of action at the molecular scale, so that they have to synthesize and test experimentally only the most promising ones. “Summit and Frontera allow us to perform calculations of how candidate drug molecules bind to viral proteins much faster than would have been possible otherwise,” says Crain. “The Consortium resources have allowed us to incorporate very HPC-intensive steps into the screening protocol, which is a very powerful approach but rarely possible to do.”

The Consortium has also helped, says Crain, to bring together an international team of experts. “Some of the Oxford team, for example, have extensive experience in the structure of viral proteins, and techniques related to screening of candidate drugs,” he says. “The AI teams at IBM in New York and in the UK have been working on developing new methods that can ‘discover’ functional molecules — which may or may not have been made previously — very efficiently.”

This article first appeared on the COVID-19 HPC Consortium blog"
"Cheap Renewable Energy Has Arrived

The cost of renewable energy is now on par with natural gas.

Photo by Science in HD on Unsplash

At a time when the only climate-related news that gets relayed paints a bleak picture of our future on this planet, it feels good to share a positive story about how far science has come in renewable energy.

According to research conducted by the University of Calgary, the cost of renewable energy has dropped massively, such that it can now compete with natural gas. The age of affordable renewable energy has arrived.

Over the last ten years, wind power costs have dropped by 70% and solar power costs have dropped by 90%. This decline in cost is dramatic and relates to how the levelized cost of wind and solar power is now similar to that of the marginal cost to run an efficient natural gas plant. Levelized cost is a measure that includes the cost of building and running power plants. Therefore, not only have renewable resources begun to match natural gas in price, they have actually become cheaper to operate than existing fossil fuel power plants.

A report conducted a year ago by the Pembina Institute supports these claims. According to the study, renewable energy (including solar, wind, and battery storage) provided the same services to consumers as new fossil fuel power plants even during peak demand scenarios.

This study was conducted in Alberta, Canada, which already has some of the lowest natural gas costs in the world. For renewable energy to compete with an already cheap competitor is incredible, and speaks to the innovation that has occurred in the last ten years in the renewable energy industry.

Furthermore, the report discusses case studies from the United States that discovered how investment in renewable energy portfolios would save consumers over $29 billion a year and would cut greenhouse gas emissions by 100 million tons.

The Pembina report goes on to describe how an analysis conducted by the Rocky Mountain Institute found that when compared to the energy generated at fossil fuel power plants, the cost of renewable energy was $9 to $24 less per megawatt-hour. After reviewing these findings, Pembina announced that these costs would drop further as the technology advanced, something that was proven by the report conducted by the University of Calgary back in November of this year.

The University of Calgary found that the leading causes for the reduction in solar energy costs included “improvements in PV (photovoltaic) module prices, advancements in solar technology and an increase in global average capacity factor (actual energy production relative to potential).” Wind energy saw similar reductions in cost attributed to “lower turbine prices, more efficient operations, and maintenance, and a better global average capacity factor.”

However, despite cost reductions, renewable energy sources still only account for 8.5% of the total global energy supply. This share in the total global energy supply is projected to increase due to increases in renewable energy investment that are outpacing investment in any other energy source by 7.6% per year. Furthermore, thanks to countries like China who are leading the world in solar panel production, the cost of manufacturing solar panels is quickly decreasing thanks to demand.

Some hurdles remain in the way of renewable energy taking center stage though. First, renewable energy is notoriously intermittent, so to many consumers, it seems like lower-quality energy. To mitigate this issue, the University of Calgary suggests implementing improved storage technology in the form of batteries, compressed air, and pumped hydro for times of high energy demand or when the sun isn’t shining and the wind isn’t blowing.

Second, there needs to be the ability to send energy to locations lacking in energy to improve and support a renewable energy-based power grid.

Finally, the University of Calgary report suggests that low-carbon sources of energy will be required to support renewable energy. Having backup energy will be vital when dealing with transitioning to renewable energy and for supplying reliable electricity daily.

While the report isn’t sure whether this supporting energy will come in the form of biomass, new nuclear reactors, or hydrogen-peaking plants, they conclude low-emission sources will need to be reliable in the next decade to support the switch to renewable energy."
"How Unsplash Went From a Tumblr Page To Fully-Fledged Platform

And how we can apply it to our projects.

Photo by Rubén García on Unsplash

You’ve heard of Unsplash, no doubt. The thumbnail of this very article is integrated straight into this post from the service. I searched it within the text editor and picked one I liked. Boom, my article now has a header.

It wasn’t always this way. I couldn’t always type away and select an extremely high-quality photo straight from my text editor to be used completely royalty-free. It took a lot of work in the right places, over a lot of time.

But before putting in all that work to turn this into what it is today, it had to start somewhere. And starting is where most founders screw up. Starting has the most amount of friction. And taking that first step to publish a piece of work might take a year, several developers, and a lot of anxiety.

Unsplash, however, published its first version in 3 hours with $38. There was no umming and ahhing over the design. There was no unnecessary complexity. The first version was so basic that a teenager posting edgy content on the internet used the same technology — it was on Tumblr.

The Original Unsplash on Tumblr | GIF adapted from Source

This is the story of how Unsplash started and how you can use the same principles to launch practically any idea that comes to mind."
"Paige, AI in pathology and genomics

Fundamentally transforming the diagnosis and treatment of cancer

Paige has raised $25M in total. We talked with Leo Grady, its CEO.

How would you describe Paige in a single tweet?

AI in pathology and genomics will fundamentally transform the diagnosis and treatment of cancer.

How did it all start and why?

Paige was founded out of Memorial Sloan Kettering to bring technology that was developed there to doctors and patients worldwide. For over a decade, Thomas Fuchs and his colleagues have developed a new, powerful technology for pathology. This technology can improve cancer diagnostics, driving better patient care at lower cost. Paige is building clinical products from this technology and extending the technology to the development of new biomarkers for the biopharma industry.

What have you achieved so far?

TEAM: In the past year and a half, Paige has built a team with members experienced in AI, entrepreneurship, design and commercialization of clinical software.

PRODUCT: We have achieved FDA breakthrough designation for the first product we plan to launch, a testament to the impact our technology will have in this market.

CUSTOMERS: None yet, as we are working on CE and FDA regulatory clearances. We are working with several biopharma companies.

What do you plan to achieve in the next 2–3 years?

Commercialization of multiple clinical products for pathologists, as well as the development of novel biomarkers that can help speed up and better inform the diagnosis and treatment selection for patients with cancer."
"It’s true that the surrounding technologies are changing not our lives but also our behavior and attitudes. They or, rather, their creators learn not only to help us, but also to manipulate us in new ways. Our over-trust to technologies might be underneath the new notion of “post-truth” where nothing is ever provable.

The recently emerged era of the connected world has produced new concerns about our behavior and psychology in the new settings. The connectivity of the world around us is changing our lives at home and at work. It gives us more opportunities but also shapes our identities.

At Home

Privacy

Does it still exist? We leave hundreds of digital footprints everyday when we visit a webpage or pay with a credit card. In the connected world, where IoT devices will gain the ability to see and record all our moves, all of our notions of privacy will vanish. It is believed, that such constant surveillance violates the social and psychological foundation of humans, destroying our sense of privacy at every dimension.

We are told that this “ubiquitous surveillance” is emerging from our own voluntary choices because it is out choice to buy smart devices and accept cookies and privacy disclosure agreements on websites. Yet, for example, standard smartphone operating systems don’t permit you to choose safeguards to prevent being surveilled, so our choices are prompted by technology, not our free will.

How will this lack of privacy affect us in the long run? The Helsinki Privacy Experiment explored such long-term psychological consequences of surveillance at home when participants had to live with all their activities, both online and offline, tracked. At first, their responded to the constant intrusion of a camera by changing their behavior — sometimes stopping their activities entirely or hiding them. Yet, after several months, 10 out of 12 participants simply got used to it.

Privacy, it turns out, may not be so valuable after all.

Permanent connectivity

From the emergence of laptops, to the newest devices such as smart watches, the market offers a wide range of accessories that cement the idea of “always-on”. Once you had to look at your phone to stay in touch and up to date with everything, now it can be strapped around your wrist. We are so much attached to our “always-on/always-on-us” devices that we now live in two separate worlds: plugged and unplugged.

Availability

We believe that we live in a society where we expect to be able to reach everyone instantly. In this society there is no excuse not to be able to communicate at any time, because you are supposed to be constantly available. With wearable technology, it becomes even easier to do blurring of the lines between “real-life” and “virtual-life”. People stay connected to one another at any time through diverse microdata.

Digital fatigue

Constant connectivity can take a toll on physical and mental health. We can speak of addiction to smart devices, physical problems due to device-prompted positioning of our bodies, and constant stress. As a result of the blurred line between the online and offline life, many users feel the need to declare a “purge”, or a “holiday” from the connected world.

Changing habits

There exists a separate group of smart devices that are aimed at changing our habits and lifestyle, such as fitness bracelets that try to make us eat less and move more. Though giving valuable recommendations, such devices are still notoriously poor in terms of addressing the person’s motivation. When we use a smart device to have a healthier life, we should still enjoy it and not feel oppressed by the machines. Yet, some studies suggest that constant consulting with wearable devices can reduce our content with workouts.

Self-discovery

Smart devices collect all kinds of data about us to provide physical and psychological monitoring. At the same time, not all the information we get can be beneficial for us: with too much knowledge of ourselves and our vices we are forced to self-discovery which may lead to lower self-esteem and even self-rejection.

At Work

Impersonal workplace

With the IoT, teams can collaborate across very long distances saving corporate money and giving more independence and freedom to knowledge employees. Of course, many enterprises choose to reduce their office footprint in favor of “virtual teams.” At the same time, when team collaboration is mediated through software, it can be more difficult to understand the nuance behind communication and identify another’s intentions — potentially leading to disruptive misunderstandings and the sense of alienation

Really incessant work

“Always on” devices have led to an “always on” workplace, where people feel trapped to work on a 24-hour basis through their phones. Many people report discomfort when they do not check their work email at night or over the weekend. This reduces morale and makes off-time less restorative.

Working in a panopticon

The IoT provides immense capacity to track our movements throughout the workday. Potentially, this data can be useful — a good example is tracking trucks to ensure they are on time, but in practice it reduces autonomy and creates a sense of constant observation.

Greater safety and security

Especially in manufacturing and resource extraction, the Internet of Things can mitigate risks that were once believed to be intractable by integrating safety systems into a true network. Whilst the current model relies on the individual vigilance, in future workers will be able to think less actively about their safety and feel more at ease without a constant anticipation of life-threatening events."
"Apple Is Crushing Deep Links That Were Used As Ad-Tracking Devices

iOS 14’s new opt-in ad tracking feature is about to debilitate deep links and cut advertisement revenue streams

Photo by Denis Cherkashin on Unsplash

The fact that Apple has been cracking down on privacy in recent years is no mystery.

With iOS 13, the tech giant gave users more transparency about their location use.

Subsequently, with iOS 14, Apple doubled down with the new approximate location permission. This caused distress among advertisement agencies.

But, still, the loss of revenue wasn’t significant until Apple unveiled a new opt-in ad tracking feature with iOS 14.

For those who are new, iOS 14 has plans to roll out a pop-up that lets the user choose whether they’d want ads to track them across apps and websites or not. If the user decides against ad tracking, the app cannot collect the IDFA, ad identifier.

As digital agencies heavily rely on personalized to determine user trends and ultimately monetize them, this single dialog had upheaved the entire mobile advertising ecosystem during the WWDC 2020.

Unsurprisingly, Facebook cried foul play, and Apple obliged by postponing the feature to give developers and businesses time to cope up with the changes.

Now that the controversial privacy feature is finally rolling out at the start of 2021, Deep Links that were used in the app ads are about to go obsolete."
"JDK 15 is the open-source reference implementation of version 15 of the Java SE Platform, as specified by JSR 390 in the Java Community Process.

JDK 15 reached General Availability on 15 September 2020. Production-ready binaries under the GPL are available from Oracle; binaries from other vendors will follow shortly.

The features and schedule of this release were proposed and tracked via the JEP Process, as amended by the JEP 2.0 proposal. The release was produced using the JDK Release Process (JEP 3).

Removals

Solaris and SPARC Ports

The source code and build support for the Solaris/SPARC, Solaris/x64, and Linux/SPARC ports were removed. These ports were deprecated for removal in JDK 14 with the express intent to remove them in a future release.

Nashorn Javascript Engine

The Nashorn JavaScript script engine and APIs, and the jjs tool were removed. The engine, the APIs, and the tool were deprecated for removal in Java 11 with the express intent to remove them in a future release.

Deprecations

RMI Activation

RMI Activation mechanism is deprecated. RMI Activation is an obsolete part of RMI that has been optional since Java 8. No other part of RMI will be deprecated.

What's new

Helpful NullPointerExceptions

Usability of NullPointerException has been improved. Messages generated by the JVM are describing precisely which variable was null .

Before Java 15

In Java 15

Text Blocks

A text block is a multi-line string literal that avoids the need for most escape sequences, automatically formats the string in a predictable way, and gives the developer control over the format when desired.

Before Java 15

In Java 15

JVM Improvements

ZGC Garbage Collector

The Z Garbage Collector (ZGC) is a scalable low latency garbage collector. ZGC performs all expensive work concurrently, without stopping the execution of application threads for more than 10ms, which makes it suitable for applications that require low latency and/or use a very large heap (multi-terabytes).

At a glance, ZGC is:

Concurrent

Region-based

Compacting

NUMA-aware

Using colored pointers

Using load barriers

At its core, ZGC is a concurrent garbage collector, meaning all heavy lifting work is done while Java threads continue to execute. This greatly limits the impact garbage collection will have on your application’s response time.

Beginning from a Java 15 it is a production-ready GC.

Hidden Classes

Classes that cannot be used directly by the bytecode of other classes. Hidden classes are intended for use by frameworks that generate classes at run time and use them indirectly, via reflection. A hidden class may be defined as a member of an access control nest and may be unloaded independently of other classes.

EdDSA

EdDSA is a modern elliptic curve signature scheme that has several advantages over the existing signature schemes in the JDK.

DatagramSocket API

Replacement for the underlying implementations of the java.net.DatagramSocket and java.net.MulticastSocket APIs with simpler and more modern implementations that are easy to maintain and debug. The new implementations are easy to adapt to work with virtual threads, currently being explored in Project Loom.

Preview Features

A preview feature is a new feature whose design, specification, and implementation are complete, but which is not permanent, which means that the feature may exist in a different form or not at all in future JDK releases.

Pattern Matching

Pattern matching involves testing whether an object has a particular structure, then extracting data from that object if there’s a match. You can already do this with Java; however, pattern matching introduces new language enhancements that enable you to conditionally extract data from objects with code that’s more concise and robust.

More specifically, JDK 15 extends the instanceof operator: you can specify a binding variable; if the result of the instanceof operator is true , then the object being tested is assigned to the binding variable.

Records

Introduced as a preview feature in Java SE 14, record classes help to model plain data aggregates with less ceremony than normal classes. Java SE 15 extends the preview feature with additional capabilities such as local record classes.

A record class declares a sequence of fields, and then the appropriate accessors, constructors, equals , hashCode , and toString methods are created automatically. The fields are final because the class is intended to serve as a simple ""data carrier"". It means that records are Immutable.

Sealed Classes

Sealed classes and interfaces restrict which other classes or interfaces may extend or implement them.

One of the primary purposes of inheritance is code reuse: When you want to create a new class and there is already a class that includes some of the code that you want, you can derive your new class from the existing class. In doing this, you can reuse the fields and methods of the existing class without having to write (and debug) them yourself.

Before Sealed Classes

With Sealed Classes

Conclusion

In this article, we have checked what was added and removed in Java 15. And how all the changes that were delivered with a Java 15 can improve your existing projects."
"In my second year of graduate school I became fixated by the idea of designing a helmet for bike share. I was studying abroad and found riding in Tokyo to be alarming. The enormous public Mamachari bikes topped out at 8 miles per hour, but still I had plenty of white-knuckle encounters with the city’s slow but unyielding traffic. I was unused to cars driving on the left, the roads were narrow and hilly, and my brakes didn’t work too well either. On more than one occasion I scrunched into someone’s hedge so as to avoid a bright pink Kei truck. Only racers wear bike helmets in Japan, but I would have liked one all the same.

So, I started messing around. I wanted something that could go in a briefcase or a vending machine, cost little, be sustainably made, work like a regular helmet, and not look too foolish. Traditional Styrofoam was out of the question, as it doesn’t biodegrade, and most other materials were too heavy, too expensive, or wouldn’t fold up. I had read somewhere about paper honeycomb absorbing blows well. So, I thought to myself, let’s try that.

I knocked together a prototype that looked like a pineapple, put it on the floor and tried to drive my boot through it. It didn’t dent at all. I showed my it to a professor.

“A paper helmet?” she laughed. “What an idea. That won’t possibly work.”

“You just watch me,” I thought. A mildly interesting side project suddenly became a burning passion.

Then as now, telling me something won’t work activates a deeply stubborn part of my nature. If a client or contractor tells me something can’t be done my first impulse is to dig in my heels and insist that nothing is impossible. “I’ll just sort it out myself then” is one of my most commonly used phrases. Often the problem in question turns out to genuinely be insoluble and I have to sheepishly admit it. Other times by luck or advice or sheer will, I’m able to prove everyone wrong and force the absurd idea through — whether the effort was worthwhile is another issue.

Isis is a wrong thinker, this means she practices thinking differently at her studio | Photography Dyson

This strain of stubbornness is common amongst designers and inventors, who regularly have to convince a baffled world that a seemingly poor idea is actually the nucleus for innovation. This is not to say there are “no bad ideas” — a phrase used far too often — because, of course, there are.

Instead, I’d say that approaching any problem from strange or counter-intuitive directions can lead to amazing results. An argument can be made that without ideas that spring from a non-linear thought process our world would look very different. Without wrong thinking we would not have internal combustion engines, saxophones, cheese, super glue, penicillin — the list goes on.

“Industrial design falls squarely between art and science, and a good designer can operate to an extent in both.”

Before I went into industrial design I built bicycle frames for a living. I liked designing the fancy lugs and the colour schemes and tailoring the frame geometry exactly to a customer’s body (I retain the ability to tell someone how tall they are and what size bike they should ride just by looking at them). But after a few years I became bored — a bike is a bike.

Shortly before I quit to go back to school for design, I was at a trade show and found myself admiring a very unusual bicycle indeed. ‘Old Faithful’ looks like what someone would draw if they had heard a bike described but never actually seen one: there is no top tube, only one fork blade, almost no space between the pedals, and the bars and saddle are placed ludicrously close together. It was invented by a Scottish cyclist named Graeme Obree. Already competitive on ordinary bikes, Obree believed that traditional frame design was inefficient and built a bike for himself that solved all the problems, as be he saw them. The strange bar and saddle position upended traditional cycling posture and forces the rider to fold themself up a bit like a downhill skier. It looks ridiculous, and frankly if someone had asked me to build one I would have laughed them straight out of my bicycle shop.

I was lucky enough to try a replica of Old Faithful at that show, and it was unlike any cycling experience I’ve ever had. My arms cramped up in minutes and it felt deeply counter-intuitive to be leading with my face, but once I got going it began to make sense. As I wobbled around the bike testing area, dodging electric trikes and full suspension mountain bikes, I got a tiny echo of how very powerful this machine could be with the right rider in the right conditions.

Sir James Dyson meeting with Isis after her EcoHelmet won the 2016 James Dyson Award | Photography Dyson

Most new bicycle designs tend to build on what came before them, refining rather than rethinking. This one broke most of the accepted rules of bicycle design, and most designers would have found dozens of things wrong with it. It’s worth adding that, riding Old Faithful, Graeme Obree broke the hour record — twice.

Graeme Obree was a cyclist who wanted to go faster. Unburdened by design and engineering conventions he turned wrongthink to his advantage, tinkering in his bike shop and testing the results until he was satisfied.

But Wrongthink can also happen suddenly, as when a thoughtful person does something stupid and learns from it. My father, an auto mechanic, used to tell me about an inventor named Henry ‘Smokey’ Yunick who seemed to do this a lot. One story in particular stuck with me. Smokey wanted to convert an old, empty gasoline barrel into a trash can. He began to cut one end off with a torch, and found himself flat on the ground with his eyebrows blown off. Rather than retreating in embarrassment and nursing his scorched face, Smokey stuck a rag on a pole, lit it on fire, and stuck it in another empty barrel — KABOOM. Smokey thought to himself, an empty barrel exploding had to be useful somehow, and he later developed a long-range sparkplug that ignited from gasoline vapours alone.

I still think of this story when I do something truly ill advised — such as when I forgot to tighten down the fuel lines on a new brazing torch and blew my own eyebrows off (I gained no engineering insights, unfortunately). Smokey Yunick would not have admitted to being wrong thinker of course, nor did he consider himself an engineer, but he held twelve patents and many of his inventions are still used today.

In its most useful format, wrongthink arises organically. When a project is predicated on wrong thinking it becomes an intellectual exercise and can lead to a great deal of labour for no particular reason. Every year, American civil engineering students sink time, blood and energy into the infamous National Concrete Canoe Competition.

The goal is to take a non-floating material and use it to make a functioning boat, which is then raced. Everyone works hard and has a wonderful time, but ultimately what is left is a pile of barely seaworthy boats and a questionable lesson. Yes, they have taken a bad idea and made it work, but why? Would the entrants not learn more by trying to create genuinely useful boats, or at the very least change the material choice year to year?

Humans love to create specific, repeatable processes for the things we do, but it’s not applicable in every field. While STEM projects often follow a predetermined path, many artists approach their work from an abstract place, closer to subconscious inspiration than scientific research. Industrial design falls squarely between art and science, and a good designer can operate to an extent in both.

I am as likely to attack a tricky problem in my studio with math and research as I am to go cycle around the park for a while to see if the solution is actually lurking somewhere out in the real world. Very often I’ll be doing something entirely different, like buying groceries or watching a movie, and the solution to a problem that’s been bugging me for weeks slips into my mind fully solved. This does n’t always work. I occasionally rocket out of bed having dreamed something absolutely world-shatteringly brilliant only to discover the next day that the scratchy drawing in my bedside notepad is something I saw a few days prior or perhaps makes no sense at all.

Not everyone agrees that design exists in a nebulous, thrilling place between process and inspiration. I was introduced to design thinking by an elderly professor who spoke slowly, used innumerable blurry charts and had an unimpeachable resume in UX design. He insisted that every project must be proscribed, from colour coded post-it notes at the ideation phase to scripted talks at the end. Each step was scrutinized, and the slightest deviation was quashed. I thought it was the most deadening, wrongheaded approach to design I had ever seen, and I am still averse to post-it notes years on. In a series of increasingly fractious interactions I insisted that creativity is best nurtured by going outside and doing things, seeing things, taking things apart. The professor insisted my approach was the same as throwing random bits of metal at a wall and expecting them to magically become a train. I’m convinced I only passed his class only because he wanted to make sure he never saw me again.

The EcoHelmet which Isis was told would “never work” | Photography Dyson

That said, I’ve softened towards design thinking as I’ve discovered how valuable it can be — though not necessarily for designing things. I’ve used it to get teams on the same page and, though I hate to admit it, to concentrate my own restless thoughts.

After I finished my studies in Tokyo I relocated to London for a term at Royal College of Art (RCA) and Imperial College London. This time I brought along a bike of my own and a helmet. I spent many lovely hours cycling through the city and being shouted at by motorists while I tried to remember which way one is supposed traverse a roundabout. When I did find myself on a so-called ‘Boris Bike’ I felt the lack of helmets even more keenly, as London’s traffic makes that of my native New York City look sedate. I reanimated the bike helmet project, and was met with more scepticism, though less than before.

“That’s an interesting idea,” one tutor said. I wondered whether he was employing British irony or actually meant it. “Why don’t you see if that actually works?” he advised.

I brought an armload of samples to Imperial’s crash lab and, with the help of the wonderful staff, I was finally able to get some verification. Some samples failed, but a few stopped a blow much like a traditional helmet — the paper crushed under the hammer, distributing the pressure around the dummy head and leaving it unscathed.

“We have a pass!” said the lab’s technician smiling at me.

I remember ricocheting out of lab, bits of crushed paper spilling from my hands. I had been doggedly insisting for months my helmet would work, but privately I had nothing more than a hunch I only half believed myself and an inability to admit I might be wrong. By any metric, a blow stopping, aesthetically pleasing, weather-worthy piece of sports equipment should be made of anything but paper. Now, while it was far from complete, the concept was proved sound and worth pursuing- and no one was more surprised than I — showing that sometimes you have to prove yourself wrong first."
"Claire Merchlinsky. www.nytimes.com. “How to Build Artificial Intelligence We Can Trust.”

On February 10, 1996, world chess champion Garry Kasparov lost his first game to Deep Blue. Kasparov hails from Baku; Deep Blue, from IBM. The face-off took three hours and went down in history as the first instance a computer has beaten a human at the game. But that was 1996, not 2020.

Deep Blue was not the first nor the last in a dynasty of software programs whose family identity seems to be besting humans at human activities. The chess players produced by IBM have grown increasingly adept, but there are now other classes of computers. Computers control our cars as cybernetic chauffeurs. They teach our children. They help those with disabilities, they provide medical diagnoses, and they fish through the farthest reaches of the ethernet for answers to our most recent questions.

The list goes on. Even more perplexing, some of these computers have weaseled their ways into our hearts (despite having no emotional intelligence that we know of).

I’m reminded of one such computer in particular, also from the suite of IBM savants and bearing the eerily humanoid name of Watson. Ring a bell?

In 2011, when I was 11 years old, Watson made history when “he” went on Jeopardy! and faced off against the brightest trivia minds out there, which meant 74-time champion Ken Jennings and past champion Brad Rutter. “I, for one, welcome our new computer overlords,” Jennings wrote on his question card when facing imminent and obvious defeat. With this surrender he made reference to “The Simpsons,” an apt choice from a show that has garnered a reputation for predicting our future before we can do so ourselves.

www.youtube.com. “Watson and the Jeopardy! Challenge.”

“In the end, the humans on Jeopardy! surrendered meekly,” John Markoff wrote for the New York Times after the whole affair had died down. And the humans did, treating the victory as a gimmick or publicity stunt of sorts. Those involved responded to the news with humor, perhaps masking the greater implications of a computer capable of recalling facts from nothing at the drop of a pin. Watson beckoned comparisons to HAL from 2001: A Space Odyssey. This led to unease amongst some; few would welcome a HAL into our world.

www.youtube.com. “2001 — A Space Odyssey.”

Yet most of us, including myself, were excited to see Watson win. I still remember watching the episode with my parents, gathered around a single monitor. We rarely watched Jeopardy! as a family, yet this was history and we knew it. Everyone watched. And everyone wanted Watson to win. The programmers at IBM had designed a program that could elicit in humans a sympathy response, a want to love and nurture this baby in its infancy stages. Watson appeared lovable and clueless in the first few rounds of the game show. To one answer he responded with the question, “What is Toronto????” His multiple question marks were met with laughter. Watson was clumsy, unsure, not afraid to make fun of himself.

This is the real underpin of the artificial intelligence revolution.

Artificial intelligence is here. It’s been here for decades already. But artificial intelligence that we perceive as human, as worthy of emotion, that’s the more complicated part. Watson, with his human name, is the first in a line of robots designed to evoke a human response. Think of Siri and Alexa, similar programs capable of fitting in the palms of our hands whose feminized names and voice acting give them human dimensions (surely, you’ve heard of people jokingly asking their Siris and Alexas dumb questions and waiting for programmed joke responses). When confronted with it, artificial intelligence seems to have us finding the humor of it all or, on the other side of the coin, deeply repulsed.

This repulsion stems not from what the technology is capable of, but from the eery ways in which artificial intelligence attempts to mimic human life. In 2014, Alex Garland’s Ex Machina made its rounds through film circles and the general public and had its critics lauding it as a creepy look at the “disquieting power struggle waged in terms of human weaknesses” (Buzzfeed News). A power struggle. This is the fear, the source of repulsion. This is what scares us: computers who are perhaps “human+,” more feeling, more thinking, more skilled and capable than ourselves.

The female subject of Ex Machina is a beautiful robot named Ava, whose doll-like features and desire for freedom immediately endear her to the film’s protagonist. She ultimately backstabs him in pursuit of her own agenda. This proves that she is capable of passing the Turing test, the ultimate test in the timeline of artificial intelligence that marks at what point a machine is indistinguishable from a human. In the case of Ava and even of the robots we can expect will one day walk the earth, this is not the right test. These robots do not just resemble humans; they are more human. Superior in many regards. Ava is certainly human by this measure. Not only does she have wants and needs, but a higher level of emotional intellect that allows her to manipulate. Yet Ava is more than human, cunning like no other, more intelligent, more artistic.

www.youtube.com. “Meeting Ava in Ex Machina.”

Artificial intelligence, whether real or fictional, from Watson to HAL 9000 to Ava, forces us to question what we will do in response to this new species on our planet.

Perhaps our goal should not be to build the machine that mimics its human counterpart, but the machine that is “human+.” Or maybe even the machine that is “human nothing.” The machine whose aesthetic is a sharp departure from humanoid, the machine that does not attempt to blend in with us or copy our affectations but is beautiful in its own way.

Is our fear of these computers valid? It does not matter. Whether we like it or not, these computers are here to stay. All we can control is our response to them. As Ken Jennings quoting Kent Brockman quoting H.G. Wells said, “I, for one, welcome our new computer overlords.”"
"Pretty good! The full text (not split by speaker) looks like:

Jack. I want to thank you for what he did. Not just for for pulling me back, but for your discretion. You're welcome. Look, I know what you must be thinking. Poor little rich girl. What does she know about misery? No, no, it's not what I was thinking. What I was thinking was what could have happened to this girl to make her think she had no way out. Yes. Well, I you It was everything. It was my whole world and all the people in it. And the inertia of my life plunging ahead in me, powerless to stop it. God, look at that. He would have gone straight to the bottom. 500 invitations have gone out. All of Philadelphia society will be there. And all the while I feel I'm standing in the middle of a crowded room screaming at the top of my lungs and no one even looks up. Do you love him? Pardon me, Love. You're being very rude. You shouldn't be asking me this. Well, it's a simple question. Do you love the guy or not? This is not a suitable conversation. Why can't you just answer the question? This is absurd. you don't know me, and I don't know you. And we are not having this conversation at all. You are rude and uncouth and presumptuous, and I am leaving now. Jack. Mr. Dawson, it's been a pleasure. I sought you out to thank you. And now I have thanked you and insulted. Well, you deserve it, right? Right. I thought you were leaving. I am. You are so annoying. Ha ha. Wait. I don't have to leave. Things is my part of the ship. You leave. Oh, well, well, well, now who's being rude? What is the stupid thing you're carrying around? So, what are you, an artist or something? He's a rather good. Yeah, very good, actually.

The text results are pleasantly accurate. Not something you could copy verbatim into an article, but it’s an amazing head start compared to transcribing a recording yourself from scratch.

Unfortunately, it did struggle a bit with identifying the different speakers correctly. After watching the clip again, I can see how maybe this wasn’t the best choice of scene since they do sound kind of similar, especially in the low-quality format produced by a converted YouTube video.

From personal experience, I can say that with Zoom recordings it’s had greater success thus far in correctly identifying each speaker, though I still haven’t tried it on a meeting with more than two people.

Final Thoughts

Overall, I’m happy with the functionality I was able to produce in a few hours of Thanksgiving weekend tinkering.

In truth, the reason I’m interested in this capability is I’d like to start supplanting my OC articles here on Medium with interviews of other data professionals. With this functionality figured out, I feel ready to reach out to people in my network and see if they are interested in sharing their thoughts and knowledge about working in data.

Oh, and if you are curious for the full transcribe.py script, I’ve included it below for your perusing pleasure!

Note: The code snippet above draws from this excellent AWS tutorial on the Amazon Transcribe service.

For more interesting content like this, follow me on Medium :)"
"Are you using the terminal more than GUI-based operating systems? Or, do you usually develop command-line interface programs using Python? Recently, I found an amazing Python library called “Rich” on GitHub, which already has 15.2k stars by the time I’m writing this article.

It can not only display text in colour and styles in the terminal but also emoji, tables, progress bars, markdown and even syntax highlighted code. If you have read one of my previous articles below:

The “Rich” library can almost do everything that I have introduced in that article, as well as some other awesome stuff.

In this article, I’ll introduce this library with examples to show its capabilities.

Introduction & Installation

Image source: https://github.com/willmcgugan/rich (Official GitHub README.md)

The image above shows the major features that the library can do. You can find the project on GitHub:

The library requires Python 3.6.1 or above to be functional. Installing the library is quite simple because we can use pip .

pip install rich

Rich Printing

Let’s first look at how it can print.

Tags and Emoji

It supports formatting tags and emoji."
"A Guide to Logistic Regression in SAS

Let’s explore a simple way to analyze a model by using SAS.

What is logistic regression?

Logistic regression is a supervised machine learning classification algorithm that is used to predict the probability of a categorical dependent variable. The dependent variable is a binary variable that contains data coded as 1 (yes/true) or 0 (no/false), used as Binary classifier (not in regression). Logistic regression can make use of large numbers of features including continuous and discrete variables and non-linear features. In Logistic Regression, the Sigmoid (aka Logistic) Function is used.

We want a model that predicts probabilities between 0 and 1, that is, S-shaped. There are lots of S-shaped curves. We use the logistic model: Probability = 1 / [1 +exp (B0 + b1X)] or loge[P/(1-P)] = B0 +B1X. The function on left, loge[P/(1-P)], is called the logistic function.

Building a Logistic Model by using SAS Enterprise Guide

I am using Titanic dataset from Kaggle.com which contains a training and test dataset. Here, we will try to predict the classification — Survived or deceased. Our target variable is ‘survived’. I am using SAS Enterprise guide to analyze this dataset. SAS gives a lot of output, so I posted just a relevant portion for our analysis.

Setting the library path and importing the dataset using proc import /* Setting the library path */ %let path=C:\dev\projects\SAS\PRACDATA; libname PRAC “&path”; /* Importing dataset using proc import */ proc import datafile = “C:/dev/projects/sas/pracdata/train.csv” out = PRAC.titanic dbms = CSV; run; Checking the contents of the dataset by using proc contents function /* Checking the contents of the data*/ proc contents data=work.train; run;

We have 12 variables. Our target variable is ‘Survived’ which has 1 and 0. 1 for survived and 0 for not survived. Category variables: Cabin, sex, Pclass. Numeric Variables: Passenger ID, SibSp, Parch, Survived, Age and Fare. Text variable: Ticket and Name

Checking the frequency of the target variable ‘Survived’ by using proc frequency

/* Checking the frequency of the Target Variable Survived */

proc freq data=work.train;

table Survived;

run;

We can clearly see that 342 people were survived and 549 people are not survived. A total number of observations = 891.

Data Visualization

Normally, it is good practice to research with the data by using visualization. I am using proc sgplot to visualize the class, Embark .

title ""Analysis of embarkation locations""; proc sgplot data=prac.titanic;

vbar Embarked / datalabel missing; label Embarked = ""Passenger Embarking Port""; run;

Nothing unusual can be seen in value distributions. Let’s analyze survived the rate with other variables.

title ""Survived vs Gender""; proc sgplot data=prac.titanic pctlevel=group; vbar sex / group=Survived stat=percent missing; label Embarked = ""Passenger Embarking Port""; run;

Here, we see a trend that more females survived than males. People traveled in class 3 died the most. Still, there are many ways to visualize the data. I am not going into detail.

Checking the missing values by using proc means

/* Checking the missing value and Statistics of the dataset */ proc means data=work.train N Nmiss mean std min P1 P5 P10 P25 P50 P75 P90 P95 P99 max; run;

We can see that Age has 177 missing values and no outliers detected.

Checking for categorical variables: title “Frequency tables for categorical variables in the training set”; proc freq data=PRAC.TITANIC nlevels; tables Survived; tables Sex; tables Pclass; tables SibSp; tables Parch; tables Embarked; tables Cabin; run;

We have missing value in Age, Embarked and Cabin. We need to fill all missing age instead of dropping the missing rows. One way to filling by using mean age. However, we can check the average age by passenger class using a box plot. In SAS, we need to sort it out of the class and age variable before making it a box plot.

/* Sorting out the Pclass and Age for creating boxplot */ proc sort data=work.train out=sorted; by Pclass descending Age; run; title ‘Box Plot for Age vs Class’; proc boxplot data=sorted; plot Age*Pclass; run;

We can see the wealthier passengers in the higher classes tend to be older, which makes sense. We’ll use these average age values to impute based on Pclass for Age.

/* Imputing Mean value for the age column */ data work.train2; set work.train; if age=”.” and Pclass = 1 then age = 37; else if age = “.” and Pclass = 2 then age = 29; else if age = “.” and Pclass = 3 then age = 24; run;

I have dropped the cabin variable as I don’t see it is going to impact our model, and filled the missing value in ‘embarked’ using the median. (Selected median due to category variable).

Data Partition

Splitting the dataset into training and validation by using the 70:30 ratio. First, I need to sort out the data using proc sort and splitting by using proc surveyselect .

/* Splitting the dataset into traning and validation using 70:30 ratio */ proc sort data = prac.train6 out = train_sorted; by Survived; run; proc surveyselect data = train_sorted out = train_survey outall samprate = 0.7 seed = 12345; strata Survived; run;

In order to verify the correct data partition, I am generating a frequency table by using proc freq .

/* Generating frequency table */ proc freq data = train_survey; tables Selected*Survived; run;

The Selected variable with the value of 1 will our target observation of the training part. Let us also perform quick set processing in order to leave only the columns that are interesting for us and name variables properly.

Building Model

We filled all our missing values and our dataset is ready for building a model. I am now creating a logistic regression model by using proc logistic . Logistic regression is perfect for building a model for a binary variable. In our case, the target variable is survived.

/* Creating Logistic regression model */ proc logistic data=titanic descending; where part=1; class Embarked Parch Pclass Sex SibSp Survived; model Survived(event=’1') = Age Fare Embarked Parch Pclass Sex SibSp / selection=stepwise expb stb lackfit; output out = temp p=new; store titanic_logistic; run;

One of the beauties in SAS is that for categorical variables in logistic regression, we don’t need to create a dummy variable. Here we are able to declare all of our category variables in a class.

The variable selection algorithm decided that the model will include Age, Pclass and Sex variables.

Good=1 is approximate both for the training set (Part=1) and validation set (Part=0). It amounts to 82.56% and 80.08% for the training and validation sets respectively. It is a stable model, however, if we see the Hosmer test the p-value is less. As per the book, higher, the p-value better the model fit.

If we can see the Concordant pairs, it is 86.6 %. Concordance is used to assess how well scorecards are separating the good and bad accounts in the development sample. The higher is the concordance, the larger is the separation of scores between good and bad accounts.

Testing test dataset

We can test our training model by using test dataset. It’s the same procedure for the importing test dataset in SAS by using Proc import and impute all the missing values.

Testing the test dataset by using our model

/* Testing with our model titanic_logisitic */ proc plm source=titanic_logistic; score data=test1 out=test_scored predicted=p / ilink; run;

Now we export the result into CSV file by using proc export. I separated the survived rate by using probability 0.5 and keeping only PassengerId and Survived variable in the result.

data test_scored; set test_scored; if p > 0.5 then Survived = 1; else Survived = 0; keep PassengerId Survived; run; /* Exporting the output into csv file */ proc export data=test_scored file=”C:/dev/projects/sas/pracdata/Result.csv” replace; run; Note:

Only three variables were used in the model (age, class, and sex) and the result was 74.64 %. This is not a bad model; however, we have a large scope to improve the model by using other variables.

What’s next

In my next article, I will try to use other variables and improve the model. Also, we can apply other algorithms like decision tree, random forest to check the accuracy level. I will try to post in my next blog.

If you find any mistakes or improvement required, please feel free to comment.

Reference:

1) https://support.sas.com/en/documentation.html

2) https://en.wikipedia.org/wiki/Logistic_regression

3) https://www.kaggle.com/c/titanic"
"The difference between classical and quantum computers can be represented by Google’s claim to quantum supremacy in 2019. To achieve this claim of quantum supremacy, one of Google’s 53 qubit quantum computers (called ‘Sycamore’) was able to do a calculation in just over 3 minutes when it would have taken even the world’s most powerful classical computer over 10,000 years to process the same calculation. There has been some dispute over this claim by rival company IBM who says that the calculation would have taken a matter of days and not the tens of thousands of years that Google claimed. But the main idea behind this computing transformation persists. Quantum tech aims to be faster, more efficient, and revolutionary in ways its classical counterparts never will be. And it’s able to do this because of one advantage in particular.

When classical computers store their information it’s either as a 1 or a 0. These 1’s and 0’s we call ‘bits’. But quantum computers can leverage a property known as superposition where their quantum bits (‘qubits’) can be a 1, a 0, both a 1 and a 0 at the same time, or some combination of both numbers. By taking advantage of the quantum trait of superposition these new computers can make great computational strides where classical technology just isn’t enough. The fact that subatomic particles like electrons exist in a superposition of states makes it difficult for classical computers to simulate them. This ability to simulate particles and their quantum properties is how quantum computers succeed where classical computers fail.

A visualization of classical computer bits and the larger range of qubits. Image by Pranith Hengavalli.

Yet despite their computational prowess and their promises for the future, quantum computers are really quite fickle machines. Their chips function only at temperatures close to absolute zero (−459.67 °F or −273.15 °C). The focus for qubits has been, up until now, using small superconducting loops. The oscillations of these loops means that they are systems with 2 possible quantum states, making them a viable basis for a qubit. But attention has recently turned to trapped-ion systems, a technology that formed the basis of quantum circuits before superconducting loops were in use.

Trapped-ion systems use the energy level of ions trapped in electric fields to form the computer’s qubits. The quantum states of the ions last longer than their superconducting counterparts. And where superconducting qubits only interact with other nearby qubits, the ions’ interactions are widespread and allow them to run certain complex calculations more easily. They are, however, slower at these interactions, something which may hurt their ability to correct realtime errors.

Within the world of quantum computing there is a race between materials as much as there is a race between countries."
"Return Of The Caveman. How Technology is Regressing Us

Have you picked your tribe yet? The time is rapidly approaching where you will need one. It’s not safe to be alone out there

We started out on our long evolutionary journey from dimly lit caves occupied by small groups. Groups formed out of necessity. Even at our most primitive level, we grasped the concept of safety in numbers.

It wasn’t purely a safety issue. Larger groups produced better results when foraging or hunting. Better to share the spoils than have no spoils at Better to share and socialize than to dine alone. Many hands, light work. The list goes on but I am sure the point is conceded.

As we developed societally and our numbers increased, so to did the size and complexity of our groups. The strongest was chosen to lead these hierarchies and a gradual unwritten codex of group behavior that was beneficial to the group and its members emerged. Let’s call it the Caveman Codex for harmonious coexistence.

The fact that culturally and geographically divided groups managed to produce almost identical codices points to the validly of these basic principals which would over time form the basis for our first religions.

Religion is not, in my opinion, a spiritual gift to man, but rather man’s attempt to enforce the Caveman Codex, ensuring its unquestioned adherence, thereby ensuring the survival and advancement of the group. History tells us that when we choose to ignore the values enshrined in the Codex, the wheels come off.

Our groups have become legion

Time passed and we evolved in sporadic jumps, from one age to the next, sometimes enjoying enlightened periods and then regressing. Our last and current jump has been spectacular and as far as we know, unparalleled. We are now masters of almost everything we survey, able to bend and mold our environment to better suit humanity.

Our groups are now legion, the occupants numbering sometimes billions and are as varied as they are numerous. We group ourselves by ethnicity, geographically, linguistically, sexually, politically, and even globally.

It is an easy thing for someone to lose themselves in this vast sea of humanity and in response, over the last two centuries, we have a gradual shift to focusing on the individual. On their rights, dreams, and desires, often at the cost of the group. We have not as yet been able to fully quantify that cost, partly due to insufficient data and a lack of historical comparison.

What history does tell us is that the group is essential to our survival and our ability to move forward. Destabilize the group and you destabilize the individual. Lose sight of the Codex and you lose sight of the horizon.

So how does this all tie in with technology? I will attempt to explain this as best I can. I do not have the benefit of datasets, psychologists or social anthropologists to call on, but I do have the power of observation and interpretation. You are free to disagree with my interpretation of the facts, That is the beauty of open discourse.

In 1990, Tim Berners-Lee opened the floodgates

Modern man is raised on a narrative of striving to be the best we can as individuals. A balanced approach to this would be to consider both the interests of the individual and the collective, as they are interdependent. The bias in the late twentieth century has lent heavily towards the individual. Their rights have become paramount in modern culture, trumping all else, irrespective of the cost.

Our groups were still able to partially adapt to address this imbalance as it was limited to a small cross-section of the populace. Then in 1990, Tim Berners-Lee opened the floodgates with the creation of the World Wide Web and the internet as we know it today was unleashed.

In the space of thirty years, our global connectivity and ability to communicate across all spectrums exploded and has changed our world forever. The new digital messiahs were born, spreading their messages of the importance of individualism or self. Almost overnight the world decided that self was the path to fulfillment and that for various reasons the Codex had been misleading them.

They are in some ways correct, as the original Codex has been hijacked and distorted by power bases promoting specific group agendas. These agendas are for the benefit of the few and are propagated at the expense of the group. This is true of almost all modern codices, including religions, political and social codices. Far better then to pursue your own agenda, than to follow a corrupt hierarchy.

The problem with this approach is two-fold. The first is obvious. We need the group to survive. Going it alone is not an option. The second is less glaring, but more important. By abandoning what we correctly identify to be a corrupt hierarchy, we forfeit the ability to effect any change within that group or structure.

It’s analogous to abandoning a ship that’s leaking rather than attempting to repair the leak. If everyone jumps overboard, not only will the ship undoubtedly sink, it’s passengers, now drifting alone in the sea, are also doomed.

Our world is desperately in need of ship fixers, individuals who believe in the principals espoused by the original Codex. People whose individuality and purpose stem from their unshakeable belief in the group. People whose core values and self-belief are not determined by outside factors.

The divisiveness of the online world becomes more apparent by the day. It has spawned new tribalism amongst populations globally, in a digital re-enactment that is troublingly reminiscent of our Stone Age ancestor’s development.

We are aware of the polarizing effect of the internet. We are aware historically of the dangers of tribalism, the feuding and open warfare that arises as a result. Yet we stand by, day by day, watching this catastrophic disassembly of our hierarchies and offer the following excuse. The internet is not reflective of the real world and poses no danger.

We continue to espouse our own individual views whilst vilifying the only structure and Codex that has proved over and over how critical it is to our combined survival as a species. We are lighting our torches and heading off again in search of caves, spurred on by technology.

We need to turn those torches on our groups, expose them internally to the light, cleanse them and restore the Codex to its rightful place at the pinnacle of the hierarchy. Man has proved unable to occupy this position without yielding to the siren call of its power. Perhaps our only salvation lies in re-embracing the Caveman Codex, the greatest gift our ancestors bequeathed us.

The time may already have passed for addressing these issues. Many are now resigned to simply observing humanity become ever more ensnared in the perfect trap of technology. It is a cunning digital web of our own design. One whose very existence we now choose to ignore because it is convenient.

The digital world may not be the real world, but it’s evolution and our eventual submission will not be possible without our complicity. Left unchecked it will divide us and then consume us, individually. Only together can we rediscover our humanity, put aside our differences and embrace our rightful destiny.

There is a place in this future for technology, but it cannot come at the price of our humanity. We need to be the ones firmly in control. At the moment we’re losing this fight."
"Every teacher and professor had their share of strain brought by the ongoing COVID-19 pandemic.

Through listening, adaptation, and planning, we made it to the finish line with quite a bit of energy with my students this semester.

There was one key decision that helped me have a very smooth remote teaching experience: preparing a front-loaded syllabus.

I made that choice consciously, but of course without a prediction about the implications of the then developing pandemic. I aimed to empower students with a broad spectrum of tools early in the semester and use the rest of the time for individual project development.

This strategy ended up working well for remote teaching.

Shaping a classroom through Agency, Adaptation, and Tooling

Over the years, I observed that front-loaded classes and workshops fared much better in many terms. Combined with a longer exploration process, in the end, they turned into a teaching formula for achieving high-yield, high-quality results.

I prepared the syllabus for the 2020 spring semester for the class I taught at MIT’s School of Architecture and Planning with three things in mind:

1- Agency

When you prepare a front-loaded class or a workshop, you shock the students at their freshest state. This enables them to see a lot of material in a short period, but more importantly to pick and chose whatever makes more sense to them.

Yes, no student learns everything we try to teach and it is better if we empower them with the material they are interested in. This kind of agency helps students to become a more integral part of the teaching process.

If the teaching material is comprehensive and the teaching style is generous enough, students can and will have a say in what they learn. If the material is not malleable, they will either have a hard time adapting, or won’t align with the classroom dynamics at all.

2- Adaptation

Teaching is about bi-directional adaptation.

As the students adapt to the class and teaching material, the instructor needs to adapt to the overall drive of the class. Instructors are inclined to expect the students to adapt. But not all of them think about the fact that for a symbiosis to happen, both parties would need to act.

A front-loaded class helps both parties to adjust and make choices early in the semester.

I revealed the goals and mechanics of the subject in the very first class and dived right into the material. With a fast follow-up in the second week, I developed a quick sensation about the choices I made for the amount and delivery of the teaching material. In the meantime, the students came with questions to figure out if the class was the right one for them.

One other benefit of a fast-paced start was to determine the no-adaptation types move out quickly. At MIT the students ‘shop’ for the classes the first week or two. If you show the intensity of the class early in the semester that helps refine the crowd.

In short, if you are open to change things on the course — which in my mind is a must — a front-loaded class helps you plan earlier in the semester.

3- Fluency

If you are teaching a class that includes skill-building components, there are fundamentally two tracks you can follow.

1 — You can plan to move incrementally and distribute skill-building sessions throughout the semester. This would help students to learn and digest skills over longer periods. A slower pace can also help them add skills more easily. This is a low-stress and low-risk choice, but it takes from the time that could be invested in the employment and refinement of skills.

2 — Alternatively, you can front-load the syllabus with skill-building sessions and then observe the tendencies of students in picking things up. This is a riskier move as not all the students would be able to follow the pace of the class. Students may need more support when you want to add on top of something that you have already taught in class. This would put more work-hours on the instructor.

Just looking at the overall picture, the first option appears to be more logical, as the safe bet. Yet the second option, although comes with some risks, increases the chances of break-through achievements — if they are ever to happen in the class.

Moving within a fast-paced setting, students hit a steeper learning curve, but at the same time become accustomed to the tools of the class earlier. This helps them become fluent in the tools they are using quicker.

Especially for an application and making-oriented class, the second option works miraculously better.

How so?

I learned how to teach over 15 years of piecemeal teaching

I have a quite mixed past in teaching and I have nowhere near the experience of a full-time academician. Yet, jumping back and forth between academia and professional practice, or spending time in both simultaneously helped me translate the strategies of teaching across these two domains.

What did I do?

I co-taught design studios. I developed design, geometry, and scripting classes. I happened to initiate and lead an undergraduate design program, somehow early in my career.

Last but not the least, I conducted many workshops in different schools, cultures, and countries. Especially these workshops that ran anywhere from three hours to two weeks taught me a lot about developing syllabuses, even more so than semester-long classes.

The diversity of student’s backgrounds, ages, and interests taught me a lot as well. While in professional practice, I happened to teach people whose age was (more than) double of mine. Later, I found chances to teach fresh out-of-the-high school kids.

Over and over again, I discovered front-loaded scenarios fared better. Starting vertical (and going deep) and then going horizontal (and expanding).

I applied this strategy to my latest teaching adventure.

I asked the students to develop a “design” that had to be re-thought, letting go of its preconceived “parts.” My motivation stemmed from my ever-unfolding inquiry about part-whole relationships that I explained in my latest story:

I deployed the teaching material through 4 tracks: Presence, Function, Quality, and The Whole."
"Introduction To Deep Learning

What Is Deep Learning And How Can I Study It?

This is the first article in this series, and is associated with our Intro to Deep Learning Github repository where you can find practical examples of many deep learning applications and tactics. Read the second article here, the third here, and the fourth here.

Although normally the “prework” comes before the introduction, I’m going to give the 30,000 foot view of the fields of artificial intelligence, machine learning, and deep learning at the top. I have found that this context can really help us understand why the prerequisites seem so broad, and help us study just the essentials. Besides, the history and landscape of artificial intelligence is interesting, so lets dive in!

Artificial Intelligence, Machine Learning, and Deep Learning

Deep learning is a subset of machine learning. Machine learning is a subset of artificial intelligence. Said another way — all deep learning algorithms are machine learning algorithms, but many machine learning algorithms do not use deep learning. As a Venn Diagram, it looks like this:

Deep learning refers specifically to a class of algorithm called a neural network, and technically only to “deep” neural networks (more on that in a second). This first neural network was invented in 1949, but back then they weren’t very useful. In fact, from the 1970’s to the 2010’s traditional forms of AI would consistently outperform neural network based models.

These non-learning types of AI include rule based algorithms (imagine an extremely complex series of if/else blocks); heuristic based AIs such as A* search; constraint satisfaction algorithms like Arc Consistency; tree search algorithms such as minimax (used by the famous Deep Blue chess AI); and more.

There were two things preventing machine learning, and especially deep learning, from being successful. Lack of availability of large datasets and lack of availability of computational power. In 2018 we have exabytes of data, and anyone with an AWS account and a credit card has access to a distributed supercomputer. Because of the new availability of data and computing power, Machine learning — and especially deep learning — has taken the AI world by storm.

You should know that there are other categories of machine learning such as unsupervised learning and reinforcement learning but for the rest of this article, I will be talking about a subset of machine learning called supervised learning.

Supervised learning algorithms work by forcing the machine to repeatedly make predictions. Specifically, we ask it to make predictions about data that we (the humans) already know the correct answer for. This is called “labeled data” — the label is whatever we want the machine to predict.

Here’s an example: let’s say we wanted to build an algorithm to predict if someone will default on their mortgage. We would need a bunch of examples of people who did and did not default on their mortgages. We will take the relevant data about these people; feed them into the machine learning algorithm; ask it to make a prediction about each person; and after it guesses we tell the machine what the right answer actually was. Based on how right or wrong it was the machine learning algorithm changes how it makes predictions.

We repeat this process many many times, and through the miracle of mathematics, our machine’s predictions get better. The predictions get better relatively slowly though, which is why we need so much data to train these algorithms.

Machine learning algorithms such as linear regression, support vector machines, and decision trees all “learn” in different ways, but fundamentally they all apply this same process: make a prediction, receive a correction, and adjust the prediction mechanism based on the correction. At a high level, it’s quite similar to how a human learns.

Recall that deep learning is a subset of machine learning which focuses on a specific category of machine learning algorithms called neural networks. Neural networks were originally inspired by the way human brains work — individual “neurons” receive “signals” from other neurons and in turn send “signals” to other “neurons”. Each neuron transforms the incoming “signals” in some way, and eventually an output signal is produced. If everything went well that signal represents a correct prediction!

This is a helpful mental model, but computers are not biological brains. They do not have neurons, or synapses, or any of the other biological mechanisms that make brains work. Because the biological model breaks down, researchers and scientists instead use graph theory to model neural networks — instead of describing neural networks as “artificial brains”, they describe them as complex graphs with powerful properties.

Viewed through the lens of graph theory a neural network is a series of layers of connected nodes; each node represents a “neuron” and each connection represents a “synapse”.

Different kinds of nets have different kinds of connections. The simplest form of deep learning is a deep neural network. A deep neural network is a graph with a series of fully connected layers. Every node in a particular layer has an edge to every node in the next layer; each of these edges is given a different weight. The whole series of layers is the “brain”. It turns out, if the weights on all these edges are set just right these graphs can do some incredible “thinking”.

Ultimately, the Deep Learning Course will be about how to construct different versions of these graphs; tune the connection weights until the system works; and try to make sure our machine does what we think it’s doing. The mechanics that make Deep Learning work, such as gradient descent and backpropagation, combine a lot of ideas from different mathematical disciplines. In order to really understand neural networks we need some math background.

Background Knowledge — A Little Bit Of Everything

Given how easy to use libraries like PyTorch and TensorFlow are, it’s really tempting to say, “you don’t need the math that much.” But after doing the required reading for the two classes, I’m glad I have some previous math experience. A subset of topics from linear algebra, calculus, probability, statistics, and graph theory have already come up.

Getting this knowledge at university would entail taking roughly 5 courses. Calculus 1, 2 and 3; linear algebra; and computer science 101. Luckily, you don’t need each of those fields in their entirety. Based on what I’ve seen so far, this is what I would recommend studying if you want to get into neural networks yourself:

From linear algebra, you need to know the dot product, matrix multiplication (especially the rules for multiplying matrices with different sizes), and transposes. You don’t have to be able to do these things quickly by hand, but you should be comfortable enough to do small examples on a whiteboard or paper. You should also feel comfortable working with “multidimensional spaces” — deep learning uses a lot of many dimensional vectors.

I love 3Blue1Brown’s Essence of Linear Algebra for a refresher or an introduction into linear algebra. Additionally, compute a few dot products and matrix multiplications by hand (with small vector/matrix sizes). Although we use graph theory to model neural networks these graphs are represented in the computer by matrices and vectors for efficiency reasons. You should be comfortable both thinking about and programming with vectors and matrices.

From calculus you need to know the derivative, and you ideally should know it pretty well. Neural networks involve simple derivatives, the chain rule, partial derivatives, and the gradient. The derivative is used by neural nets to solve optimization problems, so you should understand how the derivative can be used to find the “direction of greatest increase”. A good intuition is probably enough, but if you solve a couple simple optimization problems using the derivative, you’ll be happy you did. 3Blue1Brown also has an Essence of Calculus series, which is lovely as a more holistic review of calculus.

Gradient descent and backpropagation both make heavy use of derivatives to fine tune the networks during training. You don’t have to know how to solve big complex derivatives with compounding chain and product rules, but having a feel for partial derivatives with simple equations helps a lot.

From probability and statistics, you should know about common distributions, the idea of metrics, accuracy vs precision, and hypothesis testing. By far the most common applications of neural networks are to make predictions or judgements of some kind. Is this a picture of a dog? Will it rain tomorrow? Should I show Tyler this advertisement, or that one? Statistics and probability will help us assess the accuracy and usefulness of these systems.

It’s worth noting that the statistics appear more on the applied side; the graph theory, calculus, and linear algebra all appear on the implementation side. I think it’s best to understand both, but if you’re only going to be using a library like TensorFlow and are not interested in implementing these algorithms yourself — it might be wise to focus on the statistics more than the calculus & linear algebra.

Finally, the graph theory. Honestly, if you can define the terms “vertex”, “edge” and “edge weight” you’ve probably got enough graph theory under your belt. Even this “Gentle Introduction” has more information than you need.

In the next article in this series I’ll be examining Deep Neural Networks and how they are constructed. See you then!

Part 2: Deep Neural Networks as Computational Graphs

Part 3: Classifying MNIST Digits With Different Neural Network Architectures"
"2. Hike like you’re in Ahaggar National Park

When you hike through Ahaggar National Park in Algeria, each step builds off the last. Great views will come into your vision. And, if you’re in the right place, you’ll catch a sunset behind Mount Tahat.

Your career should resemble such a wonderful hike. Or, perhaps your career is a wonderful hike if you’re lucky enough to be a pro hiker. Anyway, everything you’re doing should be working towards a beautiful peak or finish. If you ask Kamel about his career, you get this feeling.

Having started with Djezzy in 2006, Kamel first worked as a call center advisor, learning a lot from his direct communications with customers. Then, he took a position as a copywriter for the company before working in advertising management. This eventually led to his role as a Brand & Communications Director today.

Kamel and the team

“Each position has been a step to the next one,” notes Kamel. “I completely understand the vision Djezzy has and I’m getting better and better at fulfilling that vision for this awesome company.”

For Kamel, success hinges on continuous improvement and a willingness to learn and try new things. If you ask him what advice he would give his childhood self, he echoes similar sentiments.

“I would tell myself: Never give up. Learn from your mistakes. Always work on getting better. And be open and listen to others”

Read Kamel’s last tidbit of advice to his childhood self (about listening to others). It brings us to this point: If you are to be a champion, you can’t do it all alone (it’s best to hike through the Ahaggar Mountains with partners, too).

“Reaching your highest potential goes beyond just improving your skills and self-confidence,” asserts Kamel. “You must understand teamwork and know how to work with others to get to where you want to be. That’s why at Djezzy I’m constantly working on building team spirit.”

The new recruitment space at Djezzy

3. Airport signs say “welcome” in all sorts of languages

You’ve probably seen those signs in the international airport that say “welcome” in many of the world’s languages. Kudos to you if you can read all or most of them.

Anyway, the lesson here is that you have to speak to your customers in their language.

Kamel recognized the importance of this while managing copywriting campaigns for Djezzy. That’s because there is a unique mix of languages and dialects used in Algeria.

Throughout Algeria, Berber languages and dialects are spoken. In the north and northeast, you’ll commonly hear Kabyle; in the east, Chaoui is used; in the Algerian Sahara, it’s Tuareg. Arabic is a standard language in Algeria and is the mother tongue for many in the country, which means it’s heard everywhere. French is also used in commerce, government, and education, and is frequently encountered in major cities. In the west, there is even an influence from Spanish.

In many regions, Algerians blend words from other languages with their native tongue, creating their own dialect. This is part of what makes Algeria so magical. But it also makes work as a copywriter more complex.

“We had to use language people were using in their daily lives — because Djezzy is all about connecting to the person,” says Kamel. “We found ways to write customized copy for each region, while maintaining our unifying campaign theme.”

Kamel and the team

To make sure such copywriting campaigns work, Kamel believes in testing and looking at the data. Because you have to speak to the customer on their level.

“For instance, we found out from our Facebook interactions that many users wanted Arabic subtitles for our French press conferences,” describes Kamel. “So now we’re doing that for all our videos.”

So, if you want to succeed with your customers, find the right language and words to reach them. As you make your pitch to them, listen to your customer’s reactions — and adjust accordingly.

4. Cook something special for your customers (literally and figuratively)

“Djezzy wants to connect to Algerians’ lives,” exclaims Kamel. “We have a show called Djezzy CLYC on our YouTube channel. This is where we talk about new product launches, new offers, and more. We’ve also sponsored MasterChef Algeria — a show all about making great food!”

Like Kamel has stated, culinary art is one of the pillars of Djezzy’s brand development. Djezzy sponsors MasterChef so that the company can align its brand with Algerian culture. That means being there when it comes time to discover culinary arts (we want to be there for the delicious meal, too!).

Before you go check out the MasterChef show, which will make you super hungry by the way, understand the deeper meaning here. It’s that you must prepare something special for your customers.

To satisfy your customers, you must give them something they would like. After all, you wouldn’t cook a dish for guests if they hate that dish. This is why Kamel is always looking at the data: He wants to know what his customers like.

The idea here is simple: You cook the flavors your guests prefer (i.e. your customers). If you ignore their tastes, you risk a meal going to waste. Make a dish they like, and you’ll be a champion.

5. Some experts are saying even robots need sleep

Even if you’re actually a robot, you still need sleep. You can’t go at full blast all the time; you’ll burn out before becoming a champion. In fact, some experts are saying that robots do need sleep to function better. So don’t feel bad for taking a break.

“I love spending time with my family and having meals with them,” says Kamel. “I like to play video games, like FIFA. I like to play football with friends and also with my son!”

Again, we’re back at football. We wonder how good robots will be at football. Anyway, you get the point here. If you want to be a champion, take time for rest and fun along the journey. It’s the only way you can reach the pinnacle of excellence.

Keep on pushing towards the top

Kamel has given you his tips on how to be a champion. Now, your mission is to just do it. As you go on your journey, remember the value of persistence — a value Kamel’s team members say he’s instilled in them. Persistence is what will get you to that beautiful destination (well, that and culinary art and football)."
"However, the path very often turns to be rough. There’s a lot of obstacles to deal with to make something just good. In between 18th and 19th centuries when the Industrial Revolution took place, and technological progress allowed to manufacture faster and in a high volume, the Three Pillars of Design were defined:

Voice of Business. How do you as a designer respond to a company’s business model to make a product relevant?

How do you as a designer respond to a company’s business model to make a product relevant? Voice of Technology. How do you design to create something within a budget and yet well made?

How do you design to create something within a budget and yet well made? Voice of Customer. How do you answer people’s needs? How do you solve people’s problems? How effectively do you solve those problems? How do you design to embrace individualities? How do you design for people to enrich their lives?

That looks like a challenge. And it is. You see, I think we as designers need a tremendous amount of self-discipline to be good in what we do. Answering those questions above, facing them in the most successful way, requires a thing that I find the most precious quality of us designers, a thing that we constantly tend to learn — humility.

Humility — ענוותנות, Hebrew “anavah” — a sign of strength and purpose, not weakness.

We often mislead humility with weakness, whereas it’s a strength. We can be highly skilled specialists, using the most advanced tools, but a result of our work comes from a process, from our relationship with whom we collaborate. Way before a product meets customers, we designers meet business stakeholders and engineers, people responsible for revenue and the fact that our ideas can come to life. That’s the place for us designers serving others with humility which is represented by:

ability to listening

not stealing somebody’s air

being aware that sometimes we may be wrong

There’s a beauty in the service of being responsible for giving a form to business intention and implementing solutions. By being designers we tend to brilliantly communicate however by asking questions. So much can be unheard when we’re not attentively listening to barely formed, fragile thoughts that have got a chance to be a foundation for a fabulous solution. During ideation and execution, any contribution is a new point of view, that can pivot a project in a way more beneficial to a final result, therefore we designers are expected to create a convenient environment for others to collaborate. And finally, constant learning means being ready to fail and seek an outcome from it to bring the value to our teams, to a project, and therefore to customers.

Sadly, very often customers can be victims of design, instead of being beneficiaries of it. Unfortunately, we stick at looking for a balance between a look and function. Obviously, a product looking good but having a very poorly solved functionality can be ugly. In the same time, we often try to believe that the thing that predominantly matters is functionality. But we very rarely ask the question:

“What do we want people to feel?”

You see, designing is three dimensional. I believe that we designers do what we do in the service of aesthetics, function, and feelings that someone, somewhere had using a product we designed. Do you remember the smell of just unboxed product that you had waited for a while? Do you remember the joy of the unboxing? Do you remember the texture of that shiny new product? Do you remember the sound of taking off a foil? We forget what a company did and said to convince us to buy their product, but we never forget how they made us feel. Those are outcomes of unreasonable intentions that are far from being just aesthetically and functionally correct. These are outcomes of the amount of care that was invested in every meticulously considered detail. The amount of care invested in expertise and research to find a solution, creative direction, material, manufacturing process that let translate reckless ideas into something unremarkable, something that will definitely be discerned but barely articulated.

Voice of Environment.

There’s one other thing that wasn’t mentioned in the Three Pillars and is ever so important. We designers, we also serve the environment. We more than anyone have a civic and moral responsibility in creating solutions, choosing materials, tools, and processes that will serve not only business, technology, and customers but also our common, the only one home we have — the planet. Our decisions are made within minutes but often impact years to come. An ignorance can turn into evil. But yet proactivity can avoid disasters and resolve problems. With deliberate solutions, we can address the quality of life of future generations.

Being a designer requires maturity, a self-awareness. We’ve got a power to build, to enrich, to embrace. We carry the responsibility of connecting creative voices. We can make trustworthy room for ideas incubation. We can shape the future.

To serve others is delightful, isn’t it?"
"How storage works in Prometheus, and why is this important?

Learn the bases that make Prometheus, so a great solution to monitor your workloads and use it for your own benefit.

Photo by Vincent Botta on Unsplash

Prometheus is one of the key systems in nowadays cloud architectures. The second graduated project from the Cloud Native Computing Foundation (CNCF) after Kubernetes itself, and it is the monitoring solution for excellence in most of the workloads running on Kubernetes.

If you already have used Prometheus some time, you know that it relies on a Time series database, and it is one of the key elements. Based on their own words from the Prometheus official page:

Every time series is uniquely identified by its metric name and optional key-value pairs called labels, and that series is similar to the tables in a relational model. And inside each of those series, we have the samples that are similar to the tuples. And each of the samples contains a float value and a milliseconds-precision timestamp.

Default on-disk approach

By default, Prometheus uses a local-storage approach storing all those samples on disk. This data is distributed on different files and folders to group different chunks of data.

So, we ve folders to create those groups, and by default, they are a two-hour block and can contain one or more files depends on the amount of data ingested in that period of time as each folder contains all the samples for that specific timeline.

Additionally, each folder also has some kind of metadata files that help locate each of the data files' metrics.

A file is persistent in a complete manner when the block is over, and before that, it keeps in memory and uses a write-ahead log technical to recover the data in case of a crash of the Prometheus server.

So, at a high-level view, the directory structure of a Prometheus server’s data directory will look something like this:

Remote Storage Integration

Default on-disk storage is good and has some limitations in terms of scalability and durability, even considering the performance improvement of the latest version of the TSDB. So, if we’d like to explore other options to store this data, Prometheus provides a way to integrate with remote storage locations.

It provides an API that allows writing samples that are being ingested into a remote URL and, at the same time, be able to read back sample data for that remote URL as shown in the picture below:

As always in anything related to Prometheus, the number of adapters created using this pattern is huge, and it can be seen in the following link in detail:

Summary

Knowing how the storage of Prometheus works is critical to understand how we can optimize their usage to improve the performance of our monitoring solution and provide a cost-efficient deployment.

In the following posts, we’re going to cover how we can optimize the usage of this storage layer, making sure that only the metrics and sample that are important to use are being stored and also how to analyze which metrics are the ones using most of the time-series database to be able to take good decision about which metrics should be dropped and which ones should be kept.

So, stay tuned for the next post regarding how we can have a better life with Prometheus and not die in the attempt."
"“ Artificial Intelligence has been brain-dead since the 1970s.” This rather ostentatious remark made by Marvin Minsky co-founder of the world-famous MIT Artificial Intelligence Laboratory was referring to the fact that researchers have been primarily concerned on small facets of machine intelligence as opposed to looking at the problem as a whole. This article examines the contemporary issues of artificial intelligence (AI) looking at the current status of the AI field together with potent arguments provided by leading experts to illustrate whether AI is an impossible concept to obtain.

Because of the scope and ambition, artificial intelligence defies simple definition. Initially, AI was defined as “the science of making machines do things that would require intelligence if done by men”. This somewhat meaningless definition shows how AI is still a young discipline and similar early definitions have been shaped by technological and theoretical progress made in the subject. So for the time being, a good general definition that illustrates the future challenges in the AI field was made by the American Association for Artificial Intelligence (AAAI) clarifying that AI is the “scientific understanding of the mechanisms underlying thought and intelligent behaviour and their embodiment in machines”.

The term “artificial intelligence” was first coined by John McCarthy at a Conference at Dartmouth College, New Hampshire, in 1956, but the concept of machine intelligence is in fact much older. In ancient Greek mythology the smith-god, Hephaestus, is credited with making Talos, a “bull-headed” bronze man who guarded Crete for King Minos by patrolling the island terrifying off impostors. Similarly, in the 13th century, mechanical talking heads were said to have been created to scare intruders, with Albert the Great and Roger Bacon reputedly among the owners. However, it is only in the last 50 years that AI has really begun to pervade popular culture. Our fascination with “thinking machines” is obvious, but has been wrongfully distorted by the science-fiction connotations seen in literature, film and television.

In reality, the AI field is far from creating the sentient beings seen in the media, yet this does not imply that successful progress has not been made. AI has been a rich branch of research for 50 years and many famed theorists have contributed to the field, but one computer pioneer that has shared his thoughts at the beginning and still remains timely in both his assessment and arguments is British mathematician Alan Turing. In the 1950s Turing published a paper called Computing Machinery and Intelligence in which he proposed an empirical test that identifies an intelligent behaviour “when there is no discernible difference between the conversation generated by the machine and that of an intelligent person.” The Turing test measures the performance of an allegedly intelligent machine against that of a human being and is arguably one of the best evaluation experiments at this present time. The Turing test, also referred to as the “imitation game” is carried out by having a knowledgeable human interrogator engage in a natural language conversation with two other participants, one a human the other the “intelligent” machine communicating entirely with textual messages. If the judge cannot reliably identify which is which, it is said that the machine has passed and is therefore intelligent. Although the test has a number of justifiable criticisms such as not being able to test perceptual skills or manual dexterity it is a great accomplishment that the machine can converse like a human and can cause a human to subjectively evaluate it as humanly intelligent by conversation alone.

Many theorists have disputed the Turing Test as an acceptable means of proving artificial intelligence, an argument posed by Professor Jefferson Lister states, “not until a machine can write a sonnet or compose a concerto because of thoughts and emotions felt, and not by the chance fall of symbols, could we agree that machine equals brain”. Turing replied by saying “that we have no way of knowing that any individual other than ourselves experiences emotions and that therefore we should accept the test.” However, Lister did have a valid point to make, developing an artificial consciousness. Intelligent machines already exist that are autonomous; they can learn, communicate and teach each other, but creating an artificial intuition, a consciousness, “is the holy grail of artificial intelligence.” When modelling AI on the human mind many illogical paradoxes surface and you begin to see how the complexity of the brain has been underestimated and why simulating it has not been as straightforward as experts believed in the 1950s. The problem with human beings is that they are not algorithmic creatures; they prefer to use heuristic shortcuts and analogies to situations well known. However, this is a psychological implication, “it is not that people are smarter then explicit algorithms, but that they are sloppy and yet do well in most cases.”

The phenomenon of consciousness has caught the attention of many Philosophers and Scientists throughout history and innumerable papers and books have been published devoted to the subject. However, no other biological singularity has remained so resistant to scientific evidence and “persistently ensnarled in fundamental philosophical and semantic tangles.” Under ordinary circumstances, we have little difficulty in determining when other people lose or regain consciousness and as long as we avoid describing it, the phenomenon remains intuitively clear. Most Computer Scientists believe that the consciousness was an evolutionary “add-on” and can, therefore, be algorithmically modelled. Yet many recent claims oppose this theory. Sir Roger Penrose, an English mathematical physicist, argues that the rational processes of the human mind are not completely algorithmic and thus transcends computation and Professor Stuart Hameroff’s proposal that consciousness emerges as a macroscopic quantum state from a critical level of coherence of quantum level events in and around cytoskeletal microtubules within neurons. Although these are all theories with not much or no empirical evidence, it is still important to consider each of them because it is vital that we understand the human mind before we can duplicate it.

Another key problem with duplicating the human mind is how to incorporate the various transitional states of consciousness such as REM sleep, hypnosis, drug influence and some psychopathological states within a new paradigm. If these states are removed from the design due to their complexity or irrelevancy in a computer then it should be pointed out that perhaps consciousness cannot be artificially imitated because these altered states have a biophysical significance for the functionality of the mind.

If consciousness is not algorithmic, then how is it created? Obviously, we do not know. Scientists who are interested in subjective awareness study the objective facts of neurology and behaviour and have shed new light on how our nervous system processes and discriminates among stimuli. But although such sensory mechanisms are necessary for consciousness, it does not help to unlock the secrets of the cognitive mind as we can perceive things and respond to them without being aware of them. A prime example of this is sleepwalking. When sleepwalking occurs (Sleepwalking comprises approximately 25 per cent of all children and 7 per cent of adults) many of the victims carry out dangerous or stupid tasks, yet some individuals carry out complicated, distinctively human-like tasks, such as driving a car. One may dispute whether sleepwalkers are really unconscious or not, but if it is, in fact, true that the individuals have no awareness or recollection of what happened during their sleepwalking episode, then perhaps here is the key to the cognitive mind. Sleepwalking suggests at least two general behavioural deficiencies associated with the absence of consciousness in humans. The first is a deficiency in social skills. Sleepwalkers typically ignore the people they encounter, and the “rare interactions that occur are perfunctory and clumsy, or even violent.” The other major deficit in sleepwalking behaviour is linguistics. Most sleepwalkers respond to verbal stimuli with only grunts or monosyllables or make no response at all. These two apparent deficiencies may be significant. Sleepwalkers use of protolanguage; short, grammar-free utterances with referential meaning but lack syntax, may illustrate that the consciousness is a social adaptation and that other animals do not lack understanding or sensation, but that they lack language skills and therefore cannot reflect on their sensations and become self-aware. In principle Francis Crick, co-discover of double helix DNA structure believed this hypothesis. After he and James Watson solved the mechanism of inheritance, Crick moved to neuroscience and spent the rest of his trying to answer the biggest biological question; what is the consciousness? Working closely with Christof Koch, he published his final paper in the Philosophical Transactions of the Royal Society of London and in it he proposed that an obscure part of the brain, the claustrum, acts like a conductor of an orchestra and “binds” vision, olfaction, somatic sensation, together with the amygdala and other neuronal processing for the unification of thought and emotion. And the fact that all mammals have a claustrum means that it is possible that other animals have high intelligence.

So how different are the minds of animals in comparison to our own? Can their minds be algorithmically simulated? Many Scientists are reluctant to discuss animal intelligence as it is not an observable property and nothing can be perceived without reason and therefore there is not much-published research on the matter. But, by avoiding the comparison of some human mental states to other animals, we are impeding the use of a comparative method that may unravel the secrets of the cognitive mind. However, primates and cetacean have been considered by some to be extremely intelligent creatures, second only to humans. Their exalted status in the animal kingdom has lead to their involvement in almost all of the published experiments related to animal intelligence. These experiments coupled with analysis of primate and cetacean’s brain structure has to lead to many theories as to the development of higher intelligence as a trait. Although these theories seem to be plausible, there is some controversy over the degree to which non-human studies can be used to infer the structure of human intelligence.

By many of the physical methods of comparing intelligence, such as measuring the brain size to body size ratio, cetacean surpasses non-human primates and even rival human beings. For example “dolphins have a cerebral cortex which is about 40% larger a human being. Their cortex is also stratified in much the same way as humans. The frontal lobe of dolphins is also developed to a level comparable to humans. In addition, the parietal lobe of dolphins which “makes sense of the senses” is larger than the human parietal and frontal lobes combined. The similarities do not end there; most cetaceans have large and well-developed temporal lobes which contain sections equivalent to Broca’s and Wernicke’s areas in humans.”

Dolphins exhibit complex behaviours; they have a social hierarchy, they demonstrate the ability to learn complex tricks, when scavenging for food on the sea floor, some dolphins have been seen tearing off pieces of sponge and wrapping them around their “bottlenose” to prevent abrasions; illustrating yet another complex cognitive process thought to be limited to the great apes, they apparently communicate by emitting two very distinct kinds of acoustic signals, which we call whistles and clicks and lastly dolphins do not use sex purely for procreative purposes. Some dolphins have been recorded having homosexual sex, which demonstrates that they must have some consciousness. Dolphins have a different brain structure than humans that could perhaps be algorithmically simulated. One example of their dissimilar brain structure and intelligence is their sleep technique. While most mammals and birds show signs of rapid REM (Rapid Eye Movement) sleep, reptiles and cold-blooded animals do not. REM sleep stimulates the brain regions used in learning and is often associated with dreaming. The fact that cold-blooded animals do not have REM sleep could be enough evidence to suggest that they are not conscious and therefore their brains can definitely be emulated. Furthermore, warm-blood creatures display signs of REM sleep, and thus dream and therefore must have some environmental awareness. However, dolphins sleep unihemispherically, they are “conscious” breathers, and if fall asleep they could drown. Evolution has solved this problem by letting one half of its brain sleep at a time. As dolphins utilise this technique, they lack REM sleep and therefore a high intelligence, perhaps consciousness is possible that does not incorporate the transitional states mentioned earlier.

The evidence for animal consciousness is indirect. But so is the evidence for the big bang, neutrinos, or human evolution. As in any event, such unusual assertions must be subject to the rigorous scientific procedure, before they can be accepted as even vague possibilities. Intriguing, but more proof is required. However merely because we do not understand something does not mean that it is false — or not. Studying other animal minds is a useful comparative method and could even lead to the creation of artificial intelligence (that does not include irrelevant transitional states for an artificial entity), based on a model not as complex as our own. Still, the central point being illustrated is how ignorant our understanding of the human brain, or any other brain is and how one day a concrete theory can change thanks to enlightening findings.

Furthermore, an analogous incident that exemplifies this argument happened in 1847, when an Irish workman, Phineas Cage, shed new light on the field of neuroscience when a rock blasting accident sent an iron rod through the frontal region of his brain. Miraculously enough, he survived the incident, but even more, astonishing to the science community at the time were the marked changes in Cage’s personality after the road punctured his brain. Where before Cage was characterized by his mild-mannered nature, he had now become aggressive, rude and “indulging in the grossest profanity, which was not previously his custom, manifesting but little deference for his fellows, impatient of restraint or advice when it conflicts with his desires” according to the Boston physician Harlow in 1868. However, Cage sustained no impairment with regards to his intelligence or memory.

The serendipity of the Phineas Cage incident demonstrates how architecturally robust the structure of the brain is and by comparison how rigid a computer is. All mechanical systems and algorithms would stop functioning correctly or completely if an iron rod punctured them, that is with the exception of artificial neural systems and their distributed parallel structure. In the last decade, AI has begun to resurge thanks to the promising approach of artificial neural systems.

Artificial neural systems or simply neural networks are modelled on the logical associations made by the human brain, they are based on mathematical models that accumulate data, or “knowledge,” based on parameters set by administrators. Once the network is “trained” to recognize these parameters, it can make an evaluation, reach a conclusion and take action. In the 1980s, neural networks became widely used with the backpropagation algorithm, first described by Paul John Werbos in 1974. The 1990s marked major achievements in many areas of AI and demonstrations of various applications. Most notably in 1997, IBM’s Deep Blue supercomputer defeated the world chess champion, Garry Kasparov. After the match, Kasparov was quoted as saying the computer played “like a god.”

That chess match and all its implications raised profound questions about neural networks. Many saw it as evidence that true artificial intelligence had finally been achieved. After all, “a man was beaten by a computer in a game of wits.” But it is one thing to program a computer to solve the kind of complex mathematical problems found in chess. It is quite another for a computer to make logical deductions and decisions on its own.

Using neural networks, to emulate brain function provides many positive properties including the parallel functioning, relatively quick realisation of complicated tasks, distributed information, weak computation changes due to network damage (Phineas Cage), as well as learning abilities, i.e. adaptation upon changes in environment and improvement based on experience. These beneficial properties of neural networks have inspired many scientists to propose them as a solution for most problems, so with a sufficiently large network and adequate training, the networks could accomplish many arbitrary tasks, without knowing a detailed mathematical algorithm of the problem. Currently, the remarkable ability of neural networks is best demonstrated by the ability of Honda’s Asimo humanoid robot that cannot just walk and dance but even ride a bicycle. Asimo, an acronym for Advanced Step in Innovative Mobility, has 16 flexible joints, requiring a four-processor computer to control its movement and balance. Its exceptional human-like mobility is only possible because the neural networks that are connected to the robot’s motion and positional sensors and control its ‘muscle’ actuators are capable of being ‘taught’ to do a particular activity.

The significance of this sort of robot motion control is the virtual impossibility of a programmer being able to actually create a set of detailed instructions for walking or riding a bicycle, instructions which could then be built into a control program. The learning ability of the neural network overcomes the need to precisely define these instructions. However, despite the impressive performance of the neural networks, Asimo still cannot think for itself and its behaviour is still firmly anchored on the lower-end of the intelligent spectrum, such as reaction and regulation.

Neural networks are slowly finding there way into the commercial world. Recently, Siemens launched a new fire detector that uses a number of different sensors and a neural network to determine whether the combination of sensor readings are from a fire or just part of the normal room environment such as dust. Over fifty per cent of fire call-outs are false and of these well over half are due to fire detectors being triggered by everyday activities as opposed to actual fires, so this is clearly a beneficial use of the paradigm.

But are there limitations to the capabilities of neural networks or will they be the solution to creating strong-AI? Artificial neural networks are biologically inspired but that does not mean that they are necessarily biologically plausible. Many Scientists have published their thoughts on the intrinsic limitations of using neural networks; one book that received high exposure within the Computer Scientist community in 1969 was Perceptron by Minsky and Papert. Perceptron brought clarity to the limitations of neural networks, although many scientists were aware of the limited ability of an incomplex perceptron to classify patterns, Minsky’s and Papert’s approach of finding “what are neural networks good for?” illustrated what is impeding future development of neural networks. Within its time period Perceptron was exceptionally constructive and its identifiable content gave the impetus for later research that conquered some of the depicted computational problems restricting the model. An example is the exclusive-or problem. The exclusive-or problem contains four patterns of two inputs each; a pattern is a positive member of a set if either one of the input bits is on, but not both. Thus, changing the input pattern by one-bit changes the classification of the pattern. This is the simplest example of a linearly inseparable problem. A perceptron using linear threshold functions requires a layer of internal units to solve this problem, and since the connections between the input and internal units could not be trained, a perceptron could not learn this classification. Eventually, this restriction was solved by incorporating extra “hidden” layers. Although advances in neural network research have solved many of the limitations identified by Minsky and Papert, numerous still remain such as networks using linear threshold units still violate the limited order constraint when faced with linearly inseparable problems Additionally, the scaling of weights as the size of the problem space increases remains an issue.

It is clear that the dismissive views about neural networks disseminated by Minsky, Papert and many other Computer Scientists have some evidential support, but still, many researchers have ignored their claims and refused to abandon this biologically inspired system.

There have been several recent advances in artificial neural networks by integrating other specialised theories into the multi-layered structure in an attempt to improve the system methodology and move one step closer to creating strong-AI. One promising area is the integration of fuzzy logic. invented by Professor Lotfi Zadeh. Other admirable algorithmic ideas include quantum inspired neural networks (QUINNs) and “network cavitations” proposed by S.L.Thaler.

The history of artificial intelligence is replete with theories and failed attempts. It is in inevitable that the discipline will progress with technological and scientific discoveries, but will they ever reach the final hurdle?"
"But what exactly is quantum computing?

To understand why it’s so incredible, one must look at the difference between a quantum computer and a regular computer. A regular computer works by switching millions of tiny transistors between 1 and 0, or “on” and “off”.

The computer can only tell each transistor to either let an electric current pass or not. There’s no other way and no in-between. So a computer has to switch through the different combinations, one by one.

First, it’s for example 1000101, then 0101101 and then 1100100. These three random numbers already represent 3 different setups and have to occur in order. The computer can not make all 3 of them simultaneously. And though coming up with these 3 will only take the computer a few nanoseconds, having to go through billions of combinations with a lot more numbers (transistors) involved, can quickly become a time-consuming effort.

A quantum computer makes use of a physical phenomenon that takes place in the still quite mysterious quantum world. A so-called “qubit”, which replaces the traditional transistor and consists of a molecule that’s deliberately spun at incredible speeds by shooting it with lasers at pinpoint accuracy while keeping it suspended in a near-absolute-zero environment, will fall into a so-called superposition.

Remember the transistor? It’s either 1 or 0. The qubit, however, can be either 0, or 1, or anything in between (meaning a little of both at the same time). It uses a quantum state, which basically means it’s everything and nothing at the same time.

To describe it really simply: Instead of having to go through the three binary number examples one after the other, a quantum computer can calculate and display all three at the same time.

Imagine the game where you put a little ping pong ball under one of three plastic cups and start switching the cups around. If you were to work like a regular computer, you’d lift them up one by one to find the ball. A quantum computer simply lifts up all three at the same time, finds the ball, and then acts as if it never lifted the two empty cups in the first place."
"Will Tech’s Monopolies Survive 2020?

How the triple turmoil of a pandemic, protests, and a presidential election threatens Silicon Valley’s status quo.

Photo: Wang Ying/Xinhua via Getty

Welcome back to Pattern Matching, OneZero’s weekly newsletter that puts the week’s most compelling tech stories in context.

There was a brief moment, at the peak of the Covid-19 pandemic’s first wave in the United States, when it looked like Big Tech might be back in the public’s good graces. With stay-at-home orders across the country, screens were no longer an addictive distraction from real life, but the locus of real life itself. Zoom was powering business meetings; Houseparty, happy hours. Facebook was once again a dominant force in news; Apple and Google were partnering on a privacy-conscious contact tracing app. Politicians in the United States and Europe who had been laying the groundwork for new regulations suddenly had more urgent things to worry about.

That moment has passed. The lifting of lockdowns has been greeted not with sighs of relief at a return to the status quo, but with rallying cries to change it. There are protests in the streets. A presidential election looms. While criminal justice reform tops the domestic agenda, the appetite for tech reform appears to have returned as well.

The Pattern

Big Tech is back in the hot seat.

💬 The European Commission this week opened two antitrust probes against Apple, focusing on how its App Store rules and Apple Pay system, respectively, hamstring competitors. The App Store investigation was sparked by a 2019 complaint from Spotify about Apple’s practice of taking 30 percent of all subscription revenues from users who sign up for third-party apps on iOS. That puts Spotify at a disadvantage in competing with Apple’s own Apple Music service, from which Apple keeps 100 percent of revenues. (In 2018, Spotify stopped allowing users to pay via iOS.) The Apple Pay investigation, meanwhile, will examine how Apple limits the use of its devices’ “tap and go” payment functionality to Apple Pay alone, once again giving its own service a big edge over competitors.

💬 Spotify is hardly the only company affected. I wrote in depth in February about the brewing antitrust case against Apple, and the developers lining up to testify against it. In a twist of timing, one of those developers, Basecamp, launched a new paid email app, called Hey, on the same day the EU investigation was announced. Apple rejected it, on the grounds that it doesn’t allow the in-app subscription options that would give Apple its 30-percent cut. Protocol’s David Pierce recounted how that decision went down, while The Verge’s Dieter Bohn blasted Apple for inconsistencies in how it enforces its rules. Even Apple blogger Jon Gruber, who often defends the company, agreed that the company’s rent-seeking has gone too far. (Meanwhile, if you’re interested in Hey, read developer Kaya Thomas’ OneZero review of the buzzy, pricey new email platform.)

💬 Even mighty Facebook can’t get its apps onto Apple devices when they compete directly with Apple’s own offerings. The New York Times reported Thursday that Apple has rejected Facebook Gaming, the social network’s new casual gaming app, at least five times in the past four months, citing policies against apps that function primarily as game stores. Google, for its part, quickly approved Facebook Gaming on the Google Play store in April. Illustrating the user-unfriendly effects of Apple’s restrictions, the Times article explains that Facebook’s approach to getting its app approved has involved continually making the interface less intuitive, on the theory that this would make it less store-like.

💬 That Apple governs its App Store with impunity, and often to its own advantage, is not new. Neither is it new that Apple’s own apps sometimes compete with, copy, and crowd out those made for its platforms by independent developers. What is different now are the scope of Apple’s first-party app ambitions, the number of developers willing to risk the giant’s ire by speaking out, and the willingness of people in power to listen. In OneZero this week, Owen Williams argues that the EU case could be “a defining moment for the technology industry, as companies like Google and Facebook may find themselves scrutinized in a similar way.” And speaking of Google…

💬 Google made a similar power play this week by integrating its Meet videoconferencing software into the Gmail app. It’s a transparent attempt to leverage Google’s dominance in one market — email, in this case — against a rival (Zoom) that was outcompeting it in another market.

💬 The regulatory fervor is not confined to Europe. Back in the United States, the right broadened its assault on Section 230 this week, as Sen. Josh Hawley introduced a bill that would make it easier to sue tech companies for inconsistencies in how they moderate content. The move comes two weeks after Donald Trump signed an executive order challenging the legal protections that online platforms enjoy under Section 230 of the Communications Decency Act. Gizmodo’s Dell Cameron argues that the bill, like Trump’s order, is mostly toothless: It still allows companies to set their own rules of moderation, as long as they stick to them and apply them equally to all parties “in good faith.”

💬 And yet that even that mushy qualifier could open the door to enough lawsuits that some companies may simply decide a more hands-off approach is safest. Which is, of course, what Trump and Hawley want: for social platforms to keep their paws off of racist or false content from right-wing sources, including the president himself. This week conveniently brought us an illustration of the kind of dustup that could turn into a lawsuit, when NBC News reported that Google had banned the financial site ZeroHedge and conservative political site The Federalist from its ad network for spreading racist conspiracy theories about the anti-police brutality protests. As an uproar spread — both sites have large, vocal followings — Google disputed NBC News’ story. Google said The Federalist was never demonetized, but that it had reached an agreement with the publisher that involved removing racist comments from The Federalist’s comment section.

💬 These are the types of interventions that liberals and civil rights activists, along with some of tech companies’ own employees, have been calling for. (Some civil rights groups are now calling on companies to boycott Facebook’s ad platform.) They’re also the type that get the right riled up and build momentum for bills like Hawley’s, as Ben Shapiro and Ted Cruz were quick to rail against Google’s moves this week. As I wrote in a previous newsletter, we appear to have at last reached the point where the big platforms have to pick a side. Twitter was the first to do so, when it started flagging some of Trump’s tweets as misleading, and the company kept up its enforcement against him this week by putting a warning label on a video he tweeted. The video fabricated fake CNN footage of a “terrified” Black toddler running away from a “racist baby,” then implied that the network was spreading divisive fake news (a classic example of Trumpian projection).

💬 Facebook has opted for the laissez-faire approach to Trump’s posts, but even it felt compelled to take action this week when the liberal blog Media Matters for America reported that the president was running Facebook ads with Nazi iconography. (Trump’s campaign then claimed the inverted red triangle was an antifa symbol — which is a lie, according to historians who study the group.) Facebook removed the 88 offending ads.

💬 Antitrust enforcement and Section 230 reform are separate issues. But the growing momentum behind both is indicative of a larger trend: Big Tech has lost the benefit of the doubt. That happened long ago in Europe, but it is finally happening in the United States as well, from both major parties. And any notion that the pandemic or a Republican presidency would ease the regulatory pressure on Silicon Valley has now been put to rest. Americans of all ideologies are fed up with business as usual, their polarization arguably stoked by the tech platforms themselves, and their economic stability undermined by the rise of the gig economy. In other words, we’re living in a mess that is partly of the tech industry’s making. And now that mess is coming back to haunt it.

Undercurrents

Under-the-radar trends, stories, and random anecdotes worth your time

🗨️ Two Black leaders at Pinterest left the company over racial discrimination, saying they were subjected to offensive comments, unfair pay, and retaliation. CEO Ben Silbermann subsequently issued a public apology and admitted “parts of our culture are broken,” Bloomberg’s Sarah Frier reported. But the ex-employees, Ifeoma Ozoma and Aerica Shimizu Banks, who made up two-thirds of the platform’s public policy and social impact team, said on Twitter that they heard the apology only through the media. Their allegations are part of a wider reckoning over tech companies’ treatment of Black employees, and they dent the reputation of a platform that had previously earned praise for some progressive policies — which, it turns out, Ozoma and Banks had been criticized by their managers for championing. Read Ozoma’s full thread here.

🗨️ Lesser-known face recognition companies are eagerly courting law enforcement, looking to fill the vacuum after IBM, Microsoft, and Amazon stepped back. Clearview AI, NEC, nd Ayonix are among those poised to capitalize by ignoring the anger over surveillance technology’s discriminatory effects on Black communities, the Wall Street Journal reported. My OneZero colleague Dave Gershgorn has written about an even longer list of companies, including many that you might not expect, that have been trying to cash in on a face recognition gold rush. In Bloomberg Opinion, Cathy O’Neil makes the case that face recognition by law enforcement will continue until or unless Congress repeals post-9/11 legislation, such as the Real ID Act, that prioritized antiterrorism efforts over civil liberties.

🗨️ Instagram’s algorithm systematically incentivizes its users to show skin in their photos, according to a report from the nonprofit AlgorithmWatch.

🗨️ The great scourge of bots on social media may be overstated, bot expert Darius Kazemi argued, in a New York Times article by Siobhan Roberts.

Headlines of the Week

Facebook Groups Are Destroying America

— Nina Jankowicz and Cindy Otis, Wired

Devin Nunes’ Attorney Says He’s at ‘Dead End’ in Quest to Reveal Identity of Twitter Cow

— Kate Irby, Fresno Bee

Thanks for reading. Reach me with tips and feedback by responding to this post on the web, via Twitter direct message at @WillOremus, or by email at oremus@medium.com."
"2. Programmers Shouldn’t Be Insecure About Their Work

Now, I’ve got some really skilled senior developers working with me. They are competent individuals who require high standards from my pull requests. Otherwise, they will easily reject them without remorse.

I used to think of these people as developers not capable of doing any wrong. I would look at them in awe and wonder if one day I would reach their level of skills and never code anything incorrectly.

But this belief was wrong.

It didn’t take much for me to gain more experience and dialogue with my colleagues, only to realize that they have insecurities too. They don't always know everything and are not always sure about the best possible solution for a given problem. They have to constantly renew their knowledge like I have to. They know that sometimes they just have to go with the flow and their intuition on what is best for the project.

I’ve freed myself from this belief and now I don’t always feel stuck in my own world thinking that I will never be a senior developer because I know that seniors have my feelings of insecurity too."
"An article published in Nature magazine in autumn 2017 makes for interesting reading. It reports on research carried out by Washington State University and Arizona State University, which shows that the wealth disparity in human societies was insignificant until the development of agriculture. That occurred in different parts of the world around 13,000 years ago. What happened next should be a warning to humans in the age of artificial intelligence (AI).

Land cultivation started when groups of nomads stayed in one location probably due to illness, injury, bad weather, or fear of other tribes. A few individuals experimented with seeds and plants and discovered that they could grow edible crops in dedicated plots and repeat the process each year. That reduced their need to constantly hunt, fish, and search for wild fruit and vegetables. Some grabbed more land than others and became the wealthiest of the group. The wealth gap increased even more, when some people learned how to tame large animals like oxen and horses and used them to till larger areas and, in the case of horses, more effectively fight adversaries and, so, acquire more land.

Having the latest and most powerful technology — in the broadest sense of the word — has always meant riches and power. The industrial revolution, which replaced much animal and human sweat with steam power, made the owners of the steam engines and factories very wealthy. Today’s technological equivalent of oxen, horses, and steam engines are computer systems and, just as in the days of the early humans, those who control that new technology are among the richest. Technology itself, however, may soon upend that age-old equivalence.

Many respected experts predict that the processing power of computers will surpass that of humans within the next few decades. Some of those experts, including Tesla and SpaceX CEO, Elon Musk, and the late theoretical physicist, Stephen Hawking, worry that artificial intelligence machines will eventually become conscious i.e. gain self-awareness, will be smarter than humans, and continue to get smarter quickly. These ultra-smart machines, the experts warn, will pose an existential threat to humanity because humans will not know what they’re thinking and so won’t be able to control them. Of course, nobody knows for sure that this will happen and, if it does, precisely when, but the expert warnings are credible enough to be taken seriously.

Scientists call the hypothetical moment in time when machines become conscious as the “singularity.” If that moment arrives, the experts suggest a number of possible scenarios. The most benign is that the machines will work for the benefit of their human creators and that there would be no reason for them to harm humans. Yet how could anyone be sure that that would be the case, since humans would not know what the machines are thinking? Even today, computer scientists don’t fully understand why complex computers make some of the decisions they make. Autonomous machines that can harm humans already exist. Drones without a human controller can be programmed to locate and attack targets. Some scientists argue that if machines become conscious, they are likely to regard humans as unnecessary and inefficient and eliminate them.

Elon Musk, among others, suggests that the only way to match these super-intelligent machines is to augment human intelligence by joining human brains to the machines. In that science-fiction scenario, the human race would become a race of cyborgs. Since cyborgs’ machine elements will doubtlessly evolve quicker than the biological elements, humans will gradually, but effectively turn into machines. That suggests that Elon Musk’s proposal is not really a solution at all and that the machines will eventually take over one way or another.

Thanks to AI, the rich will get richer until, ironically, humans lose control of the technology they invented. That’s unlikely to happen anytime soon, but it could happen sooner than most people think. When it does, for the first time in human history, being very wealthy will count for very little."
"I’ve noticed a lot of confusion in the industry about various software roles and titles, even among founders, hiring managers, and team builders. What are the various roles and responsibilities on a software team, and which job titles tend to cover which roles?

Before I dig into this too much, I’d like to emphasize that every team is unique, and responsibilities tend to float or be shared between different members of the team. Anybody at any time can delegate responsibilities to somebody else for various reasons.

If your team isn’t exactly what I describe here, welcome to the club. I suspect very few teams and particular software roles will match perfectly with what we’re about to explore. This is just a general framework that describes averages more than any particular role or team.

I’ll start with management titles and work my way through various roles roughly by seniority.

I’d also like to emphasize that you should not feel constrained by your job title. I like to build an engineering culture which favors:

Skills over titles

over titles Continuous delivery over deadlines

over deadlines Support over blame

over blame Collaboration over competition

I like to reward initiative with increased responsibility, and if somebody has the skills and initiative to take on and outgrow the title they’re hired for, I like to promote rather than risk losing a rising star to another company or team.

Software Development Roles

Engineering Fellow

CEO

CTO

CIO/Chief Digital Officer/Chief Innovation Officer

VP of Engineering/Director of Engineering

Chief Architect

Software Architect

Engineering Project Manager/Engineering Manager

Technical Lead/Engineering Lead/Team Lead

Principal Software Engineer

Senior Software Engineer/Senior Software Developer

Software Engineer

Software Developer

Junior Software Developer

Intern Software Developer

We’ll also talk a little about how these roles relate to other roles including:

VP of Product Management/Head of Product

Product Manager

VP of Marketing

Note: Sometimes “director”, or “head” titles indicate middle managers between tech managers and the C-Suite. Often, “Chief” titles indicate a C-suite title. C-suite employees typically report directly to the CEO, and have potentially many reports in the organizations they lead. At very large companies, those alternate titles often fill similar roles to C-suite executives, but report to somebody who is effectively the CEO of a smaller business unit within the larger organization. Different business units sometimes operate as if they are separate companies, complete with their own isolated accounting, financial officers, etc. Different business units can also have VPs, e.g., “Vice President of Engineering, Merchant Operations”.

Engineering Fellow

The title “fellow” is the pinnacle of achievement for software engineers. It is typically awarded in recognition of people who have made outstanding contributions to the field of computing, and is usually awarded after an engineer writes a number of top selling books, wins prizes like the Turing Award, the Nobel Prize, etc. In other words, fellows are usually already famous outside the organization, and the company is trying to strengthen their brand by more strongly associating themselves with admired and influential people.

In my opinion, organizations should not try to hire for “fellow” roles. Instead, find the best and brightest, hire them, and then grant the title (and benefits) if the engineer is deserving of it.

A fellow typically also holds another title at the company. Often a CTO, Architect, VP of Engineering, or principal role, where they are in a position to lead, mentor, or serve as an example and inspiration to other members of the organization.

CEO

The CEO is the position of most authority in an organization. Typically, they set the vision and north star for the company. They rally everybody around a common understanding of why the company exists, what the mission is, and what the company’s values are. Frequently, CEOs are also the public face of the company, and in some cases, become synonymous with the brand (e.g., Steve Jobs with Apple, Elon Musk with Tesla/SpaceX, etc.)

In some cases, CEOs are also the technical founder of a software organization, in which case, they also often fill the CTO role, and may have a VPs of Operations, Sales, Strategy, and Marketing helping with some of the other common CEO responsibilities.

The CEO of a small company frequently wears a lot of hats, as you may have picked up from all the other roles that fell out of the CEO title when I mentioned that some CEOs lead the technology team.

In any case, if there are important organizational decisions to be made, you can’t run it up the chain of responsibility any higher than the CEO.

If you are a CEO, remember that you’re ultimately responsible, and you should trust your instincts, but don’t forget that even most famous CEOs have mentors and advisors they consult with on a regular basis. Trust your gut, but seek out smart, insightful people to challenge you to improve, as well.

CTO

Like the CEO role, the CTO role shape-shifts over time. At young startups, the CTO is often a technical cofounder to a visionary or domain-driven CEO. Frequently they are not qualified to take the title at a larger company, and hopefully grow into it as the company grows. Frequently, a startup CTO finds that they prefer more technical engineering roles, and settle back into other roles, like Principal Engineer, VP of Engineering, or Chief Architect.

In many organizations, the mature CTO role is outward facing. They participate in business development meetings, frequently helping to land large partnerships or sales. Many of them hit the conference circuit and spend a lot of time evangelizing the development activities of the organization to the wider world: sharing the company’s innovations and discovering opportunities in the market which match up well with the company’s core competencies. CTOs frequently work closely with the product team on product strategy, and often have an internal-facing counterpart in engineering, such as the VP of Engineering.

CTOs also frequently set the vision and north star of the engineering team. The goals for the team to work towards.

CIO/Chief Digital Officer/Chief Innovation Officer

The Chief Innovation Officer (CIO) is like a CTO, but typically employed by a company that would not normally be considered a “tech company”. The goal of the CIO is to reshape the company into one that consumers perceive as tech-savvy and innovative: To show the world what the future of the industry looks like, no matter what that industry is. For example, a home remodeling superstore chain might have a CIO responsible for partnering with tech companies to build a mixed reality app to show shoppers what a specific couch or wall color would look like in their living room, or using blockchains and cryptocurrencies to enhance the security and efficiency of supply chain logistics.

Not to be confused with a Chief Information Officer (CIO), a title which is typically used in companies who are even more detached from technology, interested about as far as it aids their core operations. Unlike a Chief Innovation Officer, A Chief Information Officer is more likely to be leading tech integration and data migration projects than building new apps and trying to figure out how a company can disrupt itself from the inside. There are Chief Information Officers who act more like Chief Innovation Officers, but in my opinion, they should use the appropriate title.

Most tech-native companies (app developers, etc) don’t have either kind of CIO. Instead, those responsibilities fall to the CTO and VP of Engineering.

VP of Engineering/Director of Engineering

While CTOs often face outward, the VP of Engineering often faces inward. A VP of Engineering is frequently responsible for building the engineering team and establishing the engineering culture and operations. The CTO might tell the engineering team what needs to get done on the grand scale, e.g., “be the leading innovator in human/computer interaction”. The VP of Engineering helps foster a culture that manages the “how”. The best VPs of Engineering at first come across as somebody who’s there to help the team work efficiently, and then they almost disappear. Developers on the team collaborate well, mentor each other, communicate effectively, and they think, “Hey, we’re a great team. We work really well together!” and maybe they think that’s all a lucky accident.

The truth is that almost never happens by accident. It happens because there’s a VP of Engineering constantly monitoring the team’s progress, process, culture, and tone of communications. They’re encouraging developers to use certain tools, hold specific kinds of meetings at specific times in order to foster better collaboration with fewer interruptions. The best VPs of Engineering have been engineers, both on dysfunctional teams, and on highly functional teams. They know the patterns and anti-patterns for effective software development workflows.

They work with the heads of product and product managers to ensure that there’s a good product discovery process (they don’t lead it or take charge of it, just make sure that somebody is on it and doing it well), and that product and design deliverables are adequately reviewed by engineers prior to implementation hand offs. I’m going to stop there before I write a book on all the work that goes into leading effective development operations. For more of my thoughts on this topic, check out How to Build a High Velocity Development Team.

Many startups are too small to hire a full time VP of Engineering, but it’s still very important to get engineering culture right as early as possible. If you need help with this, reach out.

Chief Architect

At small organizations, the chief architect could be a technical co-founder with the self-awareness to realize that they won’t want the responsibilities of a CTO as the company grows. Maybe they don’t like to travel, or are simply more interested in software design than conference talks, business development, and sales calls that infiltrate the lives of many CTOs. The chief architect may be responsible for selecting technology stacks, designing collaborations and interfaces between computing systems, assessing compute services offerings (AWS, Azure, ZEIT Now, etc.), and so on. A chief architect may evaluate a wide range of industry offerings and make pre-approved or favored recommendations to work with particular vendors.

As the company matures, the chief architect may also need to work closely with the CTO, and sometimes partner organizations to develop integrations between services. At many companies, the CTO also serves as the chief architect.

Software Architect

A software architect serves many of the purposes of a chief architect, but is generally responsible for smaller cross-sections of functionality. Architects will often work with the chief architect to implement their slice of the larger architectural vision. Software architects often make tech stack choices for particular applications or features, rather than company-wide decisions.

Engineering Project Manager/Engineering Manager/Project Manager

An Engineering Project Manager (also called “Engineering Manager” or simply “Project Manager”) is in charge of managing the workflow of an engineering team. Some larger companies have both Engineering Managers and Project Managers. In that case, the Engineering Manager typically acts like the VP of Engineering at the local team scope, while the Project Manager takes on the responsibilities described here.

Project Managers typically interface with both product leaders and an engineering leader such as VP of Engineering, CTO, or a middle manager to cultivate and prune the work backlogs, track the progress of work tickets, detailed progress reports (milestone burn down charts, completed vs open tickets, month/month progress reports, etc.) You can think of them as the analog of a shop manager for a manufacturing assembly line. They watch the work floor and make sure that the assembly line runs smoothly, and work product isn’t piling up on the floor in front of a bottleneck.

The best Project Managers also spend a lot of time classifying issues and bugs in order to analyze metrics like bug density per feature point, what caused the most bugs (design error, spec error, logic error, syntax error, type error, etc.) and so on. Those kinds of metrics can be used to measure the effectiveness of various initiatives, and point out where improvements can be made to the engineering process.

Engineering Managers tend to develop a good understanding of the strengths of various team members, and get good at assigning work tickets to the appropriate responsible parties, although, this should be a collaborative effort, seeking feedback from individual developers on what their career goals are and what they want to focus on, within the bounds of the project scope available.

If there is time pressure or work backlogs piling up, the Project Manager should collaborate with the engineering and product leaders to figure out the root cause and correct the dysfunction as soon as possible.

Wherever possible, the Project Managers should be the only ones directly delegating tasks to individual engineers in order to avoid the multiple bosses problem. Engineers should have a clear idea of who they report directly to, and who’s in charge of delegating to them. If you’re a different kind of engineering leader, and you’re guilty of delegating directly to engineers, it’s probably a good idea to coordinate with the Engineering Manager in charge of the report you’re delegating to and delegate through them so that the work receives correct, coordinated prioritization, and the Engineering Manager is aware of what each engineer is actively working on at any given moment.

At very small organizations, the Engineering Manager is often also the CTO and VP of Engineering (with or without the corresponding titles). If that’s you, don’t worry about the previous paragraph.

A common dysfunction is that the Engineering Manager can begin to think that because product hands off work for engineering to implement, and Engineering Managers work closely with product teams, that the Engineering Manager reports to a Product Manager. In every case I’ve seen that happen, it was a mistake. See “Avoiding Dysfunctions…” below.

Tech Lead/Team Lead

The Tech Lead or Team Lead is usually the leader of a small number of developers. They are usually senior engineers who act like mentors, examples, and guides for the rest of the team. Usually, engineers report to the project manager or engineering manager, but a tech lead may be responsible for the team’s code quality measures, such as ensuring that adequate code reviews are being conducted, and that the team’s technical standards (such as TDD) are being upheld.

Engineer Career Progressions

Generally, engineers can take one of two career paths: move into management, or keep coding. Management positions aren’t for everyone. Lots of engineers prefer to stay on the technical path. That progression can take many directions, twists, and turns, but could look something like this:

Intern -> Junior Software Developer -> Software Developer/Engineer -> Senior Software Engineer -> Principal Software Engineer -> Software Architect -> Senior Software Architect -> Chief Architect -> CTO -> Engineering Fellow

Alternatively, for those engineers interested in a people leadership role, a progression might look something like this:

Intern -> Junior Software Developer -> Software Developer/Engineer -> Team Lead/Tech Lead -> Engineering Manager/Project Manager -> Senior Engineering Manager -> Director of Engineering -> VP of Engineering

Avoiding Dysfunctions in Engineering Leadership

IMO, VP of Engineering, CTO, VP of Product, and VP of Marketing should all report directly to the CEO. Each of them needs to be in charge of their own process. External facing CTOs should not have direct reports (if they do, it usually means they are filling both the CTO and VP of Engineering Roles). Instead, the Engineering leaders report to the VP of Engineering. This is to avoid the two bosses dysfunction, but also because these roles are fundamentally different: one focused on the customer and how the organization fits into the wider world, and the other focused on internal, day-to-day operations. They’re two wildly different skill sets, with sometimes competing priorities.

I’ve seen a lot of dysfunction in engineering leadership because of confusion about which engineering leaders are responsible for what, and it tends to be a recipe for disaster. Whatever is right for your organization, make sure that responsibilities and chain of authority are clear, in order to avoid engineers feeling torn between two or three different “bosses”.

Likewise, in an organization of sufficient size, product and engineering need to be two separately led teams. What I mean by that is that the product managers should own the product roadmap. They should be evangelists for the users, and they should be really plugged into the users, often engaging with them 1:1 and learning about their workflows and pain-points in great depth. They should be experts on what the market needs, and they should be very familiar with the company’s strengths and capabilities to fill those needs.

That said, the VP of Engineering (or whomever is filling that role) needs to be in charge of delivery, and production pace. While the product managers should own the roadmap, the engineering managers need to be responsible for taking those roadmap priorities, matching them to the engineering capacity, and reporting on the timing. Product and marketing teams will have strong opinions about when something should ship, but only the engineering management has a good gauge of whether or not those delivery timelines are possible given the roadmap requirements. The engineering team needs the authority not simply to push back on timing, but in most cases, to completely own timing, working with the CEO, product, and marketing teams to figure out priorities, understand strategic needs of the company, and then help shape a development cadence that can meet those needs without imposing drop-dead deadlines that ultimately hurt the company’s ability to deliver quality products at a reliable pace.

The best performing teams I’ve ever been involved with subscribed to the no deadlines approach. We build great products without announcing them in advance, and then let the marketing teams promote work that is already done. Alternatively, when you’re working in the public view, transparency is a great solution. Instead of cramming to meet an arbitrary deadline, actively share your progress, with ticket burn-down charts, a clear view of remaining work, progress, pace, and remaining scope, and change over time that can indicate scope creep. When you share detailed information about the progress being made, and share the philosophy that we can’t promise a delivery date, but we can share everything we know about our progress with you, people can see for themselves the work and the pace.

Because of differing, often competing goals, product, marketing and engineering need to be separate roles reporting directly to the CEO where none of them can dictate to each other. If your team feels time pressure to work overtime, or crunch to get some key deliverable out before some drop-dead deadline, it points to a dysfunction here. Either the engineering managers are reporting to the wrong people, or the team lacks a strong engineering leader who understands the futility of software estimates and the need for a collaborative give-and-take between engineering and product in order to ensure the flexibility of shipping scaled-back MVPs to hit delivery targets.

Product should own the continuous discovery process. Engineering should own the continuous delivery process. Marketing should work hand-in-hand with the product team to ensure that product messaging to the wider world is on-point. The whole thing should fit together like a pipeline, creating a smoothly flowing, positive feedback cycle. Like an assembly line, the slowest bottleneck in the process must set the pace for the rest of the process, otherwise, it will lead to an ever-growing backlog that piles up so much that backlog items become obsolete, and backlog management becomes a full-time job.

Product teams who feel like engineering is not keeping pace should focus first on quality of engineering hand-off deliverables. Have we done adequate design review? Has an engineer had a chance to provide constructive feedback before handoff? 80% of software bugs are caused by specification or UX design errors, and many of those can be caught before work ever gets handed off to an engineering team. Once you have that process finely tuned, ask yourself if you’ve really explored the product design space thoroughly enough. Did you build one UX and call it done, or did you try multiple variations? Building and testing variations on user workflows is one of the most valuable contributions a product team can make. Do you have a group of trusted users or customers you can run A/B prototype tests with?

One of the biggest dysfunctions of software teams is that the product team is producing sub-par deliverables (sometimes little more than a few rushed, buggy mock-ups), and failing to run any of them by customers or engineers prior to handing them off. That dysfunction causes a pileup of re-work and engineering backlog that often gets blamed on engineering teams.

Make sure that the delegation of responsibilities makes sense, that you’re not putting undue time pressure on engineering, and that you have a great product team engaged in a collaborative product discovery process, working with real users to build the best product.

Engineering managers, I’m not letting you off the hook. If these dysfunctions exist on your team, it’s your responsibility to address them with product, marketing, and business leadership, and spearhead requirements for engineering hand-offs. It’s also your responsibility to protect the productive pace of your team, go to bat for additional resources if your team is being pressured to produce more than your current capacity can handle, to report clearly on the work pacing and backlog, and to demo completed work and ensure that your team is getting due credit for the fine work that is being done.

Don’t place blame, but do demonstrate that your team is doing their very best work."
"Bear: The one and only home for all my ideas. The tagging system sold me on this. It’s not the only app with this feature, but it does do it rather well. Instead of categories, Bear allows you to add multiple sorting tags to each note. With categories, one note has one category. With tags, one note can have multiple “categories.”

As a result, the organization is more complete. Which matters to me because connections between thoughts are important. For instance, let’s say I create a note about a new strategy for my trading algorithm. That would obviously belong in the “Code” category. But what if I also want to add it to my post queue, the “Posts” category? I can’t categorize it as both “Code” and “Posts.” But with Bear, I can.

I simply add both “#code” and “#posts” to that one note. Done. Now I can find it under both “#code” and “#posts” in the sidebar. In other apps, I’d have to duplicate the information. Maybe I’d add “post about new trading strategy” to a different note under the Posts category. Obviously, Bear’s tag system is superior.

Bear’s note editor in fullscreen mode. Source: author.

Moving on, the Bear editor is one of the best I’ve used for on-the-fly ideas. Fast. Clean. Effective. Plus, Markdown support is excellent — I can format quickly even on my phone. This was a problem with my ex-idea jotter, Agenda. Editing was too slow. In contrast, there’s less friction with Bear. A big deal when I’ve got three things to write down at the same time.

Now, I’m punctilious when it comes to UI. I probably apply more selection criteria to user interfaces than I’ll apply to my wife. Corner radius of a single button looks off? Junk it. There’s no compromise when it comes to design. I look at these tools for hours a day. They must be pretty. And I’m happy to report Bear passes all the tests. In fact, I might even call it impressive. Dark. Clear. Intuitive. I can get behind the aesthetic.

The Dieci theme suits my fancy on my iPhone and iPad. On Mac, Solarized Dark suits the bigger screen. You’ve got plenty of skin options if you go pro — more on that later.

Extras

But things get fun when you leave the app, too. The developers thought of everything. It’s pretty much as integrated as a 1st party app. For instance, I set up a Siri Shortcut to dictate a new Bear note using Siri. When I suddenly come up with a bug fix while cooking, this is a lifesaver. In addition, I have a shortcut to record a post idea from my clipboard — useful when something I’m reading inspires me. Just copy some text, run the shortcut, and the idea is stored in Bear under “#posts”. The whole process takes three seconds. Not all note apps can do this.

And the Bear widgets on my iPad Pro’s Today view? Remarkably useful. I set it up to show my most recent notes. So with one tap, I can pick up where I left off. Right from the home screen. No digging through categories. No filtering. This is what notes should be: Natural. Accessible. An extension of your own mind. Bear gets as close to that as I’ve ever experienced.

I mean, you can even access your local Bear Notes database on macOS. For the non-programmers out there, this means you have infinite automation options for manipulating your notes. Granted, most of you won’t need this. But hey, it’s an option.

Pricing

Finally, let’s talk cost. Bear Pro will cost you $1.49 per month. If you don’t have commitment issues, unlike me, you could pay roughly $1.25 per month, billed annually. Whichever option you prefer, just buy it. This pricing is absurd. I’m not sure how the good folks who make this are turning a profit. A dollar and a half per month? You can find that walking down the street.

The free version doesn’t have sync. And themes are rather limited. If you’re a light user, I guess you could make it work. But it’s a deal-breaker for me. I use this every day. All the time. For hours. To do everything. They could charge ten times their current price, and it’d still be a bargain. It helps that I’m sold on the brand, too. I mean, what developer offers free themed wallpapers? Well-played, Bear. You got me there.

In summary, I can’t recommend Bear enough. It’s simple if that’s all you need. But when the rubber meets the road, it keeps up. Value for money, capable, and pretty. What else do you need? Migrating from your existing notes app is trivial, too. So there’s not much stopping you.

Alternative: Drafts

One alternative I’ve heard about is Drafts: Similar editing, similar organization, just five times uglier. Can’t stand it. Not enough negative space in the layout of the editor. The menus look messy. Too distracting. Overall, not as consistent as Bear. I need serenity in my idea sanctuary. But if you’re less pedantic, why not give it a shot."
"A lock is a thread synchronization mechanism like synchronized blocks except locks can be more sophisticated than Java’s synchronized blocks. Locks are created using synchronized blocks, so it is not like we can get totally rid of the synchronized keyword.

From Java 5 the package java.util.concurrent.locks contains several lock implementations, so you may not have to implement your own locks. But you will still need to know how to use them, and it can still be useful to know the theory behind their implementation.

Lock vs Synchronized Block

There are few differences between the use of synchronized block and using Lock API:

A synchronized block is fully contained within a method — we can have Lock API’s lock() and unlock() operation in separate methods

we can have Lock API’s lock() and unlock() operation in separate methods A synchronized block doesn’t support fairness, any thread can acquire the lock once released, no preference can be specified. We can achieve fairness within the Lock APIs by specifying the fairness property . It makes sure that the longest waiting thread is given access to the lock

. It makes sure that the longest waiting thread is given access to the lock A thread gets blocked if it can’t get access to the synchronized block. The Lock API provides the tryLock() method. The thread acquires lock only if it’s available and not held by any other thread. This reduces the blocking time of thread waiting for the lock

This reduces the blocking time of thread waiting for the lock A thread that is in the “waiting” state to acquire access to a synchronized block, can’t be interrupted. The Lock API provides a method lockInterruptibly() which can be used to interrupt the thread when it’s waiting for the lock

Lock Reentrance

All implicit monitors implement reentrant characteristics. Reentrant means that locks are bound to the current thread. A thread can safely acquire the same lock multiple times without running into deadlocks (e.g. a synchronized method calls another synchronized method on the same object).

When the thread first enters into the lock, a hold count is set to one. Before unlocking the thread can re-enter into lock again and every time hold count is incremented by one. For every unlocks request, the hold count is decremented by one and when the hold count is 0, the resource is unlocked.

Lock Fairness

Java’s synchronized blocks make no guarantees about the sequence in which threads trying to enter them are granted access. Therefore, if many threads are constantly competing for access to the same synchronized block, there is a risk that one or more of the threads are never granted access — that access is always granted to other threads. This is called starvation. To avoid this a Lock should be fair.

Reentrant Locks also offer a fairness parameter, by which the lock would abide by the order of the lock request i.e. after a thread unlocks the resource, the lock would go to the thread which has been waiting for the longest time. This fairness mode is set up by passing true to the constructor of the lock.

Lock Methods

Let’s take a look at the methods in the Lock interface:

lock() — call to the lock() method increments the hold count by 1 and gives the lock to the thread if the shared resource is initially free.

call to the method increments the hold count by 1 and gives the lock to the thread if the shared resource is initially free. unlock() — call to the unlock() method decrements the hold count by 1. When this count reaches zero, the resource is released.

call to the method decrements the hold count by 1. When this count reaches zero, the resource is released. tryLock() — if the resource is not held by any other thread, then call to tryLock() returns true and the hold count is incremented by one. If the resource is not free then the method returns false and the thread is not blocked but it exists.

if the resource is not held by any other thread, then call to returns true and the hold count is incremented by one. If the resource is not free then the method returns false and the thread is not blocked but it exists. tryLock(long timeout, TimeUnit unit) — as per the method, the thread waits for a certain time period as defined by arguments of the method to acquire the lock on the resource before exiting.

as per the method, the thread waits for a certain time period as defined by arguments of the method to acquire the lock on the resource before exiting. lockInterruptibly() — this method acquires the lock if the resource is free while allowing for the thread to be interrupted by some other thread while acquiring the resource. It means that if the current thread is waiting for a lock but some other thread requests the lock, then the current thread will be interrupted and return immediately without acquiring a lock.

Lock Implementations

Multiple lock implementations are available in the standard JDK:

ReentrantLock

ReentrantReadWriteLock

StampedLock

ReentrantLock

The class ReentrantLock is a mutual exclusion lock with the same basic behavior as the implicit monitors accessed via the synchronized keyword but with extended capabilities. As the name suggests this lock implements reentrant characteristics just as implicit monitors.

A lock is acquired via lock() and released via unlock() . It's important to wrap your code into a try/finally block to ensure unlocking in case of exceptions. This method is thread-safe just like the synchronized counterpart. If another thread has already acquired the lock subsequent calls to lock() pause the current thread until the lock has been unlocked. Only one thread can hold the lock at any given time.

ReentrantReadWriteLock

ReentrantReadWriteLock class implements the ReadWriteLock interface. The interface ReadWriteLock specifies another type of lock maintaining a pair of locks for read and write access.

The idea behind read-write locks is that it's usually safe to read mutable variables concurrently as long as nobody is writing to this variable. So the read-lock can be held simultaneously by multiple threads as long as no threads hold the write-lock. This can improve performance and throughput in the case reads are more frequent than writes.

Let’s see rules for acquiring the ReadLock or WriteLock by a thread:

Read Lock — if no thread acquired the write lock or requested for it then multiple threads can acquire the read lock

— if no thread acquired the write lock or requested for it then multiple threads can acquire the read lock Write Lock — if no threads are reading or writing then only one thread can acquire the write lock

The above example first acquires a write-lock in order to put a new value to the map after sleeping for one second. Before this task has finished two other tasks are being submitted trying to read the entry from the map and sleep for one second:

When you execute this code sample you’ll notice that both read tasks have to wait the whole second until the writing task has finished. After the write lock has been released both read tasks are executed in parallel and print the result simultaneously to the console. They don’t have to wait for each other to finish because read-locks can safely be acquired concurrently as long as no write-lock is held by another thread.

StampedLock

Java 8 ships with a new kind of lock called StampedLock which also supports read and write locks just like in the example above. In contrast to ReadWriteLock the locking methods of a StampedLock return a stamp represented by a long value. You can use these stamps to either release a lock or to check if the lock is still valid.

Another feature provided by StampedLock is optimistic locking. Most of the time read operations don’t need to wait for write operation completion and as a result of this, the full-fledged read lock isn’t required.

Instead, we can upgrade to read lock:

Working With Conditions

The Condition class provides the ability for a thread to wait for some condition to occur while executing the critical section.

This can occur when a thread acquires access to the critical section but doesn’t have the necessary condition to perform its operation. For example, a reader thread can get access to the lock of a shared queue, which still doesn’t have any data to consume.

Traditionally Java provides wait(), notify() and notifyAll() methods for thread intercommunication. Conditions have similar mechanisms, but in addition, we can specify multiple conditions:

Semaphores

In addition to locks, the Concurrency API also supports counting semaphores. Whereas locks usually grant exclusive access to variables or resources, a semaphore is capable of maintaining whole sets of permits. This is useful in different scenarios where you have to limit the amount of concurrent access to certain parts of your application.

Here’s an example of how to limit access to a long-running task simulated by sleep(5) :

The executor can potentially run 10 tasks concurrently but we use a semaphore of size 5, thus limiting concurrent access to 5. It’s important to use a try/finally block to properly release the semaphore even in case of exceptions.

Executing the above code results in the following output:

Semaphore acquired

Semaphore acquired

Semaphore acquired

Semaphore acquired

Semaphore acquired

Could not acquire semaphore

Could not acquire semaphore

Could not acquire semaphore

Could not acquire semaphore

Could not acquire semaphore

The semaphores permit access to the actual long-running operation simulated by sleep(5) up to a maximum of 5. Every subsequent call to tryAcquire() elapses the maximum wait timeout of one second, resulting in the appropriate console output that no semaphore could be acquired.

Conclusion

In this article, we have seen different implementations of the Lock interface and learned how to use them in multithreaded applications.

In the next article, we will check the Executors. Stay tuned."
"This is a series of 4 articles I am sharing here, for people who are concerned and eager to understand more about job displacement impact potentially caused by artificial intelligence technology. You would read about “safe” versus “endangered” jobs in this series. The jobs listed in each article are demonstrative from my research research and technological knowledge, which may or may not fit into your personal scenario. I highly encourage readers to take those as references and inspirations, and to start re-imagine and re-strategize your career today with our shared future — powered by AI.

How to determine what jobs are safe/unsafe?

White collar:

Repetition vs. strategic:

Does your job have minimal repetition of tasks? Do you regularly come up with insights that are important to your company? Do you make key decisions that cross functions for your company?

Simplicity vs. complexity:

Do most decisions in your job require complexity or deliberation? In your job, do you need to regularly learn and understand a lot of complex information?

Blue collar

Dexterity vs. repetition:

Does it require at least a year of training to be qualified for your job? Does your job involve very little repetition of the same task(s)?

Fixed vs. unstructured environment

Is your job usually performed in different environments each time? (e.g., a taxi driver would always work in the same taxi) Is your work environment unstructured?

For all jobs: human-contact / empathy / compassion

Is communication and persuasion one of the most important parts of your job?

Do you spend >30% of your work time with people who are not employed by your company (e.g., customers, potential customers, partners) ?

Is a key part of your job performance measured by how well you interact with people?

Does your job result in happiness, safety, or health of those your directly service?

Do you lead or manage people in your job?

Top 10 Safest Jobs from AI

Psychiatrists

Psychiatrists, social workers, marriage counselors are all professions that require strong communication skills, empathy, and the ability to win trust from clients. These are the weakest areas for AI. Also, with the changing times, growing inequality, and job displacements, the need for these services are likely to increase.

Therapists (occupational, physical, massage)

Dexterity is one of the challenges of AI. Physical therapy (such as chiropractics or massage therapy) involves applying very delicate pressures and sensing equally minute responses from the client’s body. In addition, there are added challenges of customizing care for each client, consequences of hurting a client, and the need for person-to-person interaction. The human interaction includes: the ongoing therapy, professional advice, small talk, as well as encouragement and empathy. These aspects make the job impregnable to AI for the short-term.

Medical caregivers (nurses, elderly care)

The overall healthcare industry is expected to grow substantially, due to increased income, greater benefits, AI lowering the cost of care, and aging population (which requires much more care). Many of these reasons will foster a symbiotic environment where AI helps the analytical and repetitive aspects of the healthcare, as more of the healthcare profession shifts to attentiveness, compassion, support, and encouragement.

AI researchers and engineers

As AI grows, there will be a jump in the market demand for AI professionals. Gartner estimates that in the next few years, these increases will outnumber the jobs replaced. However, one needs to keep in mind, AI tools are getting better so that some of the entry-level positions in AI will be automated over time as well. AI professionals will need to keep up with the changes, just like over the years software engineers had to learn about assembly language, high-level language, object-oriented programming, mobile programming, and now AI programming.

Fiction writers

Story-telling is one of the highest levels of creativity, which AI will be weak. Writers have to ideate, create, engage, and write with style and beauty. In particular, a great fictional book has original ideas, interesting characters, engaging plot, surprising emotions and poetic language. All of these are hard to replicate. While AI will be able to write social media messages, title suggestions, or even imitate writing styles, the best books, movies, and plays will be written by humans for the foreseeable future. Entertainment will be a hot area in the era of AI, because we will have more wealth overall, and more free time.

Teachers

AI will become great tools to teachers and education, capable of knowing how to help personalize education curriculum based on each students’ competence, progress, aptitude, and temperament. However, education will be more about helping each student to find what he or she wants, helping hone each student’s ability to learn independently, and being the friend and mentor to help each student to learn to interact with others and gain trust. These are jobs that can only be done by teachers, and require a low student:teacher ratio (like 5:1 or fewer). So there will be many more such humanistic teacher positions created in the future. In fact, a parent may be the best such teacher — if future governments would be wise enough to compensate home schooling parent-teachers. If you are or want to be a teacher, learn more how to connect to your students, one student at a time or in small groups, and less about how to lecture to 50 students.

Criminal defense attorney

Top lawyers will have nothing to worry about their jobs — reasoning across domains, winning the trust of the clients, years working with many judges, and persuading the jury are the perfect combination of complexity, strategy, and human interaction that are so hard for AI. However, a lot of paralegal and preparatory work in document review, analysis, and recommendations can be done much better by AI. And many tasks performed by paralegal: legal discovery, creating contracts, and handling small claims, parking cases, will be increasingly handled by AI. Cost of law makes it worthwhile for AI companies to go after AI-paralegals and AI-junior lawyers, but not the top lawyers themselves.

Computer Scientists & Engineers

As the information age advances, McKinsey report shows that engineering jobs (computer scientists, engineers, IT administrators, IT workers, tech consulting). This high-wage category is expected to increase by 20 million to 50 million globally by 2030. But these jobs require staying up-to-date with technology, and moving into areas that are not automated by technologies.

Scientists

Scientists are the ultimate profession of human creativity. AI can only optimize based on goals set by human creativity. While AI is not likely to replace scientists, AI would make great tools for scientists. For example, for drug discovery, AI can be used to hypothesize and test possible uses of known drugs for diseases, or filter possible new drugs for scientists to consider. AI will amplify the human scientists.

Manager/Leaders

Good managers have human interaction skills including motivation, negotiation, persuasion. Good managers will effectively connect on behalf of the companies to employees, and vice versa. More importantly, the best managers are leaders, who establish a strong culture and values, and through their action and words, cause the employees to follow with their heart. While AI can be used to manage performance, managers will continue to be humans. That said, if a manager is merely a bureaucrat sitting behind a desk and giving employees orders, he or she will be replaced by other humans."
"Joyful Exercises for Contributing to Low-Fat, Lean Muscles, and Dense Bones

Fitness is a passion for me. I do it as a ritual at home nowadays.

I used to go to the gym and love the rituals. However, nowadays, it is difficult for me to go to the gym. Not going to the gym does not mean giving up fitness goals. I created a customised gym at home for my specific needs.

I perform a wide variety of workout regimes. As I get older, the type of exercises changed a lot. I used to do a lot of cardio when I was younger. My main focus is weight training, resistance training, callisthenics, high-intensity interval training (HIIT), and mild cardio.

The purpose of this post to share with you three joyful exercises I perform a daily basis to keep my lean muscles, bones density, and low-fat percentage.

Even though I passionately work out, my approach to exercise is gentle. For example, instead of using too heavy dumbbells, I prefer using my body weight. It is natural and produces the required outcomes for me.

Let me introduce you the three simple exercise I do almost every day to maintain my low-body fat percentage, lean muscles, and dense bones.

Apart from a few hundred push-ups, I use the pull-up machine every day.

Here are the three joyful exercises; enjoy."
"Direct from the investor: 5 lessons for entrepreneurs in 2021

The end of the year is good for balance. And it’s also good for, who knows, tearing up some manuals.

Photo by Andreas Klassen on Unsplash

I consider myself a start-up adviser and investor for beginners. Great apprentice, modesty aside, but with a lot to learn.

Having been an administrator for over 04 years, I reinvented myself in the business world by doing M&As. And, six months ago, I immersed myself in this world that excites and fascinates me as a startup advisor and investor.

At 24, all right. It’s not too bad to feel like a boy. It tastes good. And so, I follow and I will continue to learn.

That said, here are my five boy cents for you, the entrepreneur, to start 2021:"
"Image by natanaelginting on freepik

In the last days, the most recent Instagram update has been in the news for the worst reasons. Many users and influencers have publicly spoken out their dissatisfaction, namely James Charles, who, in a rant video, advised his followers not to update their apps.

At the center of these negative opinions is the new layout, where the basic Instagram options — camera and notifications — have been removed from the bottom menu bar and instead are displayed in the upper-right corner of the home page, next to the direct message button.

Author/Copyright holder: Instagram. Copyright terms and license: Fair Use.

The new bottom menu features the “Reels” (mimicking Instagram’s competitor TikTok) and “Shop” button. This means that these two options are now easily accessible on every screen, while the camera and the notifications button are inaccessible in some tabs.

Instagram is very explicit with its intentions on its blog: to prioritize Reels and its e-commerce platform over traditional posts.

The company states that this change was taken in order to keep up with the fast-paced evolution of technology use patterns. However, this approach seems to be forcing the users to have new intentions when using the app and not otherwise. It even makes me wonder if we are facing a new type of dark pattern.

In my recent post about Dark Patterns, I showed how Instagram uses the Roach Motel pattern. Indeed, we already know this company has no problem in resorting to dark patterns to reach their goals. However, if Instagram is being honest about its intentions with this update, we cannot consider it a dark pattern, right?

The issue is the new bottom menu layout takes advantage of years of muscle memory developed by the users, who now have a high probability of accidentally clicking on the Reels or Shop button when looking for the app’s traditional features.

Certainly, Instagram has a competent UX Team who is able to understand the real users’ needs and motivations. It’s very possible they knew users are not interested in the same features the company is aiming to promote. Maybe this is the reason why Instagram’s official blog post seems to be anticipating users’ disappointment:

Author/Copyright holder: Instagram. Copyright terms and license: Fair Use.

What about you, what do you think about Instagram’s new update? Do you also consider it unfair for users or do you find it a good business decision?"
"Storyboard for Breaking Bad, and we all know how that ended. Image from Uproxx.

What would you say your company does? It seems like a simple question that should have a simple answer, but that’s rarely the case.

Here’s something I’ve done over my 20+ years as an entrepreneur to keep everyone focused on the tasks at hand while also keeping an eye on the future.

Entrepreneurs usually blur the lines of what their startup is, what it will be, and what it should be. This is fine until you try to start planning around those stories. At that point, you need to be asking: What are the priorities today and how do we execute on those priorities without mortgaging the future? The reverse question is just as important: How much time do we spend working on those new things that aren’t generating revenue yet?

The Three Story Rule

Every startup should have three stories, loosely related to the three arcs most storytellers use in episodic storytelling. An easy way to think about it is a television series. When you watch an episode of a TV show, the writers are usually working on three storylines:

Story A: Story with an arc that begins and ends in this episode (or maybe a two-parter).

Story B: Story with a longer arc that lasts a few episodes or more. This current episode will advance the plot of Story B in smaller increments, and maybe drop a twist in here or there.

Story C: Story with a much longer arc, maybe out to the end of the season or the end of the series itself. This current episode might not advance Story C at all, or it may just drop a few hints. At the end of the season or the series, you’ll be able to look back and piece Story C together, but that won’t be easy or even possible in real time.

Now let’s take that story strategy and apply it to your startup, and I’ll use my most recent startup as an example.

Story A: Right Now

Story A is the what your company is doing today that is generating revenue, building market share, and adding value to the company. Story A is about this fiscal quarter, this fiscal year, and next fiscal year.

At Automated Insights, Story A was our story for the first few years while we were known as Statsheet, a company that aggregated sports statistics and turned them into visualizations and automated content. This is how we made our money — either using our own data to generate content or using data like Yahoo Fantasy Football to generate Fantasy Matchup Recaps.

While we were breaking new ground in the arena of sports stats, we were one player in a sea of players, and while automating content from sports stats gave us a competitive advantage, sports was still a highly commoditized and difficult marketplace.

Story B: What’s Next

Story B is what’s going to open up new markets using new technologies or new products. Story B is about what you could do if the stars aligned properly or if you raised enough money for a longer runway, because Story B usually comes with a lot more risk for a lot more reward.

A few years into Statsheet, when we went to raise our Series A round, we pitched using our proprietary automated content engine on all kinds of data, generating machine-written stories in finance, marketing, weather, fitness, you name it. We changed our name to Automated Insights and pivoted completely with a $5 million raise.

That pivot came with a ton of risk. We had friends (and potential acquirers) in sports and we would now be making sports just a part of our story. In return, we would be one of the first players in the nascent Natural Language Generation (NLG) market, a pre-cursor to the “AI” market.

It was not a coincidence that the acronym for our new company name was also AI.

Story C: The Billion-Dollar Story

Story C usually involves a seismic change that disrupts existing markets, and as you can imagine, it’s a million times more difficult to pull off.

Uber and Lyft are on Story C. They’re no longer known as a better taxi or for solving a specific problem. They’re about creating a market in which a large portion of people can no longer live without them. In most urban areas, ride hailing services are now a necessity, as the ability they offer to do more things cheaply has made a major impact on lifestyle. There’s just no going back.

Story C was actually where my vision split from my former startup. I was focused more on real-time, plain-word insights generated from a mesh of private and public data, i.e. Alexa, Google Assistant, and Siri. The company was turning towards more of a B2B approach, first as a SaaS NLG tool, and then as a business intelligence tool.

No one was wrong here, but the latter was the direction the company took. So now I’m working on a new Story A at a new startup. And I’ve got Stories B and C in my purview.

So which story do you tell? Well, it depends on who you’re talking to.

For the press, for customers, and for potential employees, stick to Story A — if these folks aren’t jazzed about Story A, then you’re not spending enough time on Story A.

In fact, you should consider Story B and Story C to be intellectual property. It’s not the kind of thing you want to go too deeply into without an NDA or some protection in place.

For your board, your investors, and your employees, focus on Story A, of course, but also keep them aware of Story B and drop hints about Story C. Story B is where you’re headed next. It might be what you raise your next round on, or it may be your next big pivot. Story C is best kept in the distance until you’ve crushed Story A and made significant progress on Story B. It’s a goal, mainly, and you should just be making sure you’re not closing doors to it as you move forward.

Once you get your stories straight, then it’s just about execution. But come back to them often, every quarter or even every sprint, and make sure everyone is still on the same page."
"1. Multiple Assignment

When you want to give several variables the same values, you will often see code that repeats the assignment statement for each variable.

It may look something like the following code, which uses one separate assignment per variable:

a = 1

b = 1

c = 1

In Python, we can simplify this, and assign to every variable at once.

a = b = c = 1

# a == 1

# b == 1

# c == 1

After doing this, every variable was assigned to the right-most value in the chain. Since it just takes the right-most value, we can also replace 1 with a variable."
"In practice, of course, a given ML application may fit more than one of the above impact categories. An ML algorithm that can analyze documents, for instance, might be reasonably said to increase employee efficiency by helping them process documents faster. But it might also be viewed as a breakthrough since it allows the company to offer new document analysis products it never could before.

Or consider a machine learning algorithm that detects security threats (like guns or knives) in an image generated by a baggage scanner. It may reduce costs by reducing the need for a person to watch a screen constantly. It may increase efficiency by finding more threats. And it may represent a breakthrough by finding threats that humans may have never been able to detect.

Furthermore, “breakthroughs” need not necessarily be associated with new products. What if the breakthrough enables a new kind of insight for the C-suite? Such reporting may lead to strategic choices that ultimately lead to increased revenue or lower costs, even if the immediate impact is not as clear.

Business impact thinking is not meant to rigidly constrain the way you think about machine learning projects. Think of it instead as a useful guide. You should certainly be aiming in the direction of at least one business impact — but you should be flexible enough to know when you need to change the way you communicate your goals and frame your success.

It’s not unheard of for a machine learning project to begin with a goal of reducing costs by automating away entire jobs. But this isn’t always easy. Even if 80% of an employee’s work can be eliminated, the remaining 20% may be stubbornly un-automatable. This doesn’t mean the project was a failure. If the humans can focus on their 20% of the task while being free of the other 80%, they can take on new business. This could represent a massive business efficiency gain — a huge win when the business impact is viewed properly."
"Can’t Sleep? This Tech Could Put You in Sync with the Sun

A new device claims to go beyond sleep tracking to reset your circadian rhythm.

I’m dreaming too much. That’s what Fares Siddiqui, cofounder of the company Circadia, tells me after its sleep tracker spends several nights perched at my bedside. When I first saw the long stretches of REM sleep — the stage of sleep when dreaming happens — in my data, I romanticized the results. I dream big, I thought. But Siddiqui says the pattern is a result of being either sleep-deprived or anxious. Oh.

Circadia is a startup focused on circadian rhythms, and the promise that if you can understand and control your daily patterns, you’ll sleep better: “a sleep lab on your bedside table,” pledges its marketing material. Most sleep trackers — devices on your wrist, on your mattress, or at your bedside — track your tossing and turning along with functions like your heart rate to tell you how much and how well you’ve slept. But typically, they don’t tell you what to do about it.

Siddiqui’s company, funded by healthy Kickstarter and Indiegogo campaigns, is developing a connected tracker, lamp, and app. It aims to set itself apart from the current wave of sleep trackers by offering both information on your own personal rhythm, and customized advice. “We want to tell you what time it is inside of your body,” Siddiqui says. He became passionate about the topic after dealing with his own insomnia and learning about NASA’s light experiments to help astronauts’ circadian rhythms.

Your circadian rhythm, your body’s natural 24-hour cycle, affects everything from sleep and jet lag to hormones and how well your drugs work. But different people’s internal clocks may run a little ahead or behind — maybe it’s midnight in your body when the clock says it’s only 10 pm. For me, I’m hoping some circadian insight can help me feel more refreshed in the mornings. Other circadian-curious people might need to adjust to jet lag or shift work, or identify bad bedtime habits that are keeping them awake.

To learn about my own circadian biology, I let a premarket version of Circadia’s tracking device watch me sleep, I breakfasted by the glow of its lamp, and I gave personal details to its sleep-coaching app. I got an intriguing glimpse into the functioning of my body. But when it came to understanding the significance of my personal patterns, I was mostly left in the dark.

Surfing the wavelengths

In December, if all goes as planned, you’ll be able to buy Circadia’s $129 sleep tracker, which will be integrated with its lamp and app. For now, a rudimentary version of the app is free, and the lamp sells as a standalone for $79 — but it is a very handsome lamp, a sleek cylinder of blue light that morphs to red when you flip it over.

Those wavelengths are intended to reinforce my 24-hour rhythm, helping me sleep at night and be more alert during the day. When I lie down in bed, I’m supposed to leave that dim red light on for half an hour (even though my eyes are closed) to help myself fall asleep. The instructions also say 30 minutes of blue light in the morning will alleviate grogginess, so I eat a few breakfasts with the lamp lighting up my raisin bran.

Groggy mornings? Flip the lamp for blue light, which may help you wake up. Courtesy of Circadia

I can’t tell if it makes me feel more awake, but according to Sabra Abbott, a neurologist at Northwestern University Feinberg School of Medicine, the ability of blue light to promote wakefulness and adjust our body clocks is well established. Blue wavelengths in sunlight naturally help our brains calibrate our clocks by preventing production of melatonin, a hormone that makes us sleepy. That’s why experts tell us not to stare at our phones, which emit blue light, at night in bed.

Abbott uses blue-wavelength light therapy and melatonin to treat patients with circadian disorders, who might naturally fall asleep very late or wake long before dawn. Light therapy is powerful enough that those with circadian disorders should be cautious with its timing, she adds. Someone whose clock is so shifted that they only fall asleep near dawn, for example, could make things worse by using blue light in the morning.

“We want to tell you what time it is inside of your body,” Siddiqui says.

That said, she doesn’t know of any reason to turn on a red lamp while you’re falling asleep. “It’s not so much the presence of red light that’s helpful, but the absence of blue light,” she says. Siddiqui says red light prevents melatonin suppression, which is true — but it’s no better than being in the dark. If you really wanted to take advantage of red light (and didn’t mind the creepiness factor), you could do all your evening activities by red light only. But Circadia’s red light, by design, is too dim for that.

I got yellow-zoned

The second part of Circadia’s setup is a sleep tracker, an elegantly designed hand-sized disc that snaps magnetically onto a stand. Siddiqui says it scans my body with radar looking for tiny movements to infer my heart rate and breathing. From that, it figures out which parts of the night I spent in wakefulness, light sleep, deep sleep, or REM sleep. In the company’s own comparison testing, Circadia outperformed wearable devices like the Fitbit.

I follow directions to set it roughly an arm’s length away from my bed and aim it at my torso. After some fussing on my phone to connect the tracker and app over wifi (so much for avoiding blue light), I hit “start” and lie down.

A radar-based device tracks heart rate and breathing. Courtesy of Circadia

The first night, I feel self-conscious with the tracker staring at me. In the morning the app — a beta version that isn’t yet publicly available — says it took me 43 minutes to fall asleep.

Even after I get used to the tracker, the app seems to chastise me every morning, displaying a circle about half-filled with yellow and a middling “sleep index” score. The app also shows a timeline of my night that seems generally correct: It takes me a while to fall asleep. I sleep deeply at first, then shift into REM sleep in the early morning. As my husband gets ready for work, I alternate between dreaming and dozing.

Siddiqui says that later versions of the app will tell users how their circadian clocks align with the outside world. It will deliver personal recommendations for using the lamp and for changes to habits and sleep environments, so that people can recalibrate their internal clocks, sleep better, or combat jet lag. In the spring, users will also be able to sign up for advice from a human sleep coach or therapist.

For now, my only feedback comes from Siddiqui, who notices me waking up often. He also tells me that while an average person spends about a quarter of the night in REM sleep, for me it topped 40 percent on some nights, and 57 percent on one especially dreamy night. My body may be trying to catch up on missed rest by sacrificing deep and light sleep for extra REM.

But Abbott doesn’t think I should read too much into my results. Sleep tracking is an imperfect way to tell the time on someone’s internal clock; the best way is to measure melatonin production. In its most recent lab tests, Circadia was about 67 percent accurate at telling what sleep stage a person was in. So far, those lab tests have included only a small number of young, healthy males — not anyone with an actual sleep disorder.

Besides, people spend different amounts of time in certain sleep stages for many reasons, including normal variation and drug side effects — antidepressants reduce REM, for example. Circadia claims 1 in 3 people have rhythms that are out of sync, but Abbott says this is hard to know. Everyone falls somewhere on a spectrum from early bird to night owl, she says, which isn’t a disorder unless it interferes with life. But being told by an app that their sleep is abnormal might make people needlessly anxious."
"Did you forget anything recently?

Your keys maybe. Or someone’s name.

You’re in good company. A typical person forgets about 4 things a day. Most are minor lapses but then there are the big ones: a third of us have forgotten a partner’s birthday, and 1 in 5 Dads has forgotten to pick up the kids from school.

A couple years ago, I experienced one of the big ones. I forgot to call my Dad on his birthday. We live in different countries so that call is really important. He expected me to call. And I didn’t. If I close my eyes I can see the disappointment on his face when he went to sleep that night wondering why his eldest child had forgotten about him on his birthday.

Makes me nauseous just thinking about it. The thing is that I had been making constant mental notes about his birthday in the days before, but on the day itself my brain just took a holiday. There was a complete erase of the most dominant thought in my mind.

This act of forgetting, no matter how unintended, feels like a betrayal of the bond between father and son."
"1 || Put your phone to bed early in the evening

We’ve all heard of the psychological effects of using screens before bed, but they’re usually focused on eye strain and blue light — not about how damaging it can be to be followed around by a pestering technological device 24/7. While most studies point to the time spent on technology as irrelevant, it’s important to note how much time you spend thinking about things related to technology.

In the evenings, how often do you spend time thinking about that work project you have due tomorrow, that assignment you need to wrap up, that text you’re expecting, or the emails you have to check? When you’re thinking about those things as much as you do, you might as well go ahead and check your email, finish that assignment, start your workday, or send a text yourself.

“But with no iPhone to keep my mind wired, I was able to tune into my body and fall asleep according to its needs. Every single night of the experiment, I conked out within 10 minutes of getting in bed. And I didn’t make the connection at the time, but my stories were all written well before their deadlines that week.” — Amanda Montell, “The Benefits of Having an iPhone-Free Bedroom”

plugging in your phone/laptop in another room

2 || Don’t start projects too late

Put work away and don’t start something you know you won’t be able to finish. One thing I’ve found having more regular hours for my job as of late is how detrimental it can be to my evenings to haphazardly start work — especially harder projects that aren’t as urgent as I make them out to be. If there’s something big you need to get done, don’t forget to check and see if it could be done later in the week, before work one day, or earlier in the afternoon.

“If you do nothing else, plan each day of your life with intention, purpose, and passion.”― Jeff Sanders

One trick I’ve found useful, one that’s used by the likes of Jeff Sanders, podcaster behind The 5 am Miracle show, is an end time to the work day. Basically, barring an unprecedented event, at a certain amount of time — you stop working. Whether that’s 6 pm or 4 pm, you don’t do work at that point.

You can also change how you define that. Maybe work for your real job isn’t allowed after 4 pm, and you only allow yourself to write your novel or your blog. However, you want to define that, stick to your plan. You may be thinking, though, “I won’t have time for my work” or “I’ll never finish my projects.”

“Regret for the things we did can be tempered by time; it is regret for the things we did not do that is inconsolable.” — Sydney J. Harris, journalist

While that might be the case and you might need to allow more time to get work done, setting a boundary will actually allow you to work more efficiently in the time you’ve allotted for it. Whatever you do, don’t start new work too late in the day. Aim to start early, finish early, and have time to do what you need to do to relax and settle down in the evenings.

3 || Respect a boundary between work and play

One really hard thing, closely tied to projects started too late, is maintaining a healthy boundary between work and play. I have vivid memories of growing up, watching TV with my family, and all of us being on devices, doing work, school, or a personal project that would have been much more enjoyed and efficiently completed without a distracting show in the background.

“W ork refers to the effort someone makes that has value to the person or society or a sustained physical or mental effort to overcome obstacles and achieve and objective or result. Play can be described as any activity someone finds enjoyable and interesting and is valuable in itself for that reason.” — Montessori Child Development Center

Not everything you do needs to be a side hustle. Some things can be done just for fun. One thing that really changed my life for the better towards the end of high school and on into college was the realization that I could write and not worry about doing it for the money.

While I write a blog here as part of my work and am paid for other writing projects, I write fiction for fun — and freeing myself of the hustling aspect has helped me to enjoy myself so much more. It’s also allowed my evening writing time to be spent much more restfully.

4 || Look over/create your plan for the next day

Many people point to the issue of willpower that can become a problem in the morning. If you happen to wake up tired and groggier than usual, having a preset plan to rely on can really save the day. And while you can save some of the planning for the morning of, assessing your current mood and anything that’s come up/come to your mind during your sleep, you can already have a list of things to do, or a calendar with your pre-arranged commitments filled in.

“If you don’t know where you are going, you’ll end up someplace else.”― Yogi Berra

Also, if you happen to wake up a little out of it or otherwise unprepared for the day, you have a plan that you can rely on to get you started — something you’ve thought about beforehand that you can launch into as soon as you wake up. Even if you prefer to plan in the morning, you can at least already have an idea of what your day is going to look like, and leave it to your sleeping self to think of where all the pieces should go.

5 || Enjoy yourself

I know, I know —maybe it’s obvious, but in a world that’s so focused on extreme productivity, going to bed early, and setting yourself apart, it’s difficult to remember that we are designed to relax, reset, and recharge in ways that differ depending on our personality.

Whatever your relaxer of choice is, make sure you make time for it in your evening. That will do you a lot more good than spending an unfocused hour on work or being distracted by email while trying to play with your kids.

“While accomplishing your dreams, don’t forget to enjoy life too.” — Unknown

Whether relaxing is watching a film once a week with your wife, playing a game with your kids before bed, reading a novel, watching a comedy sketch, or talking to an old friend on the phone, find something that fills your soul of and gives you the opposite of stress in your life. It’s worth making time for.

6 || Be realistic with what you can make happen

Let’s be rea,l if you get home from your day job or other vocation-related commitment at 5 pm, and you aim to go to bed at 9 pm every evening, you only have four hours. That’s one hour for dinner, one hour for your spouse, one hour for reading and relaxing and getting ready for bed, and one hour for something else.

“You can do anything, but not everything.” — David Allen

You can’t realistically spend four hours rigorously writing your novel with that kind of schedule. If you can, try and give yourself thirty minutes and really focus on it. That will probably yield much better results and be a much more regular occurrence in your schedule because it fits fairly easily in the time you have.

Some things you’re going to have to reserve for the morning, others for the weekend, and more until you maybe have fewer hours or are on a break/vacation of some sort. Being realistic isn’t always fun, but it’ll yield the best results, give you the least amount of steps, and give you realistic increments of time to do what you need to do.

7 || Reset for the next day

This involves more than just planning for tomorrow. This can be laying out your clothes for the next day, prepping your gym bag so you’ll face less resistance to working out the following day, doing your laundry, cleaning the kitchen, whatever you need to do to create a fresh start for the next day.

[Read: 7 Things You Need To Do To Have Consistently Incredible Mornings]

Whatever you need to do to make tomorrow amazing, make sure you squeeze that into your morning routine. Some go as far as to get a light dimmer for their light switches that will turn on lights in the morning, an Alexa with a routine set to wake you up, or some other system that will alert you to the time and encourage you to start the day.

While in the end, tomorrow lies in the hands of tomorrow, there’s no reason you can’t start preparing for it the day before.

Have a great evening!"
"We need 2 things to be great at what we do.

From a writer’s perspective, we need to learn daily. Eating a slice of the humble pie goes a long way.

Next, we need working tools. We are not perfect. With working tools, we can get better.

Right?"
"History of the Design of O’Reilly Book Covers

In the article “A short history of the O’Reilly animals,” it’s mentioned that in the mid-1980s, O’Reilly used to sell short books on Unix topics via mail order. These were held together by staples and had plain brown covers. As time progressed, Tim O’Reilly (born 6 June 1954, and the founder of O’Reilly Media, formerly O’Reilly & Associates) decided to sell books through brick-and-mortar stores. He hired a graphics designer for the first two titles that were sold in the book stores, but he wasn’t satisfied with them.

Then enters a woman named Edie Freedman into the picture. She was neighbors with a woman who was involved with the company’s marketing and technical writing team. Over a friendly chat, the women discussed the two book covers that had been published by then by O’Reilly, and Freedman was asked if she had some better ideas.

Quoting Freedman directly here from the article:

“ I had heard of Unix, but I had a very hazy idea of what it was. I’d never met a Unix programmer or tried to edit a document using vi. Even the terms associated with Unix — vi, sed and awk, uucp, lex, yacc, curses, to name just a few — were weird. They sounded to me like words that might come out of Dungeons and Dragons, a game that was popular with a geeky (mostly male) subculture. Sometimes when designing, things come together effortlessly — everything falls into place as if it were inevitable. It just flows. As I looked for images for the book covers, I came across some odd-looking animal engravings from the 19th century. They seemed to be a good match for all those strange-sounding UNIX terms and were esoteric enough that I figured they’d probably appeal to programmers. And, as I investigated the attributes of the real animals, I quickly discovered that there were intriguing correspondences between specific technologies and specific animals. That resonance grew and expanded as I learned more about both the technologies and the animals. I was so energized and inspired that I spent an entire weekend working on the covers without much sleep. At the end of the weekend, I gave several sketches to my neighbor to take into the office. “Some of the people at O’Reilly were taken aback: they thought the animals were weird, ugly, and a bit scary. But Tim got it immediately — he liked the quirkiness of the animals, thought it would help to make the books stand out from other publishers’ offerings — and it just felt right. And so it began. We’ve published hundreds of Animal books since then, and the brand is well known worldwide.”

And this has surely worked. O’Reilly books can be recognized distinctively, due to the animals on their cover pages, even in the most crowded of the bookstands. This distinction has contributed immensely to the intangible brand recognition and value systems.

This was the history of the design of the cover pages of the O’Reilly books."
"Build Quality Microservices and Apps

Ensure governance by including these five elements in your requirements document

Photo by Gabrielle Henderson on Unsplash

Fragile apps and microservices are often the results of vague requirements, and developers left to their interpretation and devices.

A well written technical requirements document provides sufficient information to ensure IT governance and implementation consistency across teams. Proper governance leads to better product quality, increased efficiency through clarity, and improved stakeholder collaboration, e.g. testers.

Software quality should be ensured before the coding starts, and not wait for quality assurance (QA) to raise issues to fix—as it would slow down the entire delivery process.

In the following, I’ll share five attributes of good technical API/microservice specification that has served me well during my stint as a developer doing the implementation; and as a tech lead managing the governance and requirements.

#1: Short description and intended callers

Microservices are often designed and built based on bounded contexts. Each microservice at the data layer would be in charge of its own set of tables or documents within a database, e.g. products, users, etc. according to microservice design principles.

In the short description, specify the following:

Database/table names to query — prevent overlapping of queries and joins to other tables managed by other teams/services.

— prevent overlapping of queries and joins to other tables managed by other teams/services. Downstream microservices invoked — applies to logic layer or stateless microservices to communicate and manage dependencies.

With the above information specified, the developer can then communicate with the relevant database and microservice owners to align requirements and bridge implementation gaps.

#2: Microservice details and non-functional requirements

The seemingly minor details such as transport encryption protocols and response headers could potentially lead to deployment issues, e.g. certificate incompatibility, failing security tests, etc. especially in a high-security environment.

Therefore, it’s crucial to indicate such details in the requirements document:

Transport authentication/encryption — specify whether if it’s mutual TLS, HTTP/HTTPS, etc. so that the developer can manage the application configuration and certificates in the codebase.

— specify whether if it’s mutual TLS, HTTP/HTTPS, etc. so that the developer can manage the application configuration and certificates in the codebase. Microservice supported actions — indicate if the application is supporting GET/POST/PUT/DELETE requests to limit redundant exposure of endpoints.

— indicate if the application is supporting GET/POST/PUT/DELETE requests to limit redundant exposure of endpoints. Microservice response headers — ensure that appropriate response headers are returned from the microservice to avoid exposing details that would threaten security, e.g. server type, etc.

— ensure that appropriate response headers are returned from the microservice to avoid exposing details that would threaten security, e.g. server type, etc. Microservice endpoints and query params — provide examples of the specific endpoint pattern, e.g. https://[NAMESPACE_URL]/[SERVICE]/v1/data?{param}={value}, to standardise API endpoint designs.

By indicating such details, it allows the developer to build the application with basic security considerations and governance in place.

#3: Request and response details

The request and response details potentially make up the bulk of the document. The information would also likely be disseminated to other collaborators to assist with service designs, testing, etc.

The key elements to include in this section are:

Request headers — some optional yet important examples include UUID headers to facilitate request tracing and accept-language for localisation.

— some optional yet important examples include UUID headers to facilitate request tracing and accept-language for localisation. Request query parameters — with the attribute name(s), required/optional, data type, and a short description of the attribute.

— with the attribute name(s), required/optional, data type, and a short description of the attribute. Response body — include attribute names (e.g. status, status.code, data, data.uuid), data type, and a short description for the attribute.

I would recommend the above information to be presented in a table format to facilitate readability for non-technical stakeholders. The next section on sample requests/responses will cater to the technical stakeholders.

#4: Sample requests and responses

For certain stakeholders, merely providing the request and response description and attributes would not suffice. Having a sample output response in JSON format enables the reader to quickly contextualise and visualise the desired output of the service with “actual” data.

To facilitate clarity, include the following in the sample request/response section of the document:

Microservice action, URL and headers — a suggestion would be to imagine you’re querying the actual service via CURL and include the essential details.

— a suggestion would be to imagine you’re querying the actual service via CURL and include the essential details. Response with HTTP code — for each HTTP code(s), include the respective response payloads to ensure that errors are properly handled; this is especially essential for services deployed in a mesh architecture.

You can refer to a sample snippet below.

Image by Author

#5: HTTP and business response codes

Considering the high interdependency of services in a microservice architecture, errors would have to be properly handled with appropriate response codes to facilitate debugging and tracing. The list of response codes also helps with testing of the microservice application.

Things to include:

HTTP response codes — details such as code number (403), name (Forbidden), type (Client Error), description (Invalid authorisation credentials).

— details such as code number (403), name (Forbidden), type (Client Error), description (Invalid authorisation credentials). Business response codes — include code number (8989), scenario (access token expired), description (the token is more than one day old).

In most cases, the solution architect/lead would have a list of error codes established during the solution design phase to reference against and standardise."
"Source: Derivative from original on Pixabay, created with photoshop.

Best Ph.D. Programs in Machine Learning (ML) for 2021

These are the best universities to pursue a Ph.D. in machine learning, with research rankings in AI and machine learning research.

Last updated, December 19, 2020

Considering various factors such as the research areas, research focus, courses offered, duration of the program, location of the university, honors, awards, and job prospects, we came up with the best universities to help prospective students choose. This article is most suited for individuals who would like to pursue a Ph.D. with a focus on machine learning and need some guidance on their decision making.

📚 Check out our simple linear regression for machine learning tutorial. 📚

For the list of the universities, please jump at the end of the article.

Note: The universities mentioned below are in no particular order."
"Synergy

Beyond imagination, the human mind is also gifted with the ability to decipher patterns — to make sense out of nonsense.

Especially, we seem to love common denominators.

So much so that the biggest breakthroughs in science have revolved around unification as much as they have around discovery. Routinely, we’ve made paradigm-shattering discoveries by simply tying loose ends together, and we continue to operate under this ambition (it can be said that our next target in line is dark matter).

The greatest minds in history have understood this need for unification to be the ultimate prerogative. Some, like Nikola Tesla, had subsequently failed in their connecting of certain dots while others, like James Clerk Maxwell, had become famous for it.

The problem is that it’s not easy. Far from it.

As clever as we are, we’ve compartmentalized our systems of knowledge into such distinct and divided segments of study that it’s near impossible for one student to embark upon two opposing streams of belief, something that had been the norm only a hundred years ago.

The noösphere promises us a rekindling of this comprehensive approach to understanding our world. With its synergetic potential and it’s touch-point responsiveness, it holds the ability to take all that we’ve chopped up and bring it back together, even if for a moment, just to see if anything blends together comfortably, anything that we hadn’t had, or couldn’t have had, previously considered.

Because, and this is the main point to digest, the noösphere is able to do something that we ourselves have a hard time doing. It can discern and catalogue, cross-boundaries and synthesize streams of information. It can employ numerous algorithms that would take us an absurdly long time to match in terms of efficacy.

Sounds like A.I. doesn’t it?

It doesn’t necessarily have to be, though artificial intelligence will certainly be an integral part of its picture, as it currently is.

The noösphere is the environ. We are the data points.

Twitter lets political discourse unfold in real time. Instagram lets people share their experiences with a taste of immediacy. TikTok, well, it may serve useful in some respect one day.

Quora, Reddit, Wikipedia. All far from perfect, but we’re getting there.

Once we’re able to communicate faster and better and once we’re able to contextualize and idealize more comprehensively than ever before, we’ll see the connecting of a new array of dots that we hadn’t previously thought possible.

Knowledge will come together, under a real singularity, and harmonize itself to a point whereby we’ll have as comprehensive of an outlook as we can imagine.

Whatever this really means (and it may mean many very different things), it will be the milestone of our civilization.

Technologically, socially, environmentally, astronomically, biologically — information will reach the apex of interconnectedness; in so doing, we’ll have the most informed understanding that there can possibly be (correlating to our rate of new discoveries) at any given time.

Our segregation of various fields of study will no longer be isolating; our subjective experiences and insights will no longer be so subjective; our vision will no longer be obstructed by division.

The singularity of knowledge — it’s already happening, but it’s about to speed up to rates we won’t even realize until we’re able to look back on it.

Our only obligation, it seems, is to nurture this process rather than standing back and watching it unfold on its own under the presumption of a far-and-away singularity that we don’t have enough time or imaginative power to consider.

In essence, we are the singularity."
"Tools for using Kubernetes

Tools for a team of any level to realize a container architecture.

Kubernetes, the container orchestration tool originally developed by Google, has become a defacto for Agile and DevOps teams. With the advance of ML, Kubernetes has become even more important for an organization.

Here, we have summed up a list of tools which can be used to realize a container architecture for different phases and maturity levels for enterprise organizations.

Kubectl

The most important area for Devops is command line. Kubectl is the command line tool for Kubernetes that controls the Kubernetes cluster manager. Under Kubectl, there are several subcommands for more precise cluster management control, such as converting files between different API versions, or executing container commands. It is also the basis of many other tools in the ecosystem.

kuttle: kubectl wrapper — Kubernetes wrapper for sshuttle

kubectl sudo — kubernetes cmd with the security privileges of another user

mkubectx — single command across for all your selected kubernetes contexts

Kubectl-debug — Debugging the pod by a new container with troubleshooting tools pre-installed

Minikube

The next important area is development. Minikube is a great Kubernetes tool for development and testing. Minikube is used by teams to get started on and build POCs using Kubernetes. It can be used to run a single-node Kubernetes cluster locally for development and testing. There are plenty of Kubernetes features supported on Minikube, including DNS, NodePorts, ConfigMaps and Secrets, Dashboards, Container Runtime (Docker, rkt, and CRI-O), enabling CNI’s, and ingress. This step-by-step guide for a quick and easy installation.

KubeDirector

Once the team has build extensively, it will need to scale out the clusters. It brings Enterprise level capabilities for Kubernetes. KubeDirector uses standard Kubernetes facilities of custom resources and API extensions to implement stateful scaleout application clusters. This approach enables transparent integration with user/resource management and existing clients and tools.

Prometheus

Each team has a need for operational metrics to define operational efficiency and ROI. Prometheus can be leveraged for providing alerting and monitoring infrastructure to Kubernetes native applications. Prometheus, a Cloud Native Computing Foundation project, is a systems and service monitoring system. It collects metrics from configured targets at given intervals, evaluates rule expressions, displays the results, and can trigger alerts if some condition is observed to be true.

Prometheus provides the infrastructure but for metric analytics, dashboards and monitoring graphs, Grafana is used on top of Prometheus.

Skaffold

Once the team has spent time building a repeatable process for containerization with metrics and alerting, having CI/CD becomes the next phase of development. Skaffold is a command line tool that facilitates continuous development for Kubernetes applications. It helps the team to iterate on the application source code locally then deploy to local or remote Kubernetes clusters. Skaffold handles the workflow for building, pushing and deploying your application. It also provides building blocks and describes customization for a CI/CD pipeline.

CI/CD will require test automation as well. The test-infra repository contains tools and configuration files for the testing and automation needs of the Kubernetes project.

KubeFlow

Once the products gather huge amounts of data, data pipelines and data products could be build for these applications. Kubeflow is a Cloud Native platform for machine learning based on Google’s internal machine learning pipelines."
"Archives of Collections — Volumes

Stories on ILLUMINATION-Curated — All Volumes

Easy access to curated and outstanding stories

Photo by Syed Hussaini on Unsplash

ILLUMINATION-Curated is a unique collection, consists of edited and high-quality stories. Our unique publication hosts outstanding and curated stories from experienced and accomplished writers of Medium.

We compile and distribute stories submitted to ILLUMINATION-Curated daily. Our top writers make a great effort to create outstanding stories, and we help them develop visibility for their high-quality content.

The purpose of this story is to keep volumes in a single link for easy access. As a reference material, we also provide a link to all editorial resources of ILLUMINATION-Curated in this post.

Our readers appreciate the distribution lists covering stories submitted to ILLUMINATION-Curated daily. The daily volumes make it easy to access the articles and discover our writers. Some readers are closely following specific writers that they found in these circulated lists.

This archive version can be a useful resource for researchers and those who are studying specific genres. We cover over 100 topics.

This story allows our new writers to explore stories of our experienced writers and connect with them quickly and meaningfully. ILLUMINATION-Curated strives for cross-pollination. Writers learn from each other by collaborating. Our writers do not compete; instead, they enhance and extend each other’s messages.

Customised Image courtesy of Dew Langrial

07 December 2020

06 December 2020

05 December 2020

04 December 2020

03 December 2020

02 December 2020

01 December 2020

30 November 2020

29 November 2020

28 November 2020

27 November 2020

26 November 2020

25 November 2020

24 November 2020

23 November 2020

22 November 2020

21 November 2020

20 November 2020

19 November 2020

18 November 2020

17 November 2020

16 November 2020

15 November 2020

14 November 2020

13 November 2020

12 November 2020

11 November 2020

10 November 2020

09 November 2020

08 November 2020

07 November 2020

06 November 2020

05 November 2020

04 November 2020

03 November 2020

02 November 2020

01 November 2020

30 October 2020

29 October 2020

28 October 2020

27 October 2020

26 October 2020

25 October 2020

24 October 2020

23 October 2020

22 October 2020

21 October 2020

20 October 2020

19 October 2020

18 October 2020

17 October 2020

16 October 2020

15 October 2020

14 October 2020

13 October 2020

12 October 2020

11 October 2020

10 October 2020

09 October 2020

08 October 2020

07 October 2020

06 October 2020

05 October 2020

04 October 2020

03 October 2020

02 October 2020

01 October 2020

30 September 2020

29 September 2020

28 September 2020

27 September 2020

26 September 2020

25 September 2020

24 September 2020

23 September 2020

22 September 2020

21 September 2020

20 September 2020

19 September 2020

Editorial Resources About ILLUMINATION Curated"
"You work really hard for years building your company. And you’re burning through your savings as you build your company.

Picture: Depositphotos

Finally, finally you get your company to cash flow positive.

“Thank goodness,” you say to yourself. “We’re finally free of needing more money.

“Now, the business is self-sustaining. We can just invest the profits of the business back into the company.”

So it’s a rude shock when you realize that your company isn’t truly profitable even though your company is cash flow positive.

How can this be? Cash flow positive means you don’t need money any more, right?

Let me tell you about my friend Mark.

I met Mark a couple of years ago. He has a really cool company that he and his business partner started. They received some angel funding that helped them, but they are truly bootstrapping.

I love their business and their business model. Their product is unique. And, slowly but surely, Mark’s company has gained traction.

I tell Mark the same thing every time I see him. “When are you and your partner going start taking a salary?”

Mark’s answer is the same each time, “When we’re profitable.”

I wish I could get Mark to change his mind, but I haven’t been successful, yet.

You’re only truly profitable when you and your company are cash flow positive.

Congratulations. Your company is profitable, but you’re still draining your bank account.

Guess what? You haven’t achieved true profitability yet.

You’ve achieved true profitability when you are no longer draining your personal bank account of money.

Somehow, there is this misconception that your investors expect you to starve while you build your company. Nothing could be further from the truth.

Experienced investors know that it’s important for you to make a living wage. In other words, your investors want you to have a big enough salary, so you don’t have to worry about paying the bills each month.

Now, I’m not saying that you should pay yourself a huge salary. That doesn’t make sense.

However, I am saying that you should, as soon possible, pay yourself enough money, so you can pay your bills.

Why you should pay yourself a living wage.

I recommend to every entrepreneur I work with that they pay themselves something as soon as possible. Just pay yourself something. Even $100 per month is okay if that’s all you feel comfortable with.

The benefits of paying yourself a small salary go beyond the small amount of money you will make. Let me explain why, except this time I will use a negative example.

There was another entrepreneur I worked with named “James”.

James didn’t pay himself a salary.

We were going through our bi-weekly review of his company. The revenue was growing, and the company should have been cash flow positive.

In fact, cash from operations was growing, so it didn’t make any sense why James’ net cash position was dropping.

Then James gave me the answer. “My wife wants me to repay the second mortgage on our home.”

“But there’s no loan on the books,” I said. “You can’t just take money out of the company. You have shareholders.”

‘You don’t understand. We have to pay off that mortgage.”

“I understand what you want to do, but you can’t do it that way. You’re embezzling money from your company.”

You don’t want to put yourself in the position where you will be tempted to do the wrong thing.

I instantly knew I would have to stop working with James because James was embezzling money from his company.

I was bummed.

I had been working with James for a while. And James’ company had gotten to a nice amount of revenue and was cash flow positive.

James was going to blow it big time if he didn’t change his thought process. James wouldn’t change his mind, so I told James our business relationship was over.

I’m not saying that not paying yourself a salary will result in you doing what James did. Mark is proof of that. But why put yourself in a bad financial position?

Instead start the discipline of paying yourself a small salary.

You can start with as small an amount as you want, then…

Start paying yourself more as the health of your business improves, then…

Keep increasing the amount you are paying yourself more until you are paying yourself a living wage.

Disciplined cash management leads to better results for your company.

I am a big believer in being what I call “Appropriately Frugal” when you are the CEO.

Simply put, being Appropriately Frugal means that you spend money on the important stuff for your business, and save money on everything you can save money on.

However, you and your employees are not the area to save money on.

You do want to attract the best employees. Then you will need to pay them appropriately.

Again, I’m not saying that you should pay your employees crazy salaries. But I am saying that you should pay your employees market rate. Pay your employees as much as you can if you can’t afford market rate:

Your employees will feel appreciated, and…

Your employees will not have to worry about their finances.

The idea that your employees will accept less than market rate because you are just starting only makes sense if they can afford it. Otherwise…

You will not retain your employees, or…

You will not hire the best employees, or…

You will not hire any employees at all.

So why aren’t you willing to pay yourself if you are willing to pay your employees?

Why are you any different than your employees?

You are the most important asset your company has. You will not be at your best if you are constantly worrying about how you are going to pay your bills each month.

Just remember that true profitability comes when your company AND you are cash flow positive.

And Mark, I know you’re reading this post. I hope you have decided to pay yourself something. If you haven’t decided to pay yourself something, then I hope this post helps to change your mind.

For more, read: www.brettjfox.com/are-you-being-appropriately-frugal-and-why-its-so-important/"
"Google has recently unveiled 3 separate efforts to bring technology to those with disabilities to help make their daily lives easier and more accessible. The three projects are Project Euphonia, which aims to help those with speech impairments; Live Relay, which assists anyone who is hard of hearing; and Project Diva, which aims to give autonomy and independence to people with the help of Google Assist.

More than 15% of people in the United States live with a disability, and that number is only expected to grow in the years ahead as we grow older and start living longer. There has never been a better time to try to harness the power of our technology to help make the lives of the disabled more comfortable and fulfilling.

Project Euphonia

Project Euphonia aims to help those with speech difficulties caused by cerebral palsy, autism, and other developmental disorders, as well as neurologic conditions like ALS (amyotrophic lateral sclerosis), stroke, MS (multiple sclerosis), Parkinson’s Disease, or traumatic brain injuries. Google’s aim with Project Euphonia is to use the power of AI to help computers understand speech that is impaired with improved accuracy, and then, in turn, use those computers to make sure everyone using the service can be understood.

Google has partnered with the ALS Residence Initiative and the ALS Therapy Development Institute to record voices of men and women with ALS, and have worked on optimizing algorithms that can help to transcribe and recognize their words more reliably.

Live Relay

Live Relay was set up with the goal of bringing voice calls to those who are deaf or hard of hearing. By using a phone’s own speech recognition and text-to-speech software, users will be able to let the phone listen and speak on their behalf, making it possible to speak to someone who is deaf or hard of hearing.

Google also plans to integrate real-time translation into their Live Relay software, allowing anyone in the world to speak to one another regardless of any language barrier.

Project Diva

Project Diva helps those who are nonverbal or suffer from limited mobility to give Google Assistant commands without needing to use their voice, but instead by using an external switch device.

The device is a small box into which an assistive button is plugged. The signal coming from the button is then converted by the box into a command sent to the Google Assistant.

For now, Project Diva is limited to single-purpose buttons, but they are currently devising a system that makes use of RFID tags which they can then associate with certain specific commands.

This article was originally published on RussEwell.co"
in In Fitness And In Health
"Stuff You Should Know

For random knowledge, SYSK is the place to go. This award-winning podcast comes from the writers over at HowStuffWorks and is consistently ranked in the top charts. Every Tuesday, Thursday, and Saturday, Josh Clark and Charles W. “Chuck” Bryant educate listeners on different topics.

No matter the topic, they always cross-connect with pop culture. Want to learn how going to the moon works? How yawning works? What prison food is like? After lots of time listening, you’ll end up feeling like you’ve completed a degree in Out-Of-Left-Field Things.

Business Wars

There are fascinating stories behind many of the household-name companies and products that we all know. Business Wars host David Brown takes you through the audible journeys that brought many of these companies and products to what they are today.

Grasp the details of how Evan Spiegel grew Snapchat to go head-to-head with Facebook, or listen to the battle in the chocolate market between Hershey and Mars. The use of great sound effects and creative narration by this Wondery podcast makes the listening experience comparable to watching a documentary.

Reply All

For tales that keep you listening, tune in to Reply All. Focused on how people shape the internet and how the internet shapes people, hosts PJ Vogt and Alex Goldman have lively discussions about random yet intriguing situations and dig deep.

One episode, The Snapchat Thief, is about how the identity of a Snapchat account hacker was investigated and (spoiler alert) eventually found. Another episode, called Adam Pisces and the $2 Coke, is about the occurrence of a flood of strange Domino’s Pizza orders. Each segment is about 30 to 45 minutes long, a good length for the average commute.

How I Built This with Guy Raz

Chances are, you’ve at least heard about HIBT. Produced by NPR, this is a podcast about the stories behind the movements built by entrepreneurs, innovators, and idealists. Each weekly episode is 30 to 60 minutes of conversation between host Guy Raz and a notable guest.

You can hear about the origins of Atari (and Chuck E. Cheese) from Nolan Bushnell himself, and about how Sara Blakely founded Spanx. You can listen to Drybar’s Alli Webb, or to Haim Saban’s story about Power Rangers. If you want to learn about the in-depth process and interesting hurdles that go hand-in-hand with groundbreaking success, you’ll enjoy this.

Every Little Thing

Similar to Stuff You Should Know, ELT is a goldmine for random facts. As the host, Flora Lichtman takes you through some of the most pressing questions out there. How are new stamp designs created? What are dogs saying when they bark, and why do auctioneers talk so fast? How do you make that pumpkin spice flavor we all know?

This podcast also has a wide variety of invited guest speakers. In one segment you can hear from an airline pilot, and another you can learn from a microbiologist. If you’re someone who likes to learn something new every day, these segments have you covered.

Syntax.fm

If you happen to be a hardcore tech geek or want to get accustomed to tech lingo, you’ll love Syntax.fm. The hosts, Scott Tolinski and Wes Bos, teach web development for a living, so they have a wide range of tech fluency, from JavaScript to CSS to React to WordPress. Although niche, these are topics that influence the work of many.

They have unique segments like the Spooky Stories episodes, through which you can hear about moderately-disastrous tech-related incidents. They also discuss more general topics, like design foundations for developers and how to get better at solving problems. Episodes are light-hearted and full of awesome info.

The Pitch

If you’re a fan of Shark Tank, you will enjoy tuning in to The Pitch. The show, hosted by Josh Muccio, features entrepreneurs who are in need of venture funding and pitch investors, live. The goal is to give listeners an authentic look into what it’s really like to get involved with venture capital.

You’ll hear from one entrepreneur per episode, so you’ll get into the details. You’ll hear stories about new businesses, post-pitch pivoting, and will even get to follow folks through their journey months after their pitch."
"It’s always interesting to wonder how much our ancestors, predecessors, and younger selves knew where they were going. But equally fascinating, in my opinion, are those bold predictions from the past that hit completely wide of the mark. Not only is it a neat insight into the way the minds of the past considered their place in human history, but it serves as a reminder that no matter our achievements, we can never gauge our societal momentum with any real exactness.

2020 has been an eventful and chaotic year, so I figured I would turn my ear back to the voices of the past and delve into a little alternate history. It seems that those voices had a lot of ideas about what 2020 in particular might look like. Prophecies aren’t interesting, so I made sure to stick to considered, thoughtful predictions made by futurologists, writers, engineers, scientists, and other trend forecasters.

These are the unmet expectations, forgotten dreams, and unrequited wishes for the 2020 that never was.

A 26-Hour Work Week

Photo by You X Ventures on Unsplash

This one is probably the most disappointing. In 1968, physicist Herman Kahn and futurist Anthony J. Weiner predicted that by 2020 the average American would be working 26 hours per week- about 1370 per year. It was a pretty bold prediction considering the average American worked approximately 37 hours per week in 1968. And it speaks to the optimism of the Post-War period that envisioned a future of linear progress and continuous economic growth. As it stands, the average American now works roughly 35 hours per week according to the Bureau of Labor Statistics- and that figure varies according to factors such as gender, age and marital status (the average for men is 41 hours for instance). It also doesn’t include “side hustles” which many modern Americans increasingly feel the need to support themselves with. The U.S also has a relatively high figure of 11% of its employees that work over 50 hours per week (according to the OECD).

Sadly, the idea of a 26-hour work week seems less realistic now than it did in the 1960s- and not just for Americans. But if any country is going to get close to making it less of a fantasy, it will be a progressive nation like Denmark, Norway, or The Netherlands.

Humans Will Land On Mars

Photo by Nicolas Lobos on Unsplash

Although this prediction is, ultimately, wrong- it’s not far off. The idea that we would send human beings to Mars by 2020 is something I remember growing up with, in fact. Humans setting foot on Mars by the early 21st century was a recurring promise in the books and documentaries I consumed as a kid. In a 1997 issue of WIRED, Peter Leyden and Peter Schwartz gave 2020 as the year we would finally succeed in sending a manned spacecraft to the Red Planet. We’re on our way, having successfully landed several robotic craft (such as probes, rovers, and landers), but current estimates for a manned mission put it a good decade hence.

What’s most interesting about Leyden and Schwartz’s prediction however, is not that we would reach Mars by 2020, but that we would do so as part of a “joint effort supported by virtually all nations on the planet”. They describe four astronauts from a multinational team beaming images of the Martian landscape back to 11 billion people- which is also interesting, as the most recent United Nations estimates for the world population (as of September 2020) sit at 7.8 billion, with 10 billion not expected until 2057.

The beaming of those images are an important part of the prediction though, and tell us that this was as much a prediction about sociology as it was scientific discovery. The images that never were beamed to us this year have an emotional weight to them. Leyden and Schwartz envisioned the 2020 Mars landing as being a turning point in history, a triumph of global cooperation that would put an end to an Earth divided by nations and give rise to a more collective mindset.

“The images from Mars drive home another point: We’re one global society, one human race. The divisions we impose on ourselves look ludicrous from afar. The concept of a planet of warring nations, a state of affairs that defined the previous century, makes no sense.”

It’s poignant to think that this, rather than our technical capabilities, has proven to be the most unrealistic aspect of their prediction. It makes me think of classic science fiction from the Cold War era (think Gene Roddenberry’s Star Trek or Poul Anderson’s Tau Zero) in which a future spacefaring Earth always had a single identity. Nation-states were gone, but cultural identities were never lost. Ethnic and religious conflicts were seen as archaic. Although it may seem far away right now, there is hope in the idea that through technology we can achieve social progress.

The Death of Nationalism

Photo by Jørgen Håland on Unsplash

This one ties in quite nicely to the previous prediction. If you think about it, they’re essentially the same: through advances in technology, we can overcome national and ethnic divides, and come together as one. In 1968, political science professor Ithiel de Sola Pool confidently proclaimed that “By the year 2018 nationalism should be a waning force in the world,” due to our enhanced capabilities for translation and communication. While it’s true that the internet has facilitated a more interconnected world, our technical innovations haven’t brought about the greater empathy de Sola Pool hoped for. Quite the opposite, in fact. Trump, Brexit, Bolsonaro, Erdoğan, Orbán, the Front National, and the Alternative für Deutschland were and are driven by a viciously-xenophobic, fervently anti-intellectual brand of populist nationalism.

The question that remains is whether de Sola Pool’s prediction was wrong entirely or whether it was simply premature. If we are to think of human history in terms of Hegelian Dialectics, then the process of nationalism’s erasure could very well be underway. It’s just not a smooth and linear process. Rather, it’s a messy, generational progression of “two steps forward, one step back”. The French Revolution deposed a tyrannical monarchy but led to a little something known as The Terror, and from that chaos emerged a new tyrant in the form of Napoleon- a political opportunist who derailed the very liberty he professed to love. It was a good half-century before the fruits of the Revolution came to bear insofar as individual liberty was concerned. By that same token, the rise of Trump, Brexiteers, and those like them could be the last fightback of populist nationalism as the world moves inexorably to a more interconnected and interdependent future. The more they swing in one direction, the likelier it is that the next generation of policymakers will move to compensate. My point being, we won’t know for definite that de Sola Pool was off the mark until many years hence.

Hyper-Intelligent Apes Will Be Our Personal Slaves

Photo by Margaux Ansel on Unsplash

No, I’m not kidding. During my research for this article, this was the prediction for 2020 that seemed to crop up the most in my internet searches. Probably because people can’t quite believe that this was a serious prediction for the world in which we now live. In 1967 The Futurist published an article that stated “By the year 2020, it may be possible to breed intelligent species of animals, such as apes, that will be capable of performing manual labor.”

According to the writer this included everything from vacuuming the house to pruning the rosebushes, and even driving our cars. These apes, which would be specially-bred and trained as chauffeurs, would supposedly reduce the amount of car crashes. Now I’ve never seen a chimp drive a car outside of a circus, so I can’t attest as to whether or not they would be more adept at spotting potential hazards on the road than we are. But these aren’t just any old apes- the article implies they’re a kind of super-ape, bred for specific purposes in the same manner as dogs. Alas these apes don’t exist, but the basic idea that by 2020 we will use our enhanced technology to find new uses for animals is not incorrect. Scientists and mechanical engineers at Singapore’s Nanyang Technological University have recently experimented with the creation of “cyborg insects”, successfully implanting electrodes into the leg-muscles of beetles in order to control how they move. These remote-control bugs- far cheaper than robots of the same size- can theoretically be put to a number of uses- from espionage to search-and-rescue. It’s not as impressive as a baboon trying to scrub dried oatmeal from a breakfast bowl, but it’s in the spirit of things.

Telepathy & Teleportation

Photo by David Clode on Unsplash

Perhaps the most surprising aspect of this prediction is not so much that it exists, but that it was made as recently as 2014. Michael J. O’Farrell, founder of The Mobile Institute and veteran of the tech industry, proclaimed in the 2014 book Shift 2020 that both telepathy and teleportation will have been made possible by the current year. This breakthrough was supposed to have been achieved through a process known as “nanomobility”.

O’Farrell writes that “By 2020, I predict people will become incubators for personally controlled and protected Embodied Application Platforms and Body Area Networks, with a primary source-code Physical State and hyper-interactive genetically reproduced Virtual States. All states would host a mass of molecular-sized web-servers; IP domains and AP transport protocols capable of self-sustaining replication, atomically powered quantum computing and persona-patented commerce. I have coined the phrase nanomobility to capture and describe this new uncharted state.”

So what’s the modern reality of telepathy and teleportation?

Well the truth is that they simply don’t exist- at least, not in the way we typically imagine these concepts. The closest we’ve gotten to telepathy is electro-encephalography (EEG), in which a device not dissimilar in shape to a swimming cap is outfitted with large electrodes and placed upon the scalp of the subject. These electrodes record electrical activity which is then interpreted by a computer. Scientists have used this interface to both send signals from the brain and receive electrical pulses in turn. Volunteers have been able to transmit brain activity to each other, to computer software, and even to animals- with one volunteer able to stimulate the motor area of a sedated rat’s brain in order to get it to move its tail.

The closest scientists have come to something resembling teleportation is a process known as quantum teleportation, which is less an act of transportation so much as it is communication. Quantum information has been proven capable of transmitting from one place to another. In 2014, researchers at the Technical University Delft reported having teleported the information of two entangled quantumbits three meters apart. These breakthroughs may not have impacted our everyday lives in the way that the futurists’ hoped, but they are nonetheless extraordinary accomplishments that we can only hope will serve as part of a greater journey of discovery."
"Why you should never agree to use teleportation

Spoiler: because it’ll probably kill you…at least for a little while.

If you’ve seen any sort of science fiction movie — you’ve probably come across the notion of teleportation. The ability to instantly be transported from one side of the planet — to the other.

Imagine a world where you could be in Paris for breakfast, Buenos Aires for lunch, and the newest restaurant on the moon for dinner. Pure fantasy right?

It may have been fantasy…until 2018 anyway.

Scientists in China successfully teleported a photon from Earth onto a satellite 300 miles away. This moved the concept of teleportation from being impossible to simply being a herculean endeavour.

Before we start tasting that freshly baked French bread each morning — we first need to work out how to teleport larger particles, small inanimate objects, “lesser” forms of life, and finally humans.

That is to say nothing of the seemingly astronomical amount of computing power and transmission bandwidth we will need to be capable of harnessing in order to teleport a human.

One day, a century or two from now, this technology will be mature. The question then arises — should you use a transporter, or will it mean your instant death with your life being taken over by a doppelganger? How do you know that whoever steps into the transporter is the same person who steps out?

Let us consider four ways in which a transporter might work, and whether that would mean that “you” come out the other end or a copy.

Facsimile

Body Transmission

Mind Transmission

Wormholes

Facsimile

Your body is scanned by the teleporter in your lounge room and deconstructed. You are reprinted at the destination with new “ink”.

Whilst atomically (and genetically) identical — the person at the destination would be a copy as the base materials used are different “instances” of those elements. You, of course, are dead — and will stay dead.

To demonstrate with another example — imagine transporting a house from point A to point B using this method. The house in point A has been destroyed, and while the bricks being printed in Point B look identical — they are mere copies.

Body Transmission

Your body is scanned, and deconstructed into its constituent “Lego blocks” (read: atoms). These same blocks are then fed through some sort of pipe (or via quantum entanglement) and drop out at the destination — where they are reassembled into yourself.

Unlike the previous example, the very same atoms in the original you have made it to the destination.

In this scenario — you were definitely killed but were you brought back to life and consciousness. Or was a new instance of your consciousness that was “booted up”?

Does it even matter if it’s a different instance of consciousness?

Mind Transmission

Your body is scanned. A replica is reprinted at the destination — including all the data in your brain (memories, facts, relationships, and neural pathways).

The electrochemical impulses that course through your brain are transmitted (similar to a data file over Bluetooth or wi-fi) and into your new brain.

This way, while the body is new, the original “spark of life” has been transmitted over to Point B. The consciousness of the individual may have effectively just blanked out (as you would under a coma or deep sleep) for a few milliseconds.

Wormholes

The teleportation device creates and opens a wormhole under your feet that creates a tunnel through space-time, with the other end of the wormhole terminating at your destination.

In this way — you and your atoms remain wholly intact, and you effectively walk through a door or get onto a slide which takes you to where you need to go.

This solution saves you from any death and preserves the continuity of your consciousness."
"Creating Good UX for Better AI

How to design a product that benefits both the user and the AI model

As you’ve probably noticed, Machine Learning and Artificial Intelligence are here to stay and will continue to disrupt the market. Many products have inherently integrated AI functions (i.e., Netflix’s suggestions, Facebook’s auto-tagging, Google’s question answering), and by 2024, 69% of the manager’s routine workload, will be automated, as Gartner forecasts.

A lot of work has been done around designing products that make AI accessible for users, but what about designing a product that improves the AI model? How does UX approach the development of better AI?

I’ve always been very excited about AI, and for the past couple of months, I’ve been working on the Product Management and UX of several highly technical and advanced AI products. In my experience, bridging the gap between the science behind Machine Learning(ML) and the end-user is a real challenge, but it’s crucial and valuable. Humans have a huge responsibility when it comes to teaching the different models — it can either turn into something great or go horribly wrong.

In this article, I will focus on the two sides of an AI product, and then combine them into one approach that will benefit both the end-user and the ML model.

So, first, let’s focus on the two sides of the experience:

User-centered design Model-centered design

After becoming familiar with these, I’ll combine them into one Machine Learning Experience — Model-User Design.

User-Centered Design — Creating a good product

User-centered design is the shared goal of everyone interested in UX. If the product is centered around a real user’s needs, it is far more likely to create a product-market fit and generate happy customers.

AI is pretty new to people. Many people are afraid of it for many reasons — from giving false predictions to taking away their jobs (not to mention their lives, but that’s some Terminator stuff). That’s why creating a good experience for the user is crucial.

There are a couple of tools we can use in order to create a good experience in AI products. We’ll cover some of them, including finding the right problem to solve in order to provide value, how to explain the model running “under the hood”, keeping the user involved in the learning process and preparing for mistakes.

Find a good problem to solve

The basic rule of product-market fit, which applies to all other products, applies to AI. For the product to succeed, a real problem needs to be solved. If we create the most complicated state-of-the-art AI product that predicts the flying route of a fly, that would be a great model, but no problem is being solved and no value is being created. AI should add value to users and optimize the way they work.

“The only reason your product should exist is to solve someone’s problem.” — Kevin Systrom, Co-Founder of Instagram

Explainability

Explainable AI explains what AI does to the user. The user has the right to understand why the algorithm predicted something. Explaining the why creates a more reliable connection and a feeling of trust. There are many examples such as product content suggestions on Netflix and YouTube — “Because you liked X:”, or “Based on your watch history:”.

These sentences make you understand why Netflix suggested Ozark — because you watched Breaking Bad!

You should also be aware that it’s not just about the experience, but that it’s a regulation ‘thing’. GDPR includes the right of an individual to ask for a human review of the AI’s prediction, to understand if the algorithm has made a mistake.

Control & User feedback

We should keep in mind that the model doesn’t always know what’s best for the user, and that users should feel they have the power to affect the model and “teach” it. For example — create opportunities for the user to provide feedback if the prediction is right or not.

These types of messages enable feedback from the user, which will eventually help the prediction improve.

Prepare for mistakes

An AI algorithm won’t be 100% correct all the time. That’s why the algorithm should be able to project its confidence in a prediction —if a prediction isn’t very confident, the user should know about it and take it with a grain of salt. Also, be ready to handle mistakes and errors. The user is more likely to accept mistakes in AI if they are followed with an explanation of why the model came to its prediction (as mentioned before — explainability!). This statement should also be followed by information on how to improve the model in the future.

It’s really important to remember AI has a huge impact on people’s lives. That’s why AI models’ predictions and mistakes have a colossal effect on people’s lives — wrong predictions may be highly offensive to the user (e.g., Google’s horrible false classification) or cause physical damage and even death (e.g., accidents made by self-driving cars).

Model-Centered Design — Creating a good AI

Now that we’re aligned about what user-centered design is, let’s talk about how to make the design centered around the ML model — how to improve the model and make the learning process as efficient and beneficial as possible.

When we talked about user-centered design, our goal was to make the model understand the user. Now, let’s try to make sure the user understands the model.

To make this generic and straightforward, let’s establish a very high-level flow of the machine learning process:

In order to think about Machine Learning Experience, let’s forget for a second what we know about user interface components. Let’s talk about the process and how it meets humans.

Training a model

The training part of the ML model is essentially taking a lot of data and uploading it so that the algorithm can learn from it. Let’s say we want to train a model to identify lemurs in pictures. A training process can include uploading 1,000 images, some labelled and some not. Then, waiting for the model to learn. At the end of the process the model will be trained and can identify a lemur!

As users, we’d like to make sure the algorithm learned. That’s why it’s important to visualize and clarify the training process — things like the accuracy of the model, the number of epochs that it took for it to learn, etc.

Also, if we want to make sure the model works as we want it to, we can move to inference phase.

Inference

In this part, we’d like to test the understanding of the model. Inferring, to put it in very simple words, is pressing the “run” button on the AI model, with a given input. If we take the lemur example from before, at this point, we would upload a picture and check that the model understands what a lemur is and what isn’t.

After seeing the result, the user should have the ability to provide feedback, so the model will learn and improve.

Monitoring

In order to make sure the model is performing well, monitoring is needed. It’s essential to understand the relevant metrics in order to monitor the model well. For a deeper understanding of the subject, I highly recommend reading this article:

Model-User Design — Creating a good AI Product

Now, when we know both sides of the AI-Product equation, we’re able to identify the guidelines for creating a good AI product:

When thinking about the product’s users, we need to take into consideration the ML researcher who will feed and train the algorithm. With that in mind, we have some key takeaways:

Quality Control — Help the user understand the model

To give good predictions and provide an actual value, the top motivation for the ML researcher is to make sure the algorithm is as accurate as possible. For that to happen, we need the user to have comprehensive understanding of the model’s inputs and outputs. e.g., users should understand the importance of labelling training data and giving feedback to the predictions. The better users understand the important metrics of the model, the better they’ll be able to improve the model and get better results. In other words, in order to improve the model, users need to understand the “needs” of the model.

Feedback Feedback Feedback — Help the model understand the user

In order to improve the model, it’s important to make the user’s feedback as intuitive as possible and make it a big part of the user flow. There’s only so much an algorithm can understand about human needs without actual human input (imagine expecting a baby to learn how to speak without teaching it what’s right and what’s wrong).

Make it personal

Making users feel like they’re taking an active part in a product’s functioning is highly beneficial, for two reasons:

If the users feel their contribution is making the model’s improvement, they will be much more invested. The more the users feel the model knows them and understands their needs, the more they will enjoy the effects of the model, get precise predictions, and trust the model.

Extra reading on the subject can be found on this great post about the IKEA effect:

Learn from the best (inputs)

It’s a shared motivation for the model to learn from the best quality of input. A good design can encourage the user to upload high-quality inputs and remark when and why low-quality inputs aren’t good enough. e.g., a message saying the input image’s quality is too low in a way that the user understands and “believes”, therefore, wants to upload better images."
"[Copyright : Pop Nukoonrat] © 123RF.com

Revolutionizing Construction

Construction and the methods we use are crucial to our success in modern architecture. We build houses and massive structures using our computers and we harness the that processing power to create new solutions. But artificial intelligence in construction takes things to a whole new level!

It’s a tool that can help us push the boundaries further and it can do a lot to the industry as a whole. So then why haven’t we seen more innovation?

The Construction Industry is Stagnating

This isn’t to say that there hasn’t been a lot of improvements throughout the years, but construction has remained slower to adapt.

In the past we often assumed that productivity equaled larger machines and that theory worked for a while. But now days we need something more than bigger machines, we need smarter machines and solutions.

And while several other industries such as retail, medical and businesses in general have expanded, construction has fallen a bit behind. We simply need to adapt and use more technology.

But what if we had more artificial intelligence in construction? Would this technology help lead us to a utopia?

How Artificial Intelligence Helps Construction

While it’s still early on some parts, AI has proven to show some promise in reducing costs. There is also software out there known as building information modeling, or BIM for short. AI can be trained to help suggest improvements and build solutions early on.

It can also be used in risk management/mitigation, by providing safer alternatives. Construction robots are becoming more popular along with 3D printing, but add AI to it and we have a new advantage.

AI can do the things that are too risky for us to risks our lives with. While using AI in this way is still new and very early, there’s a multitude of other areas it can help in. Of course, being a young technology also brings on risks for those using the technology early on…

The Early Risks Involved

Artificial intelligence in construction is a great solution but more technology also brings on different risks that also need to be considered. Anytime technology is involved we can typically also inherent the risk of getting hacked.

If construction software or robots were to get hacked it could jeopardize an entire project. The argument is that it’s safer to have physical workers doing the actual work than to have robots or AI trying to take over. This is only partially true though.

Construction steadily accounts for 20%+ of yearly deaths at work. AI poses the risk of hackers but as it stands the death toll is high right now, without the involvement in these life saving technologies.

Hacking risks aside, AI isn’t perfect and does make mistakes too! This isn’t always the case but it’s important to recognize that mistakes happen with new technologies. The costs to implement AI could cost more money if it’s done wrong. But it’s not all bad!

[Copyright : Kittipong Jirasukhanont] © 123RF.com

Machine Learning can Mitigate Risks

Machine learning is an important aspect of using artificial intelligence in construction. It allows a program to continually test and essentially “learn”. We’ve seen AI used in the field of medicine with success in the recent past which shows promise. But machine learning gives us some control.

But first, in case you didn’t know what machine learning is… It’s a method used to teach an AI how to accomplish something. It is given parameters to gauge success and failure, a way to remember the results and a way to improve the numbers. Think of it like a race car, if it crashes it fails, if it completes the course it succeeds.

Machine learning can take it a step further though. It can take that concept of the race car and find the most optimal way to complete the course, and that’s why it can be more productive than humans. We use machine learning to run thousands of trials and errors to succeed.

This makes the AI more capable than a human which is why it can benefit construction. A single person can only try so many times, whereas an AI can run tests or simulations thousands of times endlessly. Construction can benefit from an AI that can continually learn the most efficient and safest way to build or solve problems.

Machine learning can also incorporate previous knowledge in its tests, improving the outcome or immediate start of the training. As the AI grows it’ll also show us improved ways of doing things.

Smart AIs Equal Smarter Solutions

The future demands solutions that are safe, cheaper and ultimately faster. Construction is always a time sensitive task because a lot of it happens outside! The weather impacts it just as much as the efficiency of the workers doing it.

AI provides solutions that are faster and cheaper which means there is less risk of weather delaying a project. The faster a task is completed the less likely something will go wrong, in theory.

AI can help with many different areas by…

Providing a solution to labor shortages through the use of software automation and solutions.

Reducing risks to safety by spotting flaws or creating safer alternatives.

Actively monitoring work environments and regulations.

Can be used to collaborate on building plans while making smart suggestions (BIM software mentioned above).

Providing analytics and statistics online for clients or workers.

[Copyright : Preechar Bowonkitwanchai] © 123RF.com

Artificial Intelligence in Construction Should be Embraced

We talked about a lot of the areas that AI can help with but it needs to be given the opportunity. While some companies are already using this technology, there are still many more that are not quite there.

AI can be used to help keep track of projects and to reassure clients. Whether it’s the city contracting the work or a variety of other businesses. Clients could benefit from seeing the progress online. AI has a lot of potential and even the user interface could have AI to learn what clients find most useful to view online.

In the end, AI is going to be in the future of the construction industry. Whether we want it or not, it will help push innovation forward. But what do you think? Should we use more AI or avoid using those technologies in construction? Drop a comment below!"
"The Overlooked Conservative Case for Reining in Big Tech

Democrats aren’t the only ones ready to rewrite the antitrust rules for internet platforms

Photo: SOPA Images/Getty Images

Never in world history has one sector of the global economy risen to such global dominance, so fast, as Big Tech has in the past 20 years.

In 2000, Amazon was an online bookseller, Apple was still an underdog, Google was a scrappy startup with little revenue, and Facebook didn’t exist. Today, along with Microsoft, they are the world’s five most valuable companies, and their decisions carry a level of global influence rivaled only by nation-states. They exert control over what we can say, how we can say it, what we buy, and what we read, and they wield unilateral power over the countless smaller businesses that rely on their platforms.

Until about five years ago, a prevailing 21st-century view was that the internet sector was so dynamic that upstarts could come along at any point and depose the giants: Just look at how Google and Apple blew past Microsoft, or how Facebook conquered MySpace. That view is no longer tenable, as the top platforms’ network effects, lock-in, access to data, diversification of business lines, and ability to buy or copy rivals has given them advantages that now appear nearly insurmountable. The relevant business question is no longer, “Will they stay on top?”, but rather, “What markets will they conquer next?” (The one competitive threat that still looms is that China-based giants could outmaneuver them with products such as WeChat and TikTok. But the Trump administration’s crackdown on Chinese tech has abruptly curtailed that threat domestically, and India’s crackdown has mitigated it in the largest non-aligned market.)

What to do about that concentration of power, if anything, is a question that has rapidly grown in urgency. There is an emerging consensus that antitrust action in some form is warranted, including among Republicans who are naturally skeptical of government intervention in markets. But there has been little clarity or agreement as to what form that action should take — until now.

The Pattern

We finally have a blueprint for regulating Big Tech. Or rather, two blueprints.

Undercurrents

Under-the-radar trends, stories, and random anecdotes worth your time.

Facebook and Twitter are taking some precautionary measures ahead of the U.S. election. The most interesting came from Twitter, which announced on Friday that it will take three previously untried steps to pump the brakes on misinformation and polarizing content, starting October 20. First, it will default to a quote-tweet when you go to retweet something, encouraging you to stop and think about what you want to add to the conversation rather than simply amplifying a viewpoint. Second, it will stop surfacing tweets from people you don’t follow in your feed or notifications. Finally, it will only show trending topics that come with editorial context. You can read its full announcement here. Facebook, for its part, announced an indefinite ban on political ads starting after November 3, along with other measures aimed at thwarting misinformation around who won the election or incitements to violence in its wake.

The most interesting came from Twitter, which announced on Friday that it will take three previously untried steps to pump the brakes on misinformation and polarizing content, starting October 20. First, it will default to a quote-tweet when you go to retweet something, encouraging you to stop and think about what you want to add to the conversation rather than simply amplifying a viewpoint. Second, it will stop surfacing tweets from people you don’t follow in your feed or notifications. Finally, it will only show trending topics that come with editorial context. You can read its full announcement here. Facebook, for its part, announced an indefinite ban on political ads starting after November 3, along with other measures aimed at thwarting misinformation around who won the election or incitements to violence in its wake. Cambridge Analytica didn’t unduly influence Brexit, a U.K. commission concluded, wrapping a three-year investigation into the political consultancy’s use of Facebook data in the campaign. The Financial Times reports that probe found that the methods used by a Cambridge Analytica affiliate were “in the main, well recognised processes using commonly available technology,” and that the resulting targeting of voters was not uniquely effective. The report was taken as vindication by some who felt the Cambridge Analytica scandal was overblown all along. Some privacy advocates were quick to reply that the real scandal was always more about how the data was gathered and obtained than how it affected election outcomes. (Both can be true; I made a version of this argument in 2018.)

Headlines of the week

Five Years of Tech Diversity Reports — and Little Progress

— Sara Harrison, Wired

How Excel may have caused loss of 16,000 Covid tests in England

— Alex Hern, The Guardian

QAnon high priest was just trolling away as a Citigroup tech executive

— William Turton and Joshua Brustein, Bloomberg"
"Last year I had just landed my first job as Software Developer at Signify (the former Philips Lighting) and, after a few weeks, a colleague asked me if I wanted to go to a No-Code conference with him. “A conference with everything paid? Nice!” I ran to my laptop and started my desk-research on what Low-Code was. It kind of reminded me of the MIT App Inventor, but wider, more feature-complete. It triggered my curiosity: “Is this the future of the job?”. I did not expect that I was about to embark on a one-year-long exploration, that would have exposed me to external vendors, to other departments, and could have potentially changed the way we develop in our company.

In this article, I am going to describe the exploration process we followed, what were our expectations and learnings, and what we see looking forward.

How and Why it started

Low Code Platforms: Software that provides an environment to create end-to-end web or mobile applications and lift infrastructure through graphical user interfaces and (possibly) traditional programming.

In the continuous strive to improve our process and technology competencies, to increase our productivity and reduce the time to get from an idea to a prototype, some colleagues started looking into the Low-Code world. The choice for this technology came from past experiences in different companies in which the technology was successfully adopted, and its impressive presence at the Gartner’s Conference in 2018.

Gartner Magic Quadrant for Enterprise High-Productivity Application Platform as a Service, 2019, from DZone

There were two main desires: the ability to quickly build prototypes that could be easily integrated with the existing backend infrastructures, and the ability to co-create with UX/UI designers and let them use the tools to co-create with the customers.

Desk research depicts Low-Code in conflicting ways, from the future of the development to a disaster. A lot depends on the context, and on how these tools fit in the company culture.

From many to few

There are many Low-Code platforms in the market. Trying them all would take just too much time.

Low-Code selection process

We started by walking around some conferences, talking with employees, partners, customers. At first, we were impressed: big audiences, from half a thousand people to a few thousands, a big number of applications running on the platforms, speed and agility repeated in almost every keynote. Later, we realized that there was something missing: many demos and claims were quite generic, we left each conference without a real feeling of what is possible and how. Something more was needed.

We picked some of the vendors from the Quadrant and got in touch with them. We asked for some technical information (Does your platform support REST calls? And OAuth2?) and a short face-to-face demo. Not the nicest process, but it already started highlighting some differences:

Native Low(No)-Code platforms (created to do that and that only) and platforms that are evolving towards Low-Code : the former with greater flexibility and complexity, the latter with some flexibility on top of their earlier scope. We did not have an application scope in mind, as we usually don’t have when we come up with a new idea. So we picked the first category.

(created to do that and that only) and platforms that are : the former with greater flexibility and complexity, the latter with some flexibility on top of their earlier scope. We did not have an application scope in mind, as we usually don’t have when we come up with a new idea. So we picked the first category. “Citizen Developer”, “Citizen Developer and Developer”, “Developer” platforms: ranging from the most graphical/blocks-oriented and less flexible, to the ones that seemed more like graphical coding. Citizen developer is a recurrent expression when looking at Low-Code, and it represents an application developer without a software background. Given the complexity we were looking for, the platforms for the “Citizen Developer and Developer” category and the “Developer” one suited better.

We chose two platforms and moved to the next step: development.

Hands-On: some premises

So, we picked two platforms: the fun could start. The attention points:

The feeling : we really wanted to feel the platforms . As a bunch of developers, we wanted to do some training, read some documentation, and bend the technology to our needs. We absolutely did not want any consultant developing for us or sitting next to us daily to help us develop. Most developers learn one/two frameworks per year, and it’s highly uncommon to have consultants help you do that. Why would we treat Low-Code differently?

: we really wanted to feel the platforms As a bunch of developers, we wanted to do some training, read some documentation, and bend the technology to our needs. We or sitting next to us daily to help us develop. Most developers learn one/two frameworks per year, and it’s highly uncommon to have consultants help you do that. Why would we treat Low-Code differently? The community : we wanted to join the community. Relying on the platform technical support is nice, but on the day-to-day development, you need a community. If I am stuck writing JS code, I know the solution is on StackOverflow. Is there a SO for Low-Code?

: we wanted to join the community. Relying on the platform technical support is nice, but on the day-to-day development, you need a community. If I am stuck writing JS code, I know the solution is on StackOverflow. The learning curve : we wanted to perceive the learning curve. If we would have adopted the platform, we would have brought on board as many colleagues as possible. How much time would that have taken?

: we wanted to perceive the learning curve. If we would have adopted the platform, we would have brought on board as many colleagues as possible. How much time would that have taken? Flexibility : what can we do on these platforms? How far can we take our application?

: what can we do on these platforms? Ease of design: can we give the platform to designers and let them put the text box in the right position instead of sending JPG designs to the developers? How cool would that be!

can we and let them put the text box in the right position instead of sending JPG designs to the developers? How cool would that be! Best practices: if the prototype application becomes a product, can best practices (peer review, testing, …) be enforced?

Hands-On: Planned vs Realized"
"Do not confuse it with Dark Mode

The hype of “The Social Dilemma” made many viewers become aware of the power of technology and its influence on all of us. For UX Designers, the use of dishonest tricks in digital platforms is not a new topic. We call them dark patterns.

Evil design patterns, unfortunately, are very common. To demonstrate, I created a compilation of dark patterns we can find every day.

Youtube Disguised Ads

Author/Copyright holder: Youtube. Copyright terms and license: Fair Use.

The very first thing Youtube displays when the app is open is not a video, but an ad that really looks like a video. When a user scrolls down the app, he comes across many of these ads disguised as videos, which the user can easily click by mistake.

Spotify Roach Motel

Remember when you created your Spotify account? Probably not. Maybe you only used OAuth and immediately got logged in with your Facebook account. If not, you simply filled a small survey with your registration data and you were in. What about deleting your Spotify account? If you ever tried to do it, you probably remember how painful it was.

Spotify’s webpage makes it easy for the user to find where to Log in or to Sign up. There are clear options in the navbar, as well as a highlighted button in the center of the screen for it.

Author/Copyright holder: Spotify. Copyright terms and license: Fair Use.

Author/Copyright holder: Spotify. Copyright terms and license: Fair Use.

If you click on “Login”, you’ll find out you don’t even need to create a new account to use Spotify. You can automatically login with your Facebook, Apple, or Google account. How easy.

Deleting a Spotify Account, on the other hand, can be a painful experience. You “only” have to complete the following instructions:

Navigate to support.spotify.com/us/contact-spotify-support/. Click on “Login” in the upper right-hand corner and enter your credentials. You need to work through a series of on-screen questions. Click on “Subscription”. Choose “I want to close my Spotify account”. Click on “Contact to Close”.

Author/Copyright holder: Spotify. Copyright terms and license: Fair Use.

When you click on the “Contact to Close” button, you are taken to a support form. This means Spotify UI completely prevents a user from closing his account without having to pass through a support procedure he doesn’t have any control over.

Reddit Bait and Switch

Author/Copyright holder: Reddit. Copyright terms and license: Fair Use.

When scrolling Reddit’s feed, the user can expand the images displayed by clicking on them. However, Reddit’s feed has plenty of “promoted” posts, which are actually ads. The user is tricked into clicking on the ad’s image, but instead of the default expanding behavior, he is automatically redirected to some ad website.

Instagram Roach Motel

Instagram uses the Roach Model pattern in a different form from the one used in Spotify. In this case, besides being a mobile application, Instagram accounts are impossible to delete within the app. The user needs to access a browser, which makes the process of account deletion unnecessary harder for him.

Excerpt of Instagram Help Documentation

Skillshare Forced Continuity

Forced Continuity: When your free trial with a service comes to an end and your credit card silently starts getting charged without any warning. You are then not given an easy way to cancel the automatic renewal.

Author/Copyright holder: Skillshare. Copyright terms and license: Fair Use.

Skillshare uses one of the most common dark patterns in the subscription services. The user is asked to provide his credit card data to access a free trial, a situation that leads to automatic debts as soon as the trial period ends. The user can cancel his subscription anytime, even before the paid period begins. However, many companies do not properly notify their users that they are about to be charged until it’s too late.

Wish Confirmshaming

Confirmshaming: The act of guilting the user into opting into something. The option to decline is worded in such a way as to shame the user into compliance.

Author/Copyright holder: Wish. Copyright terms and license: Fair Use.

When someone unsubscribes from the Wish newsletter, the confirmation dialog is worded in a way of attempting to make the user feel guilty from leaving. Not only does the title state “We’re sad to see you go”, but also the user is required to choose the option “I Don’t Like Discounts” instead of a neutral “Unsubscribe”.

AliExpress Price Comparison Prevention

Price Comparison Prevention: The retailer makes it hard for you to compare the price of an item with another item, so you cannot make an informed decision.

AliExpress is an online retail service with a big product offer. In the gift above, we see the search results for a makeup brush set.

The search results screen is where the user can compare his different options. At this moment, we see how the results are displayed in individual prices. However, when the user clicks on one product details, the price changes for a price interval. The user must choose between different options of quantity, color, and shipping source to obtain the final price of his product. This way of price display makes it harder for the user to compare different products and make an informed decision.

Broadway.com Hidden Costs

Hidden Costs: You get to the last step of the checkout process, only to discover some unexpected charges have appeared, e.g. delivery charges, tax, etc.

In the capture above it is displayed how, when the user selects the seats, the price displayed is $59.50 each. However, in the checkout step, $14.88 for Service & Handling is charged for each ticket, leading to an unexpected final price. This trick is often used in ticket selling platforms, making it hard for users to plan how much they are willing to spend.

Mariana Vargas is a full-time UX Engineer and part-time singer based in Lisbon, Portugal. You can connect with her on LinkedIn."
"Revising What Makes Covid-19 Special: It’s Not Blood Clots

Infections come with blood clots — be it SARS, MERS, or other pathogens. So, what really makes Covid-19 special?

We know that Covid-19 kills by pneumonia and coagulopathy. Pneumonia means lung infection, particularly in the air sacs, that obstruct gas exchange and breathing. Coagulopathy means blood clotting disorder that comes by different names depending on where the clot happens.

In Covid-19, scientists have seen cases of pulmonary embolism (artery blockage in the lungs), alveolar capillary microthrombi (small blood clots in air sacs), deep venous thrombosis (blood clots in veins deep in the body, usually in legs), multi-organ endotheliitis (inflamed blood vessels), multi-organ microthrombi (small blood clots), ischemic infarcts (artery blockage near the brain), and perhaps more.

These medical terms are nothing but coagulopathies. Among these, deep venous thrombosis (DVT) is arguably the most relevant as it usually precedes pulmonary embolism (PE). Scientists often lump them together as DVT/PE. And DVT/PE the most common type of Covid-19 coagulopathy. It happens in about 25% of Covid-19 cases in the ICU. In contrasts, the usual prevalence of DVT/PE in non-Covid-19 ICU cases is at <2%.

Therefore, there has been a widespread recognition in the scientific literature [refs 1, 2, 3, 4, etc.] and news outlets that Covid-19 is a disease of the blood vessels. Coagulopathies are the driver of the multi-organ complications in severe Covid-19, we all think. It makes Covid-19 one particular kind of disease that can attack the blood vessels. At least I have previously written about it here:

While it is true that Covid-19 causes coagulopathies that hit distant organs, it is not what makes Covid-19 unique from other diseases. Here are a few reasons why:

These points clarify that Covid-19 is a respiratory disease that occasionally comes with coagulopathies that affect other organs. In other words, Covid-19 is not a blood clotting disease, but pneumonia that may come with blood clots.

So, what makes Covid-19 special?

It is not blood clots as it is a general characteristic of pneumonia or infections. It is not the incubation period (i.e., the duration between the first viral exposure and symptom appearance) as it is similar for all SARS, MERS, and Covid-19. It is not the propensity to affect almost every organ in the body as most systemic diseases are like that. It is not the ability to cause lingering symptoms for weeks to months, even in discharged patients, as other infections can do that.

These may be anticlimatic, but evolution does not magically build an entirely new distinct biological entity after all. Evolution works by modifying existing biological constructs. Hence, Covid-19 must also be viewed in light on other viruses (or pathogens).

In reality, Covid-19 is not very deadly with a case fatality rate of about 1%, compared to 9.7% and 34% of SARS and MERS, respectively. But Covid-19 is much more lethal to the older age groups, black people, and individuals with multiple medical comorbidities. Nearly half of infected people do not show symptoms, yet they can transmit the virus to others. Also, Covid-19 is highly contagious owing to its efficient binding capacity to the host ACE2 receptor.

Thus, the silent and efficient human spread — coupled with overpopulation, growing ageing population, increasing prevalence of medical comorbidities, international travel, poor infection control policies, and social inequalities in some countries — make Covid-19 successful."
"The Link Between Flu Vaccines and Heart Diseases Explained

What the data from 1918 to 2020 tells us about influenza and heart diseases.

Image by Dimitris Christou from Pixabay

The data before 2000

A 1932 paper reported, for the first time, that influenza led to more excess deaths from heart diseases. Researchers noted a peak in cardiovascular deaths following every influenza outbreak in the U.S. in 1918, 1922, 1923, 1926, and 1928. An excess in deaths from kidney, lung, and cerebrovascular diseases and diabetes also happened, although to a much lesser extent than heart diseases.

The pattern repeats for subsequent influenza outbreaks.

“Peak months of mortality for ischemic heart disease, cerebrovascular disease, and diabetes mellitus coincided appropriately with peaks in pneumonia and influenza [during the winter of 1959 to 1999],” stated a study in the American Journal of Epidemiology. “Weather and other factors may determine the timing and modulate the magnitude of the winter-season increase in mortality, but the primary determinant appears to be the influenza virus.” Simply put, the flu best explains the excess deaths, especially from heart diseases.

“Recognition of influenza as a trigger for acute coronary events calls for more intensive efforts to increase the vaccination rate in subjects at risk of CHD [coronary heart disease].”

Another study in the European Heart Journal confirms these findings with eight years of autopsy data (total sample of 34,892) from 1993 to 2000. “Influenza epidemics are associated with a rise in autopsy-confirmed coronary deaths,” the study revealed, by 10–30%. “Recognition of influenza as a trigger for acute coronary events calls for more intensive efforts to increase the vaccination rate in subjects at risk of CHD [coronary heart disease].”

Using hospital data of over five million people from 1897 to 2001, a paper in the New England Journal of Medicine (NEJM) noted a 5- and 3-fold increase in incidents of heart attacks and stroke, respectively, during the first three days of a respiratory tract infection. But these heart and stroke risks disappeared in persons vaccinated for tetanus, pneumococcus, or influenza.

The data after 2000

Decades after the 1918 pandemic, the theory that influenza induces heart diseases still stands.

From 1999 to 2008, a study in the Journal of Infectious Diseases estimated that influenza accounted for 3.4% of cardiovascular deaths in the U.K. and 5.6% in Hong Kong — equating to tens of thousands of deaths. And the deaths often occurred within the first three days of influenza symptoms. A reason for the higher mortality in Hong Kong, the authors speculate, could be that influenza vaccines were only widely accessible in 2004.

“This study is key because it truly highlights how common it can be to experience serious heart complications after getting the flu — including in some people that did not have any prior chronic health conditions.”

Using patient data collected from 2004 to 2015 in New York, a paper issued this month in the Journal of American Heart Association found seasonality in heart diseases and strokes— just like the flu. “We found that if someone’s going to have a heart attack, it’s going to occur within seven days of the flu-like illness, during the acute phase,” said Amelia K. Boehme, professor of epidemiology at Columbia University who directed the study. “With stroke, we see an increased risk seven to 15 days after, similar to heart attacks.”

Lastly, a 2020 study in the Annals of Internal Medicine showed that 11.7% of 89,999 older adults hospitalized with influenza from 2010 to 2018 also had a subsequent cardiovascular event. “This study is key because it truly highlights how common it can be to experience serious heart complications after getting the flu — including in some people that did not have any prior chronic health conditions,” said Dr. Natasha Bhuyan, MD, family physician in Arizona.

Clearly, seasonal influenza viruses could trigger heart diseases. Why?

The burden on the heart

Reviewing the abovementioned studies, a paper published this month in the Journal of the American College of Cardiology (JACC) delved deeper into the literature to pinpoint the possible explanations for influenza-induced heart diseases:

Influenza raises the body's metabolic demands are raised to fight infection. The fight-or-flight system, immune system, and blood vessels carrying immune cells become highly activated, which could be overly taxing for persons at risk for heart diseases.

Influenza weakens the lung function, which increases the risk of other bacterial or viral respiratory infections that further burdens the lungs, heart, and blood vessels — the cardiovascular-pulmonary system.

Influenza initiates both systemic and localized inflammation, which could trigger the rupture of plaque in blood vessels.

Influenza has been shown to infect cells of the heart, lungs, and blood vessels in animals and cultured human cells. This infection further adds a burden on the organs already affected by inflammation.

Therefore, the rough idea is that influenza could be the final push for people at pre-heart disease stages.

Influenza vaccines and heart diseases

Since influenza could trigger — i.e., a risk factor — heart diseases, does that mean its vaccine is a protective factor? One observational study in the NEJM (discussed above) has found that risks of heart diseases and stroke disappeared in vaccinated (including influenza) persons.

However, observations show associations or possible causation. Randomized controlled trials (RCTs) are required to establish causation.

Most likely flu shots will not be the silver bullet. But it could make a huge difference even if it shows some efficacy given that heart diseases kill by the millions.

A 2015 Cochrane-standard meta-analysis (latest to date) synthesized eight RCTs with a pooled sample size of 12,029 persons with a cardiac history. They calculated that influenza vaccines halved the future death rates from heart diseases — from 5.1% (placebo group) to 2.1% (vaccine group).

“In patients with cardiovascular disease, influenza vaccination may reduce cardiovascular mortality and combined cardiovascular events,” the meta-analysis concluded. “However, studies had some risk of bias, and results were not always consistent, so additional higher-quality evidence is necessary to confirm these findings.”

Regardless, at least the flu shot does not increase the risk of heart diseases, so chances are that at-risk persons would benefit.

Source: Partial table from Behrouzi et al. (2020). Journal of the American College of Cardiology (JACC)

Other on-going big RCTs are investigating if influenza vaccines are a feasible strategy to prevent heart diseases (see figure). Most likely flu shots will not be the silver bullet. But it could make a huge difference even if it shows some efficacy given that heart diseases kill by the millions. It took 17.79 million lives in 2017 — nearly double that of the #2 leading cause of death, cancers. Not to mention that heart diseases are the most expensive disease to treat, and 80% of cases are preventable.

At the same time, efforts must also be made to improve flu shots’ efficacy. “It is also known that although the seasonal influenza vaccine is better than nothing, it is not nearly as effective as it can be, having ranged from 10% to 60% in estimated effectiveness in recent years,” the JACC review stated.

Short abstract

From 1918 to 2020, the flu has contributed to the countless lives lost to heart diseases. Evidence shows that influenza serves as a trigger of heart diseases, especially within a week of symptom onset and in those already at risk for heart diseases. And that influenza vaccines could mitigate this effect to some extent. Anyhow, some level of efficacy in reducing heart diseases — that kill by the millions and cost by the billions — would be precious. All the more reason to invest in improving the effectiveness of flu shots and the public’s perception of it."
